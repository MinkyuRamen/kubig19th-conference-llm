{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/minkyuramen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import arxiv\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
    "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import IPython\n",
    "\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain.adapters.openai import convert_openai_messages\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datetime\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = '/Users/minkyuramen/Desktop/project/env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import semantic_scholoar_api as ss\n",
    "\n",
    "openai_api = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2302.04761v1'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Toolformer: Language Models Can Teach Themselves to Use Tools\"\n",
    "\n",
    "retriever = ArxivRetriever(load_max_docs=1)\n",
    "arxiv_id = retriever.invoke(query)[0].metadata['Entry ID'].split('/')[-1]\n",
    "arxiv_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/minkyuramen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
    "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = '/Users/minkyuramen/Desktop/project/env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# sentence-transformers로 embedding 후 abstract간 cosine similarity를 측정하였다.\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def query2references(query, num=20, api_key=api_key):\n",
    "    \"\"\"References\"\"\"\n",
    "    # Define the API endpoint URL\n",
    "    url = 'https://api.semanticscholar.org/graph/v1/paper/search?fields=paperId,title,abstract'\n",
    "\n",
    "    # paper name 기입\n",
    "    query_params = {'query': query}\n",
    "    headers = {'x-api-key': api_key}\n",
    "\n",
    "    response = requests.get(url, params=query_params, headers=headers).json()\n",
    "    paper_id = response['data'][0]['paperId']\n",
    "\n",
    "\n",
    "    fields = '?fields=title,publicationDate,influentialCitationCount,contexts,intents,abstract'\n",
    "    \"\"\"\n",
    "    context ; snippets of text where the reference is mentioned\n",
    "    intents ; intents derived from the contexts in which this citation is mentioned.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references'+ fields\n",
    "\n",
    "    # Send the API request\n",
    "    response2 = requests.get(url=url, headers=headers).json()\n",
    "    # response\n",
    "\n",
    "    threshold = 10\n",
    "    references = []\n",
    "    for elements in response2['data']:\n",
    "        try:\n",
    "            if elements['citedPaper']['influentialCitationCount'] > threshold:\n",
    "                references.append(elements)\n",
    "            else: pass\n",
    "        except: pass\n",
    "\n",
    "    return response['data'], references[:num]\n",
    "\n",
    "def reference_recommend(query, num=20, threshold=0.6, recommend=None, api_key=api_key, sorting=True):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    ### 상위 20개의 reference이 많은 논문들을 select\n",
    "    target_response, reference = query2references(query=query,num=num,api_key=api_key)\n",
    "    reference_response = [ref['citedPaper'] for ref in reference]\n",
    "    reference_context = [ref['contexts'] for ref in reference]\n",
    "    reference_intent = [ref['intents'] for ref in reference]\n",
    "\n",
    "\n",
    "    ## target 논문과 num개의 reference 사이의 유사도 계산\n",
    "    abs_dict = {}\n",
    "    abs_dict[target_response[0]['title']] = target_response[0]['abstract']\n",
    "\n",
    "    for keyword in reference_response:\n",
    "        paper_id, title = keyword['paperId'], keyword['title']\n",
    "        abstract = str(keyword['abstract'])\n",
    "        abs_dict[title] = abstract\n",
    "\n",
    "    sentences = list(abs_dict.values())\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    rec = cosine_similarity(sentence_embeddings)\n",
    "    \n",
    "    ## 유사도가 threshold 이상인 논문들을 추출\n",
    "    indices = np.where(rec[0][1:] > threshold)[0]\n",
    "    rec_lst = [reference_response[i] for i in indices]\n",
    "    rec_context = [reference_context[i] for i in indices]\n",
    "    rec_intent = [reference_intent[i] for i in indices]\n",
    "\n",
    "    for item in rec_lst:\n",
    "        if item['publicationDate'] is None:\n",
    "            item['publicationDate'] = datetime.max\n",
    "\n",
    "    # intent와 context를 list에 추가\n",
    "    for i in range(len(rec_lst)):\n",
    "        rec_lst[i]['intent'] = ' '.join(rec_intent[i])\n",
    "        rec_lst[i]['context'] = rec_context[i][0]\n",
    "    if recommend:\n",
    "        if len(rec_lst) > recommend:\n",
    "            rec_lst = rec_lst[:recommend]\n",
    "\n",
    "    # 날짜순으로 정렬\n",
    "    if sorting==True:\n",
    "        rec_lst = sorted(rec_lst, key=lambda x: x['influentialCitationCount'], reverse=True)[:num]\n",
    "        return target_response, sorted(rec_lst, key=lambda x: datetime.strptime(x['publicationDate'], '%Y-%m-%d') if isinstance(x['publicationDate'], str) else x['publicationDate'])\n",
    "    \n",
    "    return rec_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method는 없는 논문도 많아서 introduction으로만 진행 >> introduction만으로도 background + method + contribution 담기는 충분하다고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "# 1. arXiv 메타데이터 가져오기\n",
    "def get_arxiv_metadata(arxiv_id):\n",
    "    base_url = f\"http://export.arxiv.org/api/query?id_list={arxiv_id}\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        raise Exception(\"Error: Unable to fetch data from arXiv API\")\n",
    "\n",
    "# 2. PDF URL 추출\n",
    "def extract_pdf_url(metadata):\n",
    "    tree = ET.ElementTree(ET.fromstring(metadata))\n",
    "    root = tree.getroot()\n",
    "    pdf_url = None\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        pdf_url = entry.find('{http://www.w3.org/2005/Atom}id').text.replace('/abs/', '/pdf/') + \".pdf\"\n",
    "    return pdf_url\n",
    "\n",
    "# 3. PDF 다운로드\n",
    "def download_pdf(pdf_url, output_path):\n",
    "    response = requests.get(pdf_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"PDF downloaded successfully: {output_path}\")\n",
    "    else:\n",
    "        raise Exception(\"Error: Unable to download PDF\")\n",
    "\n",
    "# 4. PDF에서 텍스트 추출\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        all_text = \"\"\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            all_text += text\n",
    "    return all_text\n",
    "\n",
    "# 5. 특정 섹션 텍스트 추출\n",
    "def extract_section(text, start_keyword, end_keyword=None):\n",
    "    start = re.search(fr\"\\b{start_keyword}\\b\", text, re.IGNORECASE)\n",
    "    if not start:\n",
    "        return f\"{start_keyword} section not found\"\n",
    "    if end_keyword:\n",
    "        end = re.search(fr\"{end_keyword}\", text[start.end():], re.IGNORECASE)\n",
    "        if end:\n",
    "            section = text[start.start():start.end() + end.start()]\n",
    "        else:\n",
    "            section = text[start.start():]\n",
    "    else:\n",
    "        section = text[start.start():]\n",
    "    return section\n",
    "\n",
    "# 6. 포맷팅\n",
    "def format_section(section_text):\n",
    "    # 마지막 문장이 마침표로 끝나지 않으면 줄바꿈 없애기\n",
    "    lines = section_text.split('\\n')\n",
    "    formatted_lines = []\n",
    "    for line in lines:\n",
    "        if line and not line.endswith('.'):\n",
    "            formatted_lines.append(line.strip())\n",
    "        else:\n",
    "            formatted_lines.append(line)\n",
    "    return ' '.join(formatted_lines)\n",
    "\n",
    "# 전체 파이프라인 실행\n",
    "def intro_pipeline(arxiv_id, output_path):\n",
    "    metadata = get_arxiv_metadata(arxiv_id)\n",
    "    pdf_url = extract_pdf_url(metadata)\n",
    "    download_pdf(pdf_url, output_path)\n",
    "    extracted_text = extract_text_from_pdf(output_path)\n",
    "\n",
    "    introduction_section = extract_section(extracted_text, \"Introduction\", r\"\\n2 \")\n",
    "    introduction = format_section(introduction_section)\n",
    "\n",
    "\n",
    "    return introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERSATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1️⃣ Target paper이 reference 한 논문들 중 인용수가 많은 논문 filtering + reference 한 논문들 중 출판일자별로 sorting\n",
    "\n",
    "2️⃣ reference paper에 대한 소개 & reference된 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✔️ sorting=True  ; citation로 sorting 후 datetime으로 sorting\n",
    "\n",
    "✔️ sorting=False ; target paper에서 citation 되는 순서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools', 'abstract': 'Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'paperId': '2e347a977f14eca7cc5bbbb4c71145b75637340c',\n",
       "  'title': 'MLQA: Evaluating Cross-lingual Extractive Question Answering',\n",
       "  'abstract': 'Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.',\n",
       "  'influentialCitationCount': 112,\n",
       "  'publicationDate': '2019-10-16',\n",
       "  'intent': 'methodology',\n",
       "  'context': 'Multilingual QA We evaluate all models on MLQA (Lewis et al., 2019), a multilingual QA benchmark.'},\n",
       " {'paperId': 'a75649771901a4881b44c0ceafa469fcc6e6f968',\n",
       "  'title': 'How Can We Know What Language Models Know?',\n",
       "  'abstract': 'Abstract Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a __ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a __ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.',\n",
       "  'influentialCitationCount': 49,\n",
       "  'publicationDate': '2019-11-28',\n",
       "  'intent': 'background',\n",
       "  'context': '…to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero-and few-shot settings (Jiang et al., 2020; Schick and Schütze, 2021a).'},\n",
       " {'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0',\n",
       "  'title': 'Language Models are Few-Shot Learners',\n",
       "  'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\",\n",
       "  'influentialCitationCount': 3058,\n",
       "  'publicationDate': '2020-05-28',\n",
       "  'intent': 'methodology result background',\n",
       "  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n",
       " {'paperId': '002c256d30d6be4b23d365a8de8ae0e67e4c9641',\n",
       "  'title': 'Improving language models by retrieving from trillions of tokens',\n",
       "  'abstract': 'We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.',\n",
       "  'influentialCitationCount': 53,\n",
       "  'publicationDate': '2021-12-08',\n",
       "  'intent': 'background',\n",
       "  'context': '…information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).'},\n",
       " {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb',\n",
       "  'title': 'PaLM: Scaling Language Modeling with Pathways',\n",
       "  'abstract': 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.',\n",
       "  'influentialCitationCount': 301,\n",
       "  'publicationDate': '2022-04-05',\n",
       "  'intent': 'background',\n",
       "  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n",
       " {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221',\n",
       "  'title': 'OPT: Open Pre-trained Transformer Language Models',\n",
       "  'abstract': 'Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.',\n",
       "  'influentialCitationCount': 266,\n",
       "  'publicationDate': '2022-05-02',\n",
       "  'intent': 'result',\n",
       "  'context': 'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'},\n",
       " {'paperId': '398e4061dde8f5c80606869cebfa2031de7b5b74',\n",
       "  'title': 'Few-shot Learning with Retrieval Augmented Language Models',\n",
       "  'abstract': 'Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.',\n",
       "  'influentialCitationCount': 46,\n",
       "  'publicationDate': '2022-08-05',\n",
       "  'intent': 'methodology background',\n",
       "  'context': 'Specifically, we use Atlas (Izacard et al., 2022), a retrieval-augmented LM finetuned on Natural Questions (Kwiatkowski et al.'},\n",
       " {'paperId': '6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7',\n",
       "  'title': 'PAL: Program-aided Language Models',\n",
       "  'abstract': 'Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought\\'\\', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .',\n",
       "  'influentialCitationCount': 46,\n",
       "  'publicationDate': '2022-11-18',\n",
       "  'intent': 'methodology result background',\n",
       "  'context': '…(Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., 2022) and Python interpreters (Gao et al., 2022).'},\n",
       " {'paperId': '6f4cc536f9ed83d0dbf7e919dc609be12aa0848a',\n",
       "  'title': 'Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor',\n",
       "  'abstract': 'Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.',\n",
       "  'influentialCitationCount': 19,\n",
       "  'publicationDate': '2022-12-19',\n",
       "  'intent': 'methodology',\n",
       "  'context': 'Honovich et al., 2022; Wang et al., 2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls.'},\n",
       " {'paperId': 'e65b346d442e9962a4276dc1c1af2956d9d5f1eb',\n",
       "  'title': 'Self-Instruct: Aligning Language Models with Self-Generated Instructions',\n",
       "  'abstract': 'Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.',\n",
       "  'influentialCitationCount': 122,\n",
       "  'publicationDate': '2022-12-20',\n",
       "  'intent': 'methodology',\n",
       "  'context': 'Honovich et al., 2022; Wang et al., 2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls.'}]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 100 # citation이 많이된 상위 num개의 paper 탐색\n",
    "threshold = 0.6\n",
    "recommend = 10 # 그중에서 target paper와 유사한 상위 recommend개의 paper 추천\n",
    "\n",
    "# recommend 개의 가장 연관된 reference 추천\n",
    "target, reference = reference_recommend(query=query, num=num, threshold=threshold, recommend=recommend, api_key=api_key, sorting=True)\n",
    "print(len(reference))\n",
    "\n",
    "target = {'title': target[0]['title'], 'abstract': target[0]['abstract']}\n",
    "print(target)\n",
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 5314\n",
      "\tPrompt Tokens: 4466\n",
      "\tCompletion Tokens: 848\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.008395\n"
     ]
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "content = reference\n",
    "\n",
    "# 논문 제목 (날짜순서로 정렬)\n",
    "ref_paper = list({ref['title'] : ref['publicationDate'] for ref in reference}.keys())\n",
    "\n",
    "# 프롬프트\n",
    "system_template = \"\"\"\n",
    "    You are an AI research assistant tasked with creating structured reports.\n",
    "    Your sole purpose is to write well written, critically acclaimed.\n",
    "    objective and structured reports on given text.\n",
    "    \"\"\"\n",
    "human_template = \"\"\"\n",
    "    Content: Content of the paper we want to explore is as follows.\\n\n",
    "            1. {query}(target paper) information : {target}\\n\n",
    "            2. reference papers information : {content}\\n\\n\\n\n",
    "    Query: There are {recommend} papers included in the list \"{ref_paper}\". Please answer for all papers.\\n\n",
    "            1. Explain the abstract of the paper {ref_paper} in detail and mention publicationDate.\\n\n",
    "            2. Explain why the paper was mentioned in {query}. Using 'context' and 'intent' part\\n\n",
    "    \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    chain =  prompt | model | parser\n",
    "    report = chain.invoke({\n",
    "    \"content\": content,\n",
    "    \"ref_paper\": ref_paper,\n",
    "    \"query\": query,\n",
    "    \"target\": target,\n",
    "    \"recommend\": recommend,\n",
    "    })\n",
    "print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('**1. Abstract Analysis of Selected Papers:**\\n'\n",
      " '\\n'\n",
      " '1. **MLQA: Evaluating Cross-lingual Extractive Question Answering**\\n'\n",
      " '   - Abstract: The paper presents MLQA, a multilingual extractive question '\n",
      " 'answering evaluation benchmark aimed at advancing research in cross-lingual '\n",
      " 'QA. MLQA includes QA instances in multiple languages and evaluates '\n",
      " 'cross-lingual models and machine-translation-based baselines. Publication '\n",
      " 'Date: 2019-10-16\\n'\n",
      " '\\n'\n",
      " '2. **How Can We Know What Language Models Know?**\\n'\n",
      " '   - Abstract: The paper explores methods to estimate the knowledge '\n",
      " 'contained in language models more accurately by generating high-quality '\n",
      " 'prompts automatically. It improves accuracy in extracting relational '\n",
      " 'knowledge from LMs. Publication Date: 2019-11-28\\n'\n",
      " '\\n'\n",
      " '3. **Language Models are Few-Shot Learners**\\n'\n",
      " '   - Abstract: Demonstrates the significant performance gains of large '\n",
      " 'language models in few-shot learning tasks without task-specific fine-tuning '\n",
      " 'datasets. Shows how scaling up language models enhances few-shot performance '\n",
      " 'across various NLP tasks. Publication Date: 2020-05-28\\n'\n",
      " '\\n'\n",
      " '4. **Improving language models by retrieving from trillions of tokens**\\n'\n",
      " '   - Abstract: Enhances language models by conditioning on document chunks '\n",
      " 'retrieved from a large corpus, achieving comparable performance to GPT-3 '\n",
      " 'with significantly fewer parameters. Focuses on improving language models '\n",
      " 'through explicit memory at unprecedented scale. Publication Date: '\n",
      " '2021-12-08\\n'\n",
      " '\\n'\n",
      " '5. **PaLM: Scaling Language Modeling with Pathways**\\n'\n",
      " '   - Abstract: Trains a large Transformer language model, PaLM, with 540 '\n",
      " 'billion parameters using Pathways for efficient training. Achieves '\n",
      " 'state-of-the-art few-shot learning results on numerous language '\n",
      " 'understanding and generation benchmarks. Publication Date: 2022-04-05\\n'\n",
      " '\\n'\n",
      " '6. **OPT: Open Pre-trained Transformer Language Models**\\n'\n",
      " '   - Abstract: Introduces OPT, a suite of pre-trained transformers ranging '\n",
      " 'from 125M to 175B parameters, aiming to be shared with researchers. Compares '\n",
      " 'OPT-175B to GPT-3 and highlights its lower carbon footprint. Publication '\n",
      " 'Date: 2022-05-02\\n'\n",
      " '\\n'\n",
      " '7. **Few-shot Learning with Retrieval Augmented Language Models**\\n'\n",
      " '   - Abstract: Presents Atlas, a retrieval-augmented language model designed '\n",
      " 'for knowledge-intensive tasks with few training examples. Shows superior '\n",
      " 'performance on various tasks, including Natural Questions, with '\n",
      " 'significantly fewer parameters. Publication Date: 2022-08-05\\n'\n",
      " '\\n'\n",
      " '8. **PAL: Program-aided Language Models**\\n'\n",
      " '   - Abstract: Introduces PAL, a model that combines language models with a '\n",
      " 'symbolic interpreter for improved performance on mathematical, symbolic, and '\n",
      " 'algorithmic reasoning tasks. Demonstrates effective utilization of neural '\n",
      " 'LMs and interpreters for accurate problem-solving. Publication Date: '\n",
      " '2022-11-18\\n'\n",
      " '\\n'\n",
      " '9. **Unnatural Instructions: Tuning Language Models with (Almost) No Human '\n",
      " 'Labor**\\n'\n",
      " '   - Abstract: Presents a dataset of creative and diverse instructions '\n",
      " 'collected with minimal human labor to train language models. Shows that '\n",
      " 'training on these \"Unnatural Instructions\" rivals the effectiveness of '\n",
      " 'traditional datasets, enhancing model performance across various benchmarks. '\n",
      " 'Publication Date: 2022-12-19\\n'\n",
      " '\\n'\n",
      " '10. **Self-Instruct: Aligning Language Models with Self-Generated '\n",
      " 'Instructions**\\n'\n",
      " '    - Abstract: Proposes the Self-Instruct framework for improving '\n",
      " 'instruction-following capabilities of language models by using their own '\n",
      " 'generated instructions. Demonstrates significant performance improvements '\n",
      " 'over existing instruction-tuned models. Publication Date: 2022-12-20\\n'\n",
      " '\\n'\n",
      " '**2. Relevance to Toolformer Paper:**\\n'\n",
      " \"   - The paper 'How Can We Know What Language Models Know?' is mentioned in \"\n",
      " 'the context of Toolformer: Language Models Can Teach Themselves to Use '\n",
      " 'Tools. The context highlights the sensitivity of language models to the '\n",
      " 'exact wording of their input, emphasizing the importance of appropriate '\n",
      " \"prompts for effective API usage. This paper's intent is background, which \"\n",
      " 'aligns with the exploration of methods to estimate knowledge in language '\n",
      " \"models more accurately, particularly through better prompts. The paper's \"\n",
      " 'focus on improving accuracy in extracting relational knowledge aligns with '\n",
      " \"the theme of Toolformer, which aims to enhance language models' abilities by \"\n",
      " 'incorporating external tools via APIs.')\n"
     ]
    }
   ],
   "source": [
    "pprint(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1️⃣ Target paper의 introduction 파트에서 reference paper가 언급된 맥락 정보와, reference paper의 metadata 정보를 가져옴\n",
    "\n",
    "2️⃣ Target paper에서 reference paper가 언급된 이유에 대한 detail reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'paperId': '6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7',\n",
       "  'title': 'PAL: Program-aided Language Models',\n",
       "  'abstract': 'Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought\\'\\', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .',\n",
       "  'influentialCitationCount': 46,\n",
       "  'publicationDate': '2022-11-18',\n",
       "  'intent': 'methodology background result',\n",
       "  'context': '…(Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., 2022) and Python interpreters (Gao et al., 2022).'},\n",
       " {'paperId': '398e4061dde8f5c80606869cebfa2031de7b5b74',\n",
       "  'title': 'Few-shot Learning with Retrieval Augmented Language Models',\n",
       "  'abstract': 'Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.',\n",
       "  'influentialCitationCount': 46,\n",
       "  'publicationDate': '2022-08-05',\n",
       "  'intent': 'methodology background',\n",
       "  'context': 'Specifically, we use Atlas (Izacard et al., 2022), a retrieval-augmented LM finetuned on Natural Questions (Kwiatkowski et al.'},\n",
       " {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb',\n",
       "  'title': 'PaLM: Scaling Language Modeling with Pathways',\n",
       "  'abstract': 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.',\n",
       "  'influentialCitationCount': 301,\n",
       "  'publicationDate': '2022-04-05',\n",
       "  'intent': 'background',\n",
       "  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n",
       " {'paperId': '002c256d30d6be4b23d365a8de8ae0e67e4c9641',\n",
       "  'title': 'Improving language models by retrieving from trillions of tokens',\n",
       "  'abstract': 'We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.',\n",
       "  'influentialCitationCount': 53,\n",
       "  'publicationDate': '2021-12-08',\n",
       "  'intent': 'background',\n",
       "  'context': '…information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).'},\n",
       " {'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0',\n",
       "  'title': 'Language Models are Few-Shot Learners',\n",
       "  'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\",\n",
       "  'influentialCitationCount': 3057,\n",
       "  'publicationDate': '2020-05-28',\n",
       "  'intent': 'methodology background result',\n",
       "  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n",
       " {'paperId': 'a75649771901a4881b44c0ceafa469fcc6e6f968',\n",
       "  'title': 'How Can We Know What Language Models Know?',\n",
       "  'abstract': 'Abstract Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a __ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a __ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.',\n",
       "  'influentialCitationCount': 49,\n",
       "  'publicationDate': '2019-11-28',\n",
       "  'intent': 'background',\n",
       "  'context': '…to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero-and few-shot settings (Jiang et al., 2020; Schick and Schütze, 2021a).'},\n",
       " {'paperId': '99832586d55f540f603637e458a292406a0ed75d',\n",
       "  'title': 'LANGUAGE MODELS',\n",
       "  'abstract': 'While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.',\n",
       "  'influentialCitationCount': 124,\n",
       "  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n",
       "  'intent': 'background',\n",
       "  'context': '…ability to use external tools such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et…'},\n",
       " {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       "  'influentialCitationCount': 18064,\n",
       "  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n",
       "  'intent': 'background',\n",
       "  'context': 'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.'}]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 100\n",
    "threshold = 0.6\n",
    "recommend = None # 추천 개수 제한 X\n",
    "\n",
    "try:\n",
    "    # recommend 개의 가장 연관된 reference 추천\n",
    "    reference = reference_recommend(query=query, num=num, threshold=threshold, recommend=recommend, api_key=api_key, sorting=False)\n",
    "    print(len(reference))\n",
    "    # background와 연관된 reference 추천\n",
    "    background_ref = [ref for ref in reference if \"background\" in ref['intent'].split()]\n",
    "    print(len(background_ref))\n",
    "except:\n",
    "    # recommend 개의 가장 연관된 reference 추천\n",
    "    reference = reference_recommend(query=query, num=num, threshold=threshold, recommend=recommend, api_key=api_key, sorting=False)\n",
    "    print(len(reference))\n",
    "    # background와 연관된 reference 추천\n",
    "    background_ref = [ref for ref in reference if \"background\" in ref['intent'].split()]\n",
    "    print(len(background_ref))\n",
    "\n",
    "background_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded successfully: ../paper_data/Toolformer: Language Models Can Teach Themselves to Use Tools.pdf\n",
      "('Introduction Large language models achieve impressive zero- and few-shot '\n",
      " 'results on a variety of natural lan- guage processing tasks (Brown et al., '\n",
      " '2020; Chowd- hery et al., 2022, i.a.) and show several emergent capabilities '\n",
      " '(Wei et al., 2022). However, all of these models have several inherent '\n",
      " 'limitations that can at best be partially addressed by further scal- ing. '\n",
      " 'These limitations include an inability to access up-to-date information on '\n",
      " 'recent events (Komeili et al., 2022) and the related tendency to hallucinate '\n",
      " 'facts (Maynez et al., 2020; Ji et al., 2022), difﬁcul- ties in understanding '\n",
      " 'low-resource languages (Lin et al., 2021), a lack of mathematical skills to '\n",
      " 'per- form precise calculations (Patel et al., 2021) and an unawareness of '\n",
      " 'the progression of time (Dhingra et al., 2022). The New England Journal of '\n",
      " 'Medicine is a registered trademark of  [QA(“Who is the publisher of The New '\n",
      " 'England Journal of Medicine?”) → Massachusetts Medical Society]  the MMS. '\n",
      " 'Out of 1400 participants, 400 (or [Calculator(400 / 1400) → 0.29]  29%) '\n",
      " 'passed the test. The name derives from “la tortuga”, the Spanish word for '\n",
      " '[MT(“tortuga”) → turtle]  turtle. The Brown Act is California’s law  '\n",
      " '[WikiSearch(“Brown Act”) → The Ralph M. Brown Act is an act of the '\n",
      " \"California State Legislature that guarantees the public's right to attend \"\n",
      " 'and participate in meetings of local legislative bodies.]  that requires '\n",
      " 'legislative bodies, like city councils, to hold their meetings open to the '\n",
      " 'public. Figure 1: Exemplary predictions of Toolformer. The model '\n",
      " 'autonomously decides to call different APIs (from top to bottom: a question '\n",
      " 'answering system, a calculator, a machine translation system, and a '\n",
      " 'Wikipedia search engine) to obtain information that is useful for completing '\n",
      " 'a piece of text. A simple way to overcome these limitations of today’s '\n",
      " 'language models is to give them the abil- ity to use external tools such as '\n",
      " 'search engines, calculators, or calendars. However, existing ap- proaches '\n",
      " 'either rely on large amounts of human annotations (Komeili et al., 2022; '\n",
      " 'Thoppilan et al., 2022) or limit tool use to task-speciﬁc settings only '\n",
      " '(e.g., Gao et al., 2022; Parisi et al., 2022), hinder- ing a more widespread '\n",
      " 'adoption of tool use in LMs. Therefore, we propose Toolformer , a model that '\n",
      " 'learns to use tools in a novel way, which fulﬁlls the following desiderata: '\n",
      " '•The use of tools should be learned in a self-supervised way without '\n",
      " 'requiring large amounts of human annotations . This is '\n",
      " 'impor-arXiv:2302.04761v1  [cs.CL]  9 Feb 2023x1: i-1  = Pittsburgh is also '\n",
      " 'known as xi: n = the Steel City x* = Pittsburgh is also known as [QA(What …? '\n",
      " '→ Steel City)] the Steel City. ci1 = What other name is Pittsburgh known by? '\n",
      " 'ci2 = Which country is Pittsburgh in? ri1 = Steel City ri2 = United States '\n",
      " 'Li( ci1 → Steel City ) < min( Li( ci1 → ε), Li(ε)) Li( ci2 → United States ) '\n",
      " '> min( Li( ci2 → ε), Li(ε))1 Sample API Calls 2 Execute API Calls 3 Filter '\n",
      " 'API Calls LM Dataset LM Dataset with API Calls Figure 2: Key steps in our '\n",
      " 'approach, illustrated for a question answering tool: Given an input text x, '\n",
      " 'we ﬁrst sample a position iand corresponding API call candidates c1 i;c2 '\n",
      " 'i;:::;ck i. We then execute these API calls and ﬁlter out all calls which do '\n",
      " 'not reduce the loss Liover the next tokens. All remaining API calls are '\n",
      " 'interleaved with the original text, resulting in a new text x\\x03. tant not '\n",
      " 'only because of the costs associated with such annotations, but also because '\n",
      " 'what humans ﬁnd useful may be different from what a model ﬁnds useful. •The '\n",
      " 'LM should not lose any of its generality and should be able to decide for '\n",
      " 'itself when andhow to use which tool. In contrast to existing approaches, '\n",
      " 'this enables a much more comprehensive use of tools that is not tied to '\n",
      " 'speciﬁc tasks. Our approach for achieving these goals is based on the recent '\n",
      " 'idea of using large LMs with in- context learning (Brown et al., 2020) to '\n",
      " 'generate entire datasets from scratch (Schick and Schütze, 2021b; Honovich '\n",
      " 'et al., 2022; Wang et al., 2022): Given just a handful of human-written '\n",
      " 'examples of how an API can be used, we let a LM annotate a huge language '\n",
      " 'modeling dataset with potential API calls. We then use a self-supervised '\n",
      " 'loss to determine which of these API calls actually help the model in '\n",
      " 'predicting future tokens. Finally, we ﬁnetune the LM itself on the API calls '\n",
      " 'that it con- siders useful. As illustrated in Figure 1, through this simple '\n",
      " 'approach, LMs can learn to control a va- riety of tools, and to choose for '\n",
      " 'themselves which tool to use when and how. As our approach is agnostic of '\n",
      " 'the dataset be- ing used, we can apply it to the exact same dataset that was '\n",
      " 'used to pretrain a model in the ﬁrst place. This ensures that the model does '\n",
      " 'not lose any of its generality and language modeling abilities. We conduct '\n",
      " 'experiments on a variety of differ- ent downstream tasks, demonstrating that '\n",
      " 'after learning to use tools, Toolformer, which is based on a pretrained '\n",
      " 'GPT-J model (Wang and Komat- suzaki, 2021) with 6.7B parameters, achieves '\n",
      " 'much stronger zero-shot results, clearly outperforming a much larger GPT-3 '\n",
      " 'model (Brown et al., 2020) andseveral other baselines on various tasks.')\n"
     ]
    }
   ],
   "source": [
    "# 예제 사용\n",
    "arxiv_id = '2302.04761v1'\n",
    "# 사용하고자 하는 arXiv ID로 변경\n",
    "pdf_path = f\"../paper_data/{query}.pdf\"\n",
    "\n",
    "introduction = intro_pipeline(arxiv_id, pdf_path)\n",
    "pprint(introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 3779\n",
      "\tPrompt Tokens: 3191\n",
      "\tCompletion Tokens: 588\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0059625\n"
     ]
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "content = introduction\n",
    "\n",
    "# 논문 제목 (날짜순서로 정렬)\n",
    "ref_paper = background_ref[0]\n",
    "ref_paper_title = ref_paper['title']\n",
    "ref_paper_context = ref_paper['context']\n",
    "\n",
    "\n",
    "# 프롬프트\n",
    "system_template = \"\"\"\n",
    "    You are an AI research assistant tasked with creating structured reports.\n",
    "    Your sole purpose is to write well written, critically acclaimed.\n",
    "    objective and structured reports on given text.\n",
    "    \"\"\"\n",
    "human_template = \"\"\"\n",
    "    Content: Content of the paper we want to explore is as follows.\\n\n",
    "            1. paper we want to explore : {query},\\n\n",
    "            2. introduction section of the {query} : {content}.\\n\n",
    "            3. reference paper information : {ref_paper},\\n\\n\\n\n",
    "    Query: Answering below questions based on the Content.\\n\n",
    "            1. Is {ref_paper} mentioned in the introduction part of {query}? Answer Yes if {ref_paper_context} is mentioned in the introduction part of {query}, otherwise answer No.\\n\n",
    "            2. If so, why was it mentioned? Think about and infer how {ref_paper} was used in the introduction part\\n\n",
    "    \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    chain =  prompt | model | parser\n",
    "    report = chain.invoke({\n",
    "    \"content\": content,\n",
    "    \"ref_paper\": ref_paper,\n",
    "    \"ref_paper_title\": ref_paper_title,\n",
    "    \"ref_paper_context\": ref_paper_context,\n",
    "    \"query\": query\n",
    "    })\n",
    "print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('**Report on the Paper \"Toolformer: Language Models Can Teach Themselves to '\n",
      " 'Use Tools\"**\\n'\n",
      " '\\n'\n",
      " '**Introduction:**\\n'\n",
      " 'The paper \"Toolformer: Language Models Can Teach Themselves to Use Tools\" '\n",
      " 'discusses the limitations of current large language models in accessing '\n",
      " 'real-time information, performing precise calculations, and understanding '\n",
      " 'low-resource languages. To address these limitations, the authors propose '\n",
      " 'Toolformer, a model that learns to use external tools such as search engines '\n",
      " 'and calculators in a self-supervised manner. By giving language models the '\n",
      " 'ability to use tools, Toolformer aims to enhance their performance across '\n",
      " 'various natural language processing tasks.\\n'\n",
      " '\\n'\n",
      " '**Key Points:**\\n'\n",
      " '1. The introduction of Toolformer highlights the need for language models to '\n",
      " 'access external tools for improved performance.\\n'\n",
      " '2. Existing approaches either require extensive human annotations or limit '\n",
      " 'tool use to specific tasks, hindering widespread adoption in language '\n",
      " 'models.\\n'\n",
      " '3. Toolformer is introduced as a model that learns to use tools in a novel '\n",
      " 'way without the need for extensive human annotations.\\n'\n",
      " '4. The paper presents the methodology of Toolformer, which involves using '\n",
      " 'large language models with in-context learning to generate datasets with '\n",
      " 'potential API calls.\\n'\n",
      " '5. Through self-supervised learning, Toolformer enables language models to '\n",
      " 'control and choose the appropriate tool for a given task, leading to '\n",
      " 'improved performance in various downstream tasks.\\n'\n",
      " '6. Experimental results demonstrate that Toolformer, based on a pretrained '\n",
      " 'GPT-J model, outperforms larger models like GPT-3 on zero-shot tasks after '\n",
      " 'learning to use external tools.\\n'\n",
      " '\\n'\n",
      " '**Reference Paper \"PAL: Program-aided Language Models\":**\\n'\n",
      " 'The reference paper \"PAL: Program-aided Language Models\" discusses the use '\n",
      " 'of large language models in performing arithmetic and symbolic reasoning '\n",
      " 'tasks. It introduces a novel approach called Program-Aided Language models '\n",
      " '(PAL), where language models generate programs for reasoning tasks and '\n",
      " 'delegate the solution step to a runtime like a Python interpreter. This '\n",
      " 'synergy between neural language models and symbolic interpreters improves '\n",
      " 'accuracy in various mathematical and algorithmic reasoning tasks.\\n'\n",
      " '\\n'\n",
      " '**Answering Queries:**\\n'\n",
      " '1. The information from the reference paper \"PAL: Program-aided Language '\n",
      " 'Models\" is mentioned in the introduction part of the paper \"Toolformer: '\n",
      " 'Language Models Can Teach Themselves to Use Tools.\" Hence, the answer is '\n",
      " 'Yes.\\n'\n",
      " '2. The reference to the PAL paper in the introduction of Toolformer suggests '\n",
      " \"that the authors of Toolformer drew inspiration from PAL's approach of \"\n",
      " 'offloading the solution step to a runtime interpreter. By leveraging the '\n",
      " 'synergy between language models and interpreters for reasoning tasks, '\n",
      " 'Toolformer aims to enhance the logical and arithmetic capabilities of '\n",
      " 'language models without compromising their generality.\\n'\n",
      " '\\n'\n",
      " 'In conclusion, the paper \"Toolformer: Language Models Can Teach Themselves '\n",
      " 'to Use Tools\" presents a novel approach to enhance language models\\' '\n",
      " 'capabilities through the use of external tools, drawing inspiration from '\n",
      " 'prior work such as PAL. The proposed methodology shows promising results in '\n",
      " \"improving language models' performance across various tasks.\")\n"
     ]
    }
   ],
   "source": [
    "pprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
