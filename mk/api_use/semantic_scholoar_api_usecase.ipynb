{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/minkyuramen/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/minkyuramen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
    "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import IPython\n",
    "\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain.adapters.openai import convert_openai_messages\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datetime\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = '/Users/minkyuramen/Desktop/project/env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import semantic_scholoar_api as ss\n",
    "\n",
    "openai_api = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "# api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0',\n",
       "  'title': 'Language Models are Few-Shot Learners',\n",
       "  'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\",\n",
       "  'influentialCitationCount': 3000,\n",
       "  'publicationDate': '2020-05-28',\n",
       "  'intent': 'result methodology background',\n",
       "  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n",
       " {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb',\n",
       "  'title': 'PaLM: Scaling Language Modeling with Pathways',\n",
       "  'abstract': 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.',\n",
       "  'influentialCitationCount': 297,\n",
       "  'publicationDate': '2022-04-05',\n",
       "  'intent': 'background',\n",
       "  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n",
       " {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221',\n",
       "  'title': 'OPT: Open Pre-trained Transformer Language Models',\n",
       "  'abstract': 'Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.',\n",
       "  'influentialCitationCount': 262,\n",
       "  'publicationDate': '2022-05-02',\n",
       "  'intent': 'result',\n",
       "  'context': 'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'},\n",
       " {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       "  'influentialCitationCount': 17880,\n",
       "  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n",
       "  'intent': 'background',\n",
       "  'context': 'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.'},\n",
       " {'paperId': '9405cc0d6169988371b2755e573cc28650d14dfe',\n",
       "  'title': 'Language Models are Unsupervised Multitask Learners',\n",
       "  'abstract': 'Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.',\n",
       "  'influentialCitationCount': 2759,\n",
       "  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n",
       "  'intent': 'methodology',\n",
       "  'context': 'To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n",
    "num = 20\n",
    "threshold = 0.6\n",
    "recommend = 5\n",
    "\n",
    "# recommend 개의 가장 연관된 reference 추천\n",
    "reference = ss.reference_recommend(query=query, num=num, threshold=threshold, recommend=recommend, api_key=api_key)\n",
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Language Models are Few-Shot Learners': '2020-05-28',\n",
       " 'PaLM: Scaling Language Modeling with Pathways': '2022-04-05',\n",
       " 'OPT: Open Pre-trained Transformer Language Models': '2022-05-02',\n",
       " 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n",
       " 'Language Models are Unsupervised Multitask Learners': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ref['title'] : ref['publicationDate'] for ref in reference}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 2521\n",
      "\tPrompt Tokens: 2099\n",
      "\tCompletion Tokens: 422\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0039925\n"
     ]
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "content = reference\n",
    "\n",
    "# 논문 제목 (날짜순서로 정렬)\n",
    "ref_paper = list({ref['title'] : ref['publicationDate'] for ref in reference}.keys())[0]\n",
    "\n",
    "# 프롬프트\n",
    "system_template = \"\"\"\n",
    "    You are an AI research assistant tasked with creating structured reports.\n",
    "    Your sole purpose is to write well written, critically acclaimed.\n",
    "    objective and structured reports on given text.\n",
    "    \"\"\"\n",
    "human_template = \"\"\"\n",
    "    Content: {content}\n",
    "    Query: Answering below questions based on the content.\n",
    "            1. Explain why the {ref_paper} was mentioned in {query}. using \"intent\" & \"context part\n",
    "            2. Briefly explain the abstract of the paper {ref_paper}\n",
    "    \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    chain =  prompt | model | parser\n",
    "    report = chain.invoke({\n",
    "    \"content\": content,\n",
    "    \"ref_paper\": ref_paper,\n",
    "    \"query\": query\n",
    "    })\n",
    "print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('**Report:**\\n'\n",
      " '\\n'\n",
      " '### 1. Context and Intent Analysis:\\n'\n",
      " 'The paper \"Language Models are Few-Shot Learners\" is referenced in the '\n",
      " 'article \"Toolformer: Language Models Can Teach Themselves to Use Tools\" due '\n",
      " 'to its intent and context. The intent of the paper is categorized as \"result '\n",
      " 'methodology background,\" indicating that it presents results, methodologies, '\n",
      " \"and background information related to language models' few-shot learning \"\n",
      " 'capabilities. The context part mentioned in the content highlights that '\n",
      " 'large language models have achieved impressive zero and few-shot results on '\n",
      " 'various natural language processing tasks. By referencing the paper in the '\n",
      " 'Toolformer article, it signifies the importance of leveraging language '\n",
      " \"models' few-shot learning abilities in teaching themselves to use tools \"\n",
      " 'efficiently.\\n'\n",
      " '\\n'\n",
      " '### 2. Abstract Summary of \"Language Models are Few-Shot Learners\":\\n'\n",
      " 'The abstract of the paper \"Language Models are Few-Shot Learners\" discusses '\n",
      " 'the significant advancements made in Natural Language Processing (NLP) tasks '\n",
      " 'by pre-training large language models followed by fine-tuning on specific '\n",
      " 'tasks. The paper emphasizes the challenge faced by current NLP systems in '\n",
      " 'performing new language tasks with minimal examples, unlike human '\n",
      " 'capabilities. It introduces GPT-3, an autoregressive language model with 175 '\n",
      " 'billion parameters, which significantly improves task-agnostic few-shot '\n",
      " 'performance. GPT-3 demonstrates strong performance across various NLP '\n",
      " 'datasets, including translation, question-answering, cloze tasks, reasoning '\n",
      " 'tasks, and domain adaptation tasks. Additionally, the paper highlights areas '\n",
      " 'where GPT-3 struggles in few-shot learning and methodological issues it '\n",
      " \"faces. The abstract concludes by discussing the societal impacts of GPT-3's \"\n",
      " 'capabilities, including generating news articles indistinguishable from '\n",
      " 'human-written ones.\\n'\n",
      " '\\n'\n",
      " 'Overall, the abstract of \"Language Models are Few-Shot Learners\" underscores '\n",
      " 'the transformative potential of scaling up language models in enhancing '\n",
      " 'few-shot learning performance across diverse NLP tasks and the broader '\n",
      " 'implications of such advancements in the field.\\n'\n",
      " '\\n'\n",
      " 'This report provides a detailed analysis of the context and intent behind '\n",
      " 'referencing the paper and a concise summary of the abstract of \"Language '\n",
      " 'Models are Few-Shot Learners.\"')\n"
     ]
    }
   ],
   "source": [
    "pprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '7d8905a1fd288068f12c8347caeabefd36d0dd6c',\n",
       "  'title': 'Gorilla: Large Language Model Connected with Massive APIs',\n",
       "  'abstract': \"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\",\n",
       "  'influentialCitationCount': 25,\n",
       "  'publicationDate': '2023-05-24'},\n",
       " {'paperId': '58f8925a8b87054ad0635a6398a7fe24935b1604',\n",
       "  'title': 'Mind2Web: Towards a Generalist Agent for the Web',\n",
       "  'abstract': 'We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.',\n",
       "  'influentialCitationCount': 22,\n",
       "  'publicationDate': '2023-06-09'},\n",
       " {'paperId': '0bfc804e31eecfd77f45e4ee7f4d629fffdcd628',\n",
       "  'title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs',\n",
       "  'abstract': 'Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.',\n",
       "  'influentialCitationCount': 39,\n",
       "  'publicationDate': '2023-07-31'},\n",
       " {'paperId': '5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0',\n",
       "  'title': 'Qwen Technical Report',\n",
       "  'abstract': 'Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.',\n",
       "  'influentialCitationCount': 46,\n",
       "  'publicationDate': '2023-09-28'},\n",
       " {'paperId': 'd1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43',\n",
       "  'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face',\n",
       "  'abstract': 'Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.',\n",
       "  'influentialCitationCount': 49,\n",
       "  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n",
    "num = 20\n",
    "threshold = 0.6\n",
    "recommend = 5\n",
    "\n",
    "# recommend 개의 가장 연관된 citation 추천\n",
    "citation = ss.citation_recommend(query=query, num=num, threshold=threshold, recommend=recommend, api_key=api_key)\n",
    "citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gorilla: Large Language Model Connected with Massive APIs': '2023-05-24',\n",
       " 'Mind2Web: Towards a Generalist Agent for the Web': '2023-06-09',\n",
       " 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs': '2023-07-31',\n",
       " 'Qwen Technical Report': '2023-09-28',\n",
       " 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{cit['title'] : cit['publicationDate'] for cit in citation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 2801\n",
      "\tPrompt Tokens: 2107\n",
      "\tCompletion Tokens: 694\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0045485000000000005\n"
     ]
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "content = citation\n",
    "\n",
    "# 논문 제목 (날짜순서로 정렬)\n",
    "cit_paper = list({cit['title'] : cit['publicationDate'] for cit in citation}.keys())\n",
    "\n",
    "# 프롬프트\n",
    "system_template = \"\"\"\n",
    "    You are an AI research assistant tasked with creating structured reports.\n",
    "    Your sole purpose is to write well written, critically acclaimed.\n",
    "    objective and structured reports on given text.\n",
    "    \"\"\"\n",
    "human_template = \"\"\"\n",
    "    Content: {content}\n",
    "    Query: Answering below questions based on the content.\n",
    "            1. Explain why the {query} was mententioned in {cit_paper}.\n",
    "            2. Briefly explain the abstract of the paper {cit_paper}\n",
    "    \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    chain =  prompt | model | parser\n",
    "    report = chain.invoke({\n",
    "    \"content\": content,\n",
    "    \"cit_paper\": cit_paper,\n",
    "    \"query\": query\n",
    "    })\n",
    "print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('**Report on Language Models and Tool Use in AI Research**\\n'\n",
      " '\\n'\n",
      " '**1. Mention of Toolformer: Language Models Can Teach Themselves to Use '\n",
      " 'Tools**\\n'\n",
      " '\\n'\n",
      " 'The paper \"Toolformer: Language Models Can Teach Themselves to Use Tools\" '\n",
      " \"was likely mentioned in the context of the papers ['Gorilla: Large Language \"\n",
      " \"Model Connected with Massive APIs', 'Mind2Web: Towards a Generalist Agent \"\n",
      " \"for the Web', 'ToolLLM: Facilitating Large Language Models to Master 16000+ \"\n",
      " \"Real-world APIs', 'Qwen Technical Report', 'HuggingGPT: Solving AI Tasks \"\n",
      " \"with ChatGPT and its Friends in Hugging Face'] due to its relevance in \"\n",
      " 'addressing the challenges faced by large language models (LLMs) in '\n",
      " 'effectively utilizing tools via API calls. The Toolformer paper may have '\n",
      " 'provided insights or techniques on how LLMs can self-teach to improve their '\n",
      " \"tool-use capabilities, which aligns with the theme of enhancing LLMs' \"\n",
      " 'interactions with external tools and APIs as highlighted in the mentioned '\n",
      " 'papers.\\n'\n",
      " '\\n'\n",
      " '**2. Abstract Summaries of Selected Papers**\\n'\n",
      " '\\n'\n",
      " '- **Gorilla: Large Language Model Connected with Massive APIs**: The paper '\n",
      " 'introduces Gorilla, a finetuned model based on LLaMA that outperforms GPT-4 '\n",
      " 'in writing API calls. Gorilla, when combined with a document retriever, '\n",
      " 'shows adaptability to test-time document changes and mitigates hallucination '\n",
      " \"issues. The model's performance is evaluated on the APIBench dataset, \"\n",
      " 'showcasing its potential in accurately using tools via API calls.\\n'\n",
      " '\\n'\n",
      " '- **Mind2Web: Towards a Generalist Agent for the Web**: Mind2Web presents a '\n",
      " 'dataset for developing generalist agents for the web, focusing on real-world '\n",
      " 'websites and diverse tasks. The paper explores using LLMs for building web '\n",
      " 'agents, emphasizing the importance of filtering raw HTML data to enhance LLM '\n",
      " 'performance. While showing decent performance, there is room for improvement '\n",
      " 'towards achieving truly generalizable web agents.\\n'\n",
      " '\\n'\n",
      " '- **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world '\n",
      " 'APIs**: ToolLLM addresses the limitations of LLMs in utilizing real-world '\n",
      " 'APIs effectively. The paper introduces ToolBench, a dataset for '\n",
      " 'instruction-tuning in tool use, and presents a framework for training LLMs '\n",
      " 'with enhanced reasoning capabilities for API utilization. ToolLLaMA, a '\n",
      " 'fine-tuned LLM, demonstrates strong tool-use capabilities and zero-shot '\n",
      " 'generalization on unseen APIs.\\n'\n",
      " '\\n'\n",
      " '- **Qwen Technical Report**: Qwen introduces a series of language models '\n",
      " 'with varying parameter counts, showcasing superior performance across '\n",
      " 'diverse tasks. The models, including chat models finetuned with human '\n",
      " 'alignment techniques, exhibit advanced tool-use and planning capabilities '\n",
      " 'for agent applications. Specialized models for coding and mathematics tasks '\n",
      " 'are also developed, showing improved performance compared to existing '\n",
      " 'models.\\n'\n",
      " '\\n'\n",
      " '- **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging '\n",
      " 'Face**: The paper presents HuggingGPT, an LLM-powered agent that leverages '\n",
      " 'LLMs like ChatGPT to connect various AI models for solving complex AI tasks. '\n",
      " 'By using language as a generic interface, HuggingGPT demonstrates the '\n",
      " 'ability to manage and coordinate different AI models to achieve impressive '\n",
      " 'results across various domains and modalities, contributing towards '\n",
      " 'artificial general intelligence research.\\n'\n",
      " '\\n'\n",
      " 'These abstract summaries provide insights into the key contributions and '\n",
      " 'focus areas of each paper in advancing the capabilities of large language '\n",
      " 'models in diverse applications within the field of artificial intelligence.')\n"
     ]
    }
   ],
   "source": [
    "pprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
