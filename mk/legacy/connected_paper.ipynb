{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1RSM1REUfvYV4_NFVCucHmmtBm_jIxMIu","authorship_tag":"ABX9TyM9mqmgSxS2R2iODongds1P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.tokenize import word_tokenize\n","import numpy as np"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iNfucVyzOik9","executionInfo":{"status":"ok","timestamp":1713951015569,"user_tz":-540,"elapsed":3241,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"8b562d55-7ad1-4aa2-d9f8-992d14d8325f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","source":["# PRACTICE"],"metadata":{"id":"pEF0EeOqHgPy"}},{"cell_type":"code","source":["import requests\n","\n","# Define the API endpoint URL\n","url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n","\n","# More specific query parameter\n","query_params = {'query': 'Toolformer'}\n","\n","# semantic scholarship api 넣은\n","api_key = ''\n","headers = {'x-api-key': api_key}\n","\n","# Send the API request\n","response = requests.get(url, params=query_params, headers=headers)\n","\n","# Check response status\n","if response.status_code == 200:\n","   response_data = response.json()\n","   print(response_data)\n","   # Process and print the response data as needed\n","else:\n","   print(f\"Request failed with status code {response.status_code}: {response.text}\")\n","response_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s95bzdF3BDVM","executionInfo":{"status":"ok","timestamp":1713953289046,"user_tz":-540,"elapsed":654,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"de7ce72d-f35d-4198-c3fd-69207bf734d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'total': 4, 'offset': 0, 'data': [{'paperId': '53d128ea815bcc0526856eb5a9c42cc977cb36a7', 'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools'}, {'paperId': '0d502a1e300336ae628f5c8b99ee4d3766c8f60b', 'title': 'Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT'}, {'paperId': '64e30319758596f98d3b53c2a45ea1f493799d24', 'title': 'Demo: Certified Robustness on Toolformer'}, {'paperId': '8fb92f51434543c4a8cd4980f84cf04552c712cc', 'title': 'Efficient Tool Use with Chain-of-Abstraction Reasoning'}]}\n"]},{"output_type":"execute_result","data":{"text/plain":["{'total': 4,\n"," 'offset': 0,\n"," 'data': [{'paperId': '53d128ea815bcc0526856eb5a9c42cc977cb36a7',\n","   'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools'},\n","  {'paperId': '0d502a1e300336ae628f5c8b99ee4d3766c8f60b',\n","   'title': 'Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT'},\n","  {'paperId': '64e30319758596f98d3b53c2a45ea1f493799d24',\n","   'title': 'Demo: Certified Robustness on Toolformer'},\n","  {'paperId': '8fb92f51434543c4a8cd4980f84cf04552c712cc',\n","   'title': 'Efficient Tool Use with Chain-of-Abstraction Reasoning'}]}"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["paper_id = response_data['data'][0]['paperId']\n","paper_id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"UTaEHMWdCI2D","executionInfo":{"status":"ok","timestamp":1713953292158,"user_tz":-540,"elapsed":4,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"266455ff-a1cb-4c1c-c294-210045f2794a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'53d128ea815bcc0526856eb5a9c42cc977cb36a7'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Define the paper search endpoint URL\n","url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n","\n","# Define the required query parameter and its value (in this case, the keyword we want to search for)\n","query_params = {\n","    'query': 'semantic scholar platform',\n","    'limit': 3\n","}\n","\n","# Define a separate function to make a request to the paper details endpoint using a paper_id. This function will be used later on (after we call the paper search endpoint).\n","def get_paper_data(paper_id):\n","  url = 'https://api.semanticscholar.org/graph/v1/paper/' + paper_id\n","\n","  # Define which details about the paper you would like to receive in the response\n","  paper_data_query_params = {'fields': 'title,year,abstract,authors.name'}\n","\n","  # Send the API request and store the response in a variable\n","  response = requests.get(url, params=paper_data_query_params, headers=headers)\n","  if response.status_code == 200:\n","    return response.json()\n","  else:\n","    return None\n","\n","# Make the GET request to the paper search endpoint with the URL and query parameters\n","headers = {'x-api-key': api_key}\n","search_response = requests.get(url, params=query_params, headers = headers)\n","\n","# Check if the request was successful (status code 200)\n","if search_response.status_code == 200:\n","  search_response = search_response.json()\n","\n","  # Retrieve the paper id corresponding to the 1st result in the list\n","  paper_id = search_response['data'][0]['paperId']\n","\n","  # Retrieve the paper details corresponding to this paper id using the function we defined earlier.\n","  paper_details = get_paper_data(paper_id)\n","\n","  # Check if paper_details is not None before proceeding\n","  if paper_details is not None:\n","    print('paper_details')\n","\n","    # Your code to work with the paper details goes here\n","\n","  else:\n","    print(\"Failed to retrieve paper details.\")\n","\n","else:\n","  # Handle potential errors or non-200 responses\n","  print(f\"Relevance Search Request failed with status code {search_response.status_code}: {search_response.text}\")\n","\n","paper_details"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUsy17xhEUAa","executionInfo":{"status":"ok","timestamp":1713953294790,"user_tz":-540,"elapsed":1390,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"538be737-6679-4e66-9183-6e4cb4b2a134"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["paper_details\n"]},{"output_type":"execute_result","data":{"text/plain":["{'paperId': 'cb92a7f9d9dbcf9145e32fdfa0e70e2a6b828eb1',\n"," 'title': 'The Semantic Scholar Open Data Platform',\n"," 'abstract': 'The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-theart techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will update this living document to reflect changes as we add new data offerings and improve existing services.',\n"," 'year': 2023,\n"," 'authors': [{'authorId': '143967880', 'name': 'Rodney Michael Kinney'},\n","  {'authorId': '1667818755', 'name': 'Chloe Anastasiades'},\n","  {'authorId': '2202417686', 'name': 'Russell Authur'},\n","  {'authorId': '46181066', 'name': 'Iz Beltagy'},\n","  {'authorId': '2699105', 'name': 'Jonathan Bragg'},\n","  {'authorId': '2202412440', 'name': 'Alexandra Buraczynski'},\n","  {'authorId': '51199773', 'name': 'Isabel Cachola'},\n","  {'authorId': '2202412446', 'name': 'Stefan Candra'},\n","  {'authorId': '1648642525', 'name': 'Yoganand Chandrasekhar'},\n","  {'authorId': '2527954', 'name': 'Arman Cohan'},\n","  {'authorId': '46230609', 'name': 'Miles Crawford'},\n","  {'authorId': '145612610', 'name': 'Doug Downey'},\n","  {'authorId': '38092776', 'name': 'Jason Dunkelberger'},\n","  {'authorId': '1741101', 'name': 'Oren Etzioni'},\n","  {'authorId': '79519990', 'name': 'Rob Evans'},\n","  {'authorId': '46411828', 'name': 'Sergey Feldman'},\n","  {'authorId': '2202417961', 'name': 'Joseph Gorney'},\n","  {'authorId': '2052859168', 'name': 'D. Graham'},\n","  {'authorId': '2200023814', 'name': 'F.Q. Hu'},\n","  {'authorId': '153179615', 'name': 'Regan Huff'},\n","  {'authorId': '145104486', 'name': 'Daniel King'},\n","  {'authorId': '41018147', 'name': 'Sebastian Kohlmeier'},\n","  {'authorId': '2003338023', 'name': 'Bailey Kuehl'},\n","  {'authorId': '48758753', 'name': 'Michael Langan'},\n","  {'authorId': '2116442078', 'name': 'Daniel Lin'},\n","  {'authorId': '2143857321', 'name': 'Haokun Liu'},\n","  {'authorId': '46258841', 'name': 'Kyle Lo'},\n","  {'authorId': '3047023', 'name': 'Jaron Lochner'},\n","  {'authorId': '2071601396', 'name': 'Kelsey MacMillan'},\n","  {'authorId': '2057256191', 'name': 'Tyler Murray'},\n","  {'authorId': '145541350', 'name': 'Christopher Newell'},\n","  {'authorId': '2115660042', 'name': 'Smita R Rao'},\n","  {'authorId': '40408676', 'name': 'Shaurya Rohatgi'},\n","  {'authorId': '114609552', 'name': 'Paul Sayre'},\n","  {'authorId': '101568984', 'name': 'Zejiang Shen'},\n","  {'authorId': '2116287642', 'name': 'Amanpreet Singh'},\n","  {'authorId': '3328733', 'name': 'Luca Soldaini'},\n","  {'authorId': '48813613', 'name': 'Shivashankar Subramanian'},\n","  {'authorId': '2125122431', 'name': 'A. Tanaka'},\n","  {'authorId': '1860983', 'name': 'Alex D Wade'},\n","  {'authorId': '82676859', 'name': 'Linda M. Wagner'},\n","  {'authorId': '31860505', 'name': 'Lucy Lu Wang'},\n","  {'authorId': '46212260', 'name': 'Christopher Wilhelm'},\n","  {'authorId': '2109360564', 'name': 'Caroline Wu'},\n","  {'authorId': '82148460', 'name': 'Jiangjiang Yang'},\n","  {'authorId': '98442688', 'name': 'Angele Zamarron'},\n","  {'authorId': '15292561', 'name': 'Madeleine van Zuylen'},\n","  {'authorId': '1780531', 'name': 'Daniel S. Weld'}]}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import requests\n","\n","# Define the base URL for the API\n","base_url = \"https://api.semanticscholar.org/recommendations/v1/papers/forpaper/\"\n","\n","# Define the paperId\n","paperId = paper_id\n","\n","# Construct the full URL with the paperId as a path parameter\n","url = base_url + paperId\n","\n","# Send a GET request to the URL\n","response = requests.get(url)\n","\n","# Check if the request was successful\n","if response.status_code == 200:\n","    # Parse the JSON response\n","    data = response.json()\n","\n","    # Extract the list of recommended papers from the response\n","    recommended_papers = data.get(\"recommendedPapers\", [])\n","\n","    # Your code to work with the recommended papers list goes here\n","\n","else:\n","    # Handle the error, e.g., print an error message\n","    print(f\"Request failed with status code {response.status_code}\")"],"metadata":{"id":"BH97V7vdEXO_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommended_papers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tvdz2_MUGCzv","executionInfo":{"status":"ok","timestamp":1713953316851,"user_tz":-540,"elapsed":4,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"ae745035-125d-488e-c279-e46483ac942e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'paperId': '6d66d55bd147f3f44830e86b4514da27aa00570f',\n","  'title': 'pyKCN: A Python Tool for Bridging Scientific Knowledge'},\n"," {'paperId': 'd102624b80f03555a95c202ff85dc3aad87f4c29',\n","  'title': 'Toward FAIR Semantic Publishing of Research Dataset Metadata in the Open Research Knowledge Graph'},\n"," {'paperId': '3efa931b7e245eafe7c2ac20a905d0052f656048',\n","  'title': 'BOLD: Knowledge Graph Exploration and Analysis Platform'},\n"," {'paperId': '59f4b2103d6a78ebad51db8bd603d4c5571c0cbf',\n","  'title': 'Citation Intent Classification Through Weakly Supervised Knowledge Graphs'},\n"," {'paperId': '83529b3d040fb5f889c53c4d33a281825a60c332',\n","  'title': 'Building and Exploiting a Web of Machine-Readable Scientific Facts to Make Discoveries'},\n"," {'paperId': 'e3969f02744c9ab994d7bb34b1a91d0d7405fa51',\n","  'title': 'Construction of Functional Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model'},\n"," {'paperId': '79068ce4a4467dd0749e5c11b145df369f4edb95',\n","  'title': 'A web-based platform for extracting and modeling knowledge from biomedical literature as a labeled graph.'},\n"," {'paperId': 'f513e1fc1c12cbfb2e2a50ce433f80299d9a7e38',\n","  'title': 'Enriching the Metadata of Community-Generated Digital Content through Entity Linking: An Evaluative Comparison of State-of-the-Art Models'},\n"," {'paperId': 'eee6b3f6d60ee8a471db3db48f0df657fe0c1efd',\n","  'title': 'AceMap: Knowledge Discovery through Academic Graph'},\n"," {'paperId': '0fd55e413b587d3b090093d56665e472da2f63de',\n","  'title': 'Comparative Study of Domain Driven Terms Extraction Using Large Language Models'},\n"," {'paperId': 'f29ed645198138af2c4d2e9122b2834ee8bb5b4e',\n","  'title': 'A Bibliometric Analysis of Recent Developments and Trends in Knowledge Graph Research (2013–2022)'},\n"," {'paperId': '369167166539257852eb89d0ebb47ea703dfdb8c',\n","  'title': 'KG-CTG: Citation Generation Through Knowledge Graph-Guided Large Language Models'},\n"," {'paperId': '38fe0464af2ccc8db9ca3edc6ba1421931f645b2',\n","  'title': 'Improving On-line Scientific Resource Profiling by Exploiting Resource Citation Information in the Literature'},\n"," {'paperId': '200db5ae725052bce474771f5bc1dfe2e59a1608',\n","  'title': 'The Integration of the Japan Link Center’s Bibliographic Data into OpenCitations'},\n"," {'paperId': '3402fece99a850aa96c73355e30d601397f3c7f6',\n","  'title': 'Analysis on open data as a foundation for data-driven research'},\n"," {'paperId': 'cad5ba148482e2b4c66dce97ed682423c49e9115',\n","  'title': 'GPDminer: a tool for extracting named entities and analyzing relations in biological literature'},\n"," {'paperId': 'f62acbdbb9ae49bfaf20ba652f84e99d884c4f19',\n","  'title': 'Use of a Structured Knowledge Base Enhances Metadata Curation by Large Language Models'},\n"," {'paperId': '7cd8e00379628a07b7f383a2e381ee0da8783657',\n","  'title': 'in 26 Billion RDF Triples'},\n"," {'paperId': '3537dd1503e2db49824a62c276e2fbf387bf7bb2',\n","  'title': 'A Framework to Generate, Store, and Publish FAIR Data in Experimental Sciences'},\n"," {'paperId': '0d5e963a1aba858e6d1b29c25788b04eb57b775a',\n","  'title': 'NLP Scholar: A Dataset for Examining the State of NLP Research'},\n"," {'paperId': '0d5e963a1aba858e6d1b29c25788b04eb57b775a',\n","  'title': 'NLP Scholar: A Dataset for Examining the State of NLP Research'},\n"," {'paperId': '3a5cd0ec2e1b74260e746a17cd11abb6af4c218c',\n","  'title': 'SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model'},\n"," {'paperId': '86b94e022943f41e47febe1d6ebb26a4f9a62f57',\n","  'title': 'Global status of dataset repositories at a glance: study based on OpenDOAR'},\n"," {'paperId': 'c5fc83256c2c7bc5081061932471957361546857',\n","  'title': 'BIOINFORMATICS ORIGINAL'},\n"," {'paperId': 'fa4ade5272a7907dc0b3bee5fd8bc32b55b1a360',\n","  'title': 'Graph-based Named Entity Information Retrieval from News Articles using Neo4j'},\n"," {'paperId': '7151439e6e9ef5b1f1855cf192980ae7dec94fbb',\n","  'title': 'Analysis and Modelling of Structured Data with Automatic Data Analysis Web Application'},\n"," {'paperId': '71024999159a83199704359df63ce1ad7ecb79ab',\n","  'title': 'WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations'},\n"," {'paperId': '4b2006ed71df44b8cb038cac62776e6bd6ad6ce4',\n","  'title': 'The Future of Web Data Mining: Insights from Multimodal and Code-based Extraction Methods'},\n"," {'paperId': 'e37d95aa0a8350a460e34ad367d155704ef3e0c4',\n","  'title': 'Applying a Context-based Method to Build a Knowledge Graph for the Blue Amazon'},\n"," {'paperId': '4d59251e26b020d2b80aae3b170a704a73f21e63',\n","  'title': 'Research Online Research'},\n"," {'paperId': '1527e9319945a6cdb9c0d77222d0031699ea90ce',\n","  'title': 'Evolutionary ﬁne-tuning of automated semantic annotation systems'},\n"," {'paperId': 'e7c247c86f2f9e6fee922918db25a723cc106818',\n","  'title': 'Talkoot: Drupal Extensions to Create Online Collaborative Portals for Earth Science'},\n"," {'paperId': 'a3447f393cb9871fab4536e7d1a35d776c188470',\n","  'title': 'in Computer Science and Software Engineering'},\n"," {'paperId': 'a3447f393cb9871fab4536e7d1a35d776c188470',\n","  'title': 'in Computer Science and Software Engineering'},\n"," {'paperId': '006134d7d87ad8ad5836c20cac57b326da632672',\n","  'title': 'S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document'},\n"," {'paperId': '3bce212d28a200c52973f55fde5def60145d0e90',\n","  'title': 'Learning Pattern-Based Extractors from Natural Language and Knowledge Graphs: Applying Large Language Models to Wikipedia and Linked Open Data'},\n"," {'paperId': '6d11e241e0c46e48a64d1e2e07248db6b7cf5468',\n","  'title': 'From Detection to Application: Recent Advances in Understanding Scientific Tables\\xa0and Figures'},\n"," {'paperId': '8c7c6a05f6d124406e42022c0ac5f059ee270a19',\n","  'title': 'Databases and Ontologies Identifying Informative Subsets of the Gene Ontology with Information Bottleneck Methods'},\n"," {'paperId': 'b7b7d84f0bac0d2ab35ae5eef3c57f582bc8593d',\n","  'title': 'Theoretical and practical advances in genome halving'},\n"," {'paperId': '8c7c6a05f6d124406e42022c0ac5f059ee270a19',\n","  'title': 'Databases and Ontologies Identifying Informative Subsets of the Gene Ontology with Information Bottleneck Methods'},\n"," {'paperId': '4c42d8077db4e09936818aed0efa9dff5cf23e5d',\n","  'title': 'Related Work and Citation Text Generation: A Survey'},\n"," {'paperId': '9d0fe34789dcef68dda8212dfa2a20daae8dd146',\n","  'title': 'PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge.'},\n"," {'paperId': 'a99eaf65b778ecaa6ed34cc78c1c24adca9126db',\n","  'title': 'Integration of automatic spatial annotations from different sources by means of semantic technologies'},\n"," {'paperId': 'e5fa346cf96c8f59b0dc2dc63b515cbe568f09f7',\n","  'title': 'Towards Complex Ontology Alignment using Large Language Models'},\n"," {'paperId': 'a81c324e1972ba516282a5fdbf98000783e17319',\n","  'title': 'Persistent homology centrality improves link prediction performance in Pubmed co-occurrence networks'},\n"," {'paperId': '9ca0d6739c3242472d57f84e795512a8955a4380',\n","  'title': 'FAIR Jupyter: a knowledge graph approach to semantic sharing and granular exploration of a computational notebook reproducibility dataset'},\n"," {'paperId': 'b9587bcb80cdc8b3b9289392f5b6a459521fe7bd',\n","  'title': 'Duplication-Driven Distributional Topic Modeling: A Catalyst for Strengthened Classification and Semantic Graphs'},\n"," {'paperId': '683b90e9c4d8d7a756610a5a69c260176d1d9e2b',\n","  'title': 'A Bibliometric Analysis of Text Mining: Exploring the Use of Natural Language Processing in Social Media Research'},\n"," {'paperId': 'e1a294a8fcd223ef1ac06fec213d7cfaef88a07f',\n","  'title': 'A Language Model based Framework for New Concept Placement in Ontologies'},\n"," {'paperId': '7d8c11c9f34ad88e918b7de44cc8972745d3c173',\n","  'title': 'Search Engine for Open Geospatial Consortium Web Services Improving Discoverability through Natural Language Processing-Based Processing and Ranking'},\n"," {'paperId': 'ab3716938873cbcbe726e9f575ec2ba26bb7f379',\n","  'title': 'AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models'},\n"," {'paperId': '7c2f9dfd0fc233411aee01239f818374d515246a',\n","  'title': 'RealKIE: Five Novel Datasets for Enterprise Key Information Extraction'},\n"," {'paperId': 'b2567b9260b3b132e14bbe406d85c0774b3cc38c',\n","  'title': 'Assessing the quality of information extraction'},\n"," {'paperId': '0c8c9c910f6b7a5aa229db1f1c43b25612860ded',\n","  'title': 'Thesaurus of Modern Slovene 2.0'},\n"," {'paperId': 'e7d952f3c655ccd83d89a629eb1e9820976a175d',\n","  'title': 'Data in Brief'},\n"," {'paperId': 'efe164813ac60675aa9d1a98881cc8cc28517a4a',\n","  'title': 'Expert Systems With Applications'},\n"," {'paperId': '5775943977c5c525e1093a1070ea1bd50b84f6c6',\n","  'title': 'WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction'},\n"," {'paperId': '6dac27d84274447cb29b9fcac642d99fc4674c61',\n","  'title': 'srahunter: A User-Friendly Tool for Efficient Retrieval and Management of SRA Data'},\n"," {'paperId': 'fc41cdb6fb713a98e033e265bc2107f97f9eacee',\n","  'title': 'Publishing CoreKB Facts as Nanopublications'},\n"," {'paperId': 'a5f9623cde00f0152cc6fbc57ab2fd77372aae62',\n","  'title': 'Building a Data Warehouse for Social Media: Review and Comparison'},\n"," {'paperId': '0b87b928e898299859906cf8cb107b4e2df979b9',\n","  'title': 'Advancing Biomedical Text Mining with Community Challenges'},\n"," {'paperId': '184753b614b35cde3f2e221cea3bc60fe016d29e',\n","  'title': 'Editing Conceptual Knowledge for Large Language Models'},\n"," {'paperId': '0b0f7a9ac90a44cc04c8e0a3baf9cbc5d71de69d',\n","  'title': 'Comparing Research Topics through Metatags Analysis: A Multi-module Machine Algorithm Approaches Using Real World Data on Digital Humanities'},\n"," {'paperId': 'd543b7e6192a2aae348f3f3fefd4464446d5e46d',\n","  'title': 'Q-KGSWS: Querying the Hybrid Framework of Knowledge Graph and Semantic Web Services for Service Discovery'},\n"," {'paperId': '2e5c62d0fbe16965cc5b1a14274170404ce6a7a1',\n","  'title': 'Approaches to Inference Search in the Ontological Knowledge Base'},\n"," {'paperId': 'fe99095df95638b6768e1c10682034d25353ed89',\n","  'title': 'MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining'},\n"," {'paperId': '51d7c74c28d8015015f33b994bd9fe8f1ca415f4',\n","  'title': 'Author name disambiguation literature review with consolidated meta-analytic approach'},\n"," {'paperId': 'd68e93cad6a038f233cba5c7f3ca5448a1744d55',\n","  'title': 'WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset'},\n"," {'paperId': '5ec8fc786373a9976f2af6df66b72330e2a2eb1d',\n","  'title': 'RESEARCH OUTPUTS / RÉSULTATS DE RECHERCHE'},\n"," {'paperId': '383cf65e4ed72046b78d32f420fa9615ee429b79',\n","  'title': 'Three Approaches for Mining Deﬁnitions from Relational Data in the Web of Data'},\n"," {'paperId': '73bc3bccd307713feb524b61b8f6487fff673dd2',\n","  'title': 'Map4Scrutiny – A Linked Open Data Solution for Politicians Interest Registers'},\n"," {'paperId': 'aab7329b642cc9eee98550fb1f074d8761940281',\n","  'title': 'A Holistic Ontology for Digital Libraries'},\n"," {'paperId': '0f7e8bdf3f4bd7db4567a4545237edff1edd4711',\n","  'title': 'Special issue on software citation, indexing, and discoverability'},\n"," {'paperId': 'cdfc4f13008ae450d977f603dd76f96b62a48b85',\n","  'title': 'Hierarchical Clustering for Property Graph Schema Discovery'},\n"," {'paperId': 'e6f0889bc0a4644be52e49dd42e1ae14595805ce',\n","  'title': 'Hierarchical clustering for property graph schema discovery'},\n"," {'paperId': '3b8e0a390fea6f66c0ed39a3a38888d600e2ef5e',\n","  'title': 'Wiki-TabNER: Advancing Table Interpretation Through Named Entity Recognition'},\n"," {'paperId': '84b99762e1ef5739672ca6ddc071936d94b610bf',\n","  'title': 'EnzChemRED, a rich enzyme chemistry relation extraction dataset'},\n"," {'paperId': '882f3d1b718e833156ccd6f5bda3190cd06791ea',\n","  'title': 'Mapping the Increasing Use of LLMs in Scientific Papers'},\n"," {'paperId': '0c9451d17110ce7111bba6409b1607f7eb9a50b4',\n","  'title': 'MillenniumDB: A multi-modal, multi-model graph database engine'},\n"," {'paperId': '7594d1a774f9d4c6fb05daf8ddf6c7cd10b4c9b5',\n","  'title': 'Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing'},\n"," {'paperId': '92281ac88e8736d0eff1b23ddaee0440b944f703',\n","  'title': 'KDBI special issue: MapIntel: A Visual Analytics Platform for Competitive Intelligence'},\n"," {'paperId': '7eb17c3fad9af9955a86c9e073bcf6345ae22bed',\n","  'title': 'Linked open data per la valorizzazione di collezioni culturali: il dataset mythLOD'},\n"," {'paperId': '0af7eb1d5665de5fbc41e007c806c0317940e080',\n","  'title': 'in Computer'},\n"," {'paperId': '0af7eb1d5665de5fbc41e007c806c0317940e080',\n","  'title': 'in Computer'},\n"," {'paperId': 'a23d9566f06faf3fde90bb80f507264d3123a8f0',\n","  'title': 'A Case Study for Language-Free Keyword Extraction with Statistical and Graph-Based Features'},\n"," {'paperId': '3fcc93c03745693307bdfcbb58815a27050b1e65',\n","  'title': 'Implementing Data Workflows and Data Model Extensions with RDF-star'},\n"," {'paperId': '94de91859aa47da14ad09d7b50b62d37d6bc1fc8',\n","  'title': 'Investigating the Challenges and Prospects of Construction Models for Dynamic Knowledge Graphs'},\n"," {'paperId': '2f2a4855a23ea89654678c44bca8db4eea19f87c',\n","  'title': 'The Most Cited Scientific Information Sources in Wikipedia Articles Across Various Languages'},\n"," {'paperId': '774e71ec0e43100a52bfca54db339652be564d72',\n","  'title': 'GraphER: A Structure-aware Text-to-Graph Model for Entity and Relation Extraction'},\n"," {'paperId': '9628ab686890690dbf596c9a0884e5a7ef98b0d2',\n","  'title': 'Validating and Exploring Large Geographic Corpora'},\n"," {'paperId': 'b60813dfa674a493067aeec870ac93ce7d08fce7',\n","  'title': 'A review of reasoning characteristics of RDF‐based Semantic Web systems'},\n"," {'paperId': '081579b4af58c3f7b267a25893ad246d5c7b298e',\n","  'title': 'Semantic Data Representation for Explainable Windows Malware Detection Models'},\n"," {'paperId': '2ce2de64d2d0e45ffe772f74939ad8eece037aed',\n","  'title': 'BioLORD-2023: semantic textual representations fusing large language models and clinical knowledge graph insights.'},\n"," {'paperId': 'b625b4f4fbe74016718a5b3fdc4554e3f9bec00e',\n","  'title': 'Library and Information Science students and DCMI Metadata Terms: do they understand the resource?'},\n"," {'paperId': 'db96e019410006c3ee0ae0184800ab206f8704dd',\n","  'title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction'},\n"," {'paperId': '3a9832e12b63b4bddeb2ffd988c4dfc6b55fd7f3',\n","  'title': 'NLP4RE Tools: Classification, Overview, and Management'},\n"," {'paperId': 'e16ba49a2988e11139eb6827d8e66c1707e1b96f',\n","  'title': 'LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices'},\n"," {'paperId': '9135bd1f7489fb51217fff48d3c34ffa1fd66df0',\n","  'title': 'Optimization of traditional methods for determining the similarity of project names and purchases using large language models'},\n"," {'paperId': '7b030e59067f2e017b3720eb08b7a00f751591e4',\n","  'title': 'HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents'},\n"," {'paperId': 'f5353209e85701e350b3a76d22a317f199971d3c',\n","  'title': 'On the Use of Virtual Knowledge Graphs to Improve Environmental Sensor Data Accessibility'}]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# START"],"metadata":{"id":"-c-5YdpuHeWJ"}},{"cell_type":"code","source":["import requests\n","\n","# Define the API endpoint URL\n","url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n","query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n","\n","# paper name 기입\n","query_params = {'query': query}#,'fields': 'citations'}#,citations.influentialCitationCount,citations.title,citations.year'}\n","\n","# semantic scholarship api 넣는다.\n","api_key = ''\n","headers = {'x-api-key': api_key}\n","\n","response = requests.get(url, params=query_params, headers=headers).json()\n","paper_id = response['data'][0]['paperId']\n","\n","# paper name에 대응되는 paper id\n","print(response['data'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPT5ettKHqm1","executionInfo":{"status":"ok","timestamp":1713953343795,"user_tz":-540,"elapsed":548,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"0695530d-1eb8-4fe9-cec0-95b96312d32a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'paperId': '53d128ea815bcc0526856eb5a9c42cc977cb36a7', 'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools'}\n"]}]},{"cell_type":"code","source":["import requests\n","\n","# Define the API endpoint URL\n","fields = '?fields=title,year,url,publicationDate,referenceCount,citationCount,influentialCitationCount' #,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,journal\n","\"\"\"\n","  abstract의 경우 포함시 load 많기에 따로 sorting 후 추후 처리\n","\n","  refernceCount ; 본 논문이 참조한 총 논문 갯수\n","  citationCount ; 이 논문에 대한 총 인용 횟수\n","  influentialCitationCount ; 출판물의 인용 횟수와 각각의 주변 맥락을 포함한 다양한 요소를 분석하는 기계 학습 모델을 활용하여 결정\n","\"\"\"\n","\n","url = 'https://api.semanticscholar.org/recommendations/v1/papers/forpaper/' + paper_id + fields\n","\n","api_key = ''\n","headers = {'x-api-key': api_key}\n","\n","# Send the API request\n","response = requests.get(url=url, headers=headers).json()\n","response['recommendedPapers']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQPUIEoQGHEv","executionInfo":{"status":"ok","timestamp":1713953347241,"user_tz":-540,"elapsed":636,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"8420e982-9412-4bc0-d394-fb4f3f073140"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'paperId': '7109683221040314999f4985c2483f7453a40dd9',\n","  'url': 'https://www.semanticscholar.org/paper/7109683221040314999f4985c2483f7453a40dd9',\n","  'title': 'Towards Practical Tool Usage for Continually Learning LLMs',\n","  'year': 2024,\n","  'referenceCount': 81,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-14'},\n"," {'paperId': 'e9b40e2cf481ebe5bf40b6958eebaf5ab1274481',\n","  'url': 'https://www.semanticscholar.org/paper/e9b40e2cf481ebe5bf40b6958eebaf5ab1274481',\n","  'title': 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 65,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-29'},\n"," {'paperId': '29a56e1c377ac6a4457656b57ef7631ed2bdb509',\n","  'url': 'https://www.semanticscholar.org/paper/29a56e1c377ac6a4457656b57ef7631ed2bdb509',\n","  'title': 'LLMCRIT: Teaching Large Language Models to Use Criteria',\n","  'year': 2024,\n","  'referenceCount': 30,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-02'},\n"," {'paperId': '1819a53eddb4a5334937561bb57542d7f11c8308',\n","  'url': 'https://www.semanticscholar.org/paper/1819a53eddb4a5334937561bb57542d7f11c8308',\n","  'title': 'What Are Tools Anyway? A Survey from the Language Model Perspective',\n","  'year': 2024,\n","  'referenceCount': 88,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-18'},\n"," {'paperId': 'a8b1484d3ee6b9ad6e4c48d6bcdbd1048493599d',\n","  'url': 'https://www.semanticscholar.org/paper/a8b1484d3ee6b9ad6e4c48d6bcdbd1048493599d',\n","  'title': 'Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models',\n","  'year': 2024,\n","  'referenceCount': 69,\n","  'citationCount': 2,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-13'},\n"," {'paperId': 'a5ed7a04640b11374871ccd6f6498bbc25aa8239',\n","  'url': 'https://www.semanticscholar.org/paper/a5ed7a04640b11374871ccd6f6498bbc25aa8239',\n","  'title': 'Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 49,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-03'},\n"," {'paperId': '90e97ee8a3421fb83f967c8f9bed7328f8aa8d9c',\n","  'url': 'https://www.semanticscholar.org/paper/90e97ee8a3421fb83f967c8f9bed7328f8aa8d9c',\n","  'title': 'Aligning Large and Small Language Models via Chain-of-Thought Reasoning',\n","  'year': 2024,\n","  'referenceCount': 37,\n","  'citationCount': 3,\n","  'influentialCitationCount': 0,\n","  'publicationDate': None},\n"," {'paperId': 'b3dae5011b9b44b861f884b2b788f2fc9bf59a4e',\n","  'url': 'https://www.semanticscholar.org/paper/b3dae5011b9b44b861f884b2b788f2fc9bf59a4e',\n","  'title': 'ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph',\n","  'year': 2024,\n","  'referenceCount': 36,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-29'},\n"," {'paperId': '3a20c3669a54bb2cfe6ca56663c077c19140e764',\n","  'url': 'https://www.semanticscholar.org/paper/3a20c3669a54bb2cfe6ca56663c077c19140e764',\n","  'title': 'Materials science in the era of large language models: a perspective',\n","  'year': 2024,\n","  'referenceCount': 105,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-11'},\n"," {'paperId': '884a573e07ebcc80e855670e769d803a77505cbe',\n","  'url': 'https://www.semanticscholar.org/paper/884a573e07ebcc80e855670e769d803a77505cbe',\n","  'title': 'Large Language Models are Contrastive Reasoners',\n","  'year': 2024,\n","  'referenceCount': 26,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-13'},\n"," {'paperId': 'd563c934e3bd6633d69300ea4725a11064479872',\n","  'url': 'https://www.semanticscholar.org/paper/d563c934e3bd6633d69300ea4725a11064479872',\n","  'title': 'Online Training of Large Language Models: Learn while chatting',\n","  'year': 2024,\n","  'referenceCount': 72,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-04'},\n"," {'paperId': 'fc45b0c7249c9e48de2cd35fc3d9984490229392',\n","  'url': 'https://www.semanticscholar.org/paper/fc45b0c7249c9e48de2cd35fc3d9984490229392',\n","  'title': 'Case-Based or Rule-Based: How Do Transformers Do the Math?',\n","  'year': 2024,\n","  'referenceCount': 32,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-27'},\n"," {'paperId': '4ff598828d16fae431ca88045162720e905bb0ee',\n","  'url': 'https://www.semanticscholar.org/paper/4ff598828d16fae431ca88045162720e905bb0ee',\n","  'title': 'Teaching Machines to Code: Smart Contract Translation with LLMs',\n","  'year': 2024,\n","  'referenceCount': 0,\n","  'citationCount': 1,\n","  'influentialCitationCount': 1,\n","  'publicationDate': '2024-03-13'},\n"," {'paperId': '5f1391e2849ff35452ee85454746ea0a15d65be2',\n","  'url': 'https://www.semanticscholar.org/paper/5f1391e2849ff35452ee85454746ea0a15d65be2',\n","  'title': 'Towards Modeling Learner Performance with Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 65,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-29'},\n"," {'paperId': '3c45d3c19a17a3eada0a1ff20b75fdee7520117b',\n","  'url': 'https://www.semanticscholar.org/paper/3c45d3c19a17a3eada0a1ff20b75fdee7520117b',\n","  'title': 'Calibrating Large Language Models Using Their Generations Only',\n","  'year': 2024,\n","  'referenceCount': 81,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-09'},\n"," {'paperId': 'ded3266d047b36a963c1324aa9f98705d598bdcf',\n","  'url': 'https://www.semanticscholar.org/paper/ded3266d047b36a963c1324aa9f98705d598bdcf',\n","  'title': 'From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs',\n","  'year': 2024,\n","  'referenceCount': 36,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-28'},\n"," {'paperId': '1f1093fbd6eb15b2a79e0e39d5e3621f8090fc84',\n","  'url': 'https://www.semanticscholar.org/paper/1f1093fbd6eb15b2a79e0e39d5e3621f8090fc84',\n","  'title': 'ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies',\n","  'year': 2024,\n","  'referenceCount': 47,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-02'},\n"," {'paperId': '1bda8efbbf4abae6c8c1da97d6137396807b1e09',\n","  'url': 'https://www.semanticscholar.org/paper/1bda8efbbf4abae6c8c1da97d6137396807b1e09',\n","  'title': 'ReFT: Representation Finetuning for Language Models',\n","  'year': 2024,\n","  'referenceCount': 97,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-04'},\n"," {'paperId': '85f93b7d15805c48eae3af35422754d2ced6b860',\n","  'url': 'https://www.semanticscholar.org/paper/85f93b7d15805c48eae3af35422754d2ced6b860',\n","  'title': 'Large Language Models as Planning Domain Generators (Student Abstract)',\n","  'year': 2024,\n","  'referenceCount': 32,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-24'},\n"," {'paperId': '3fb1818b9ab3f8d36ed3f8329087058f561c05da',\n","  'url': 'https://www.semanticscholar.org/paper/3fb1818b9ab3f8d36ed3f8329087058f561c05da',\n","  'title': 'ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline',\n","  'year': 2024,\n","  'referenceCount': 57,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-03'},\n"," {'paperId': 'ae4635297ad87fcb3ec4105a51b5cbcb4075e5e2',\n","  'url': 'https://www.semanticscholar.org/paper/ae4635297ad87fcb3ec4105a51b5cbcb4075e5e2',\n","  'title': 'LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error',\n","  'year': 2024,\n","  'referenceCount': 68,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-07'},\n"," {'paperId': '0a88726548d4ab50c3b28f1ce839220997875cf6',\n","  'url': 'https://www.semanticscholar.org/paper/0a88726548d4ab50c3b28f1ce839220997875cf6',\n","  'title': 'InstructEdit: Instruction-based Knowledge Editing for Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 60,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-25'},\n"," {'paperId': '03ae1a664b52503594e4753f59615443d70695b6',\n","  'url': 'https://www.semanticscholar.org/paper/03ae1a664b52503594e4753f59615443d70695b6',\n","  'title': 'RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners',\n","  'year': 2024,\n","  'referenceCount': 54,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-19'},\n"," {'paperId': 'a1228951fa61089866579aa4671bb1fb1128df69',\n","  'url': 'https://www.semanticscholar.org/paper/a1228951fa61089866579aa4671bb1fb1128df69',\n","  'title': 'Third-Party Language Model Performance Prediction from Instruction',\n","  'year': 2024,\n","  'referenceCount': 29,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-19'},\n"," {'paperId': 'a5a5de7f4a2ac60773e4f3de78e1299998aceb7b',\n","  'url': 'https://www.semanticscholar.org/paper/a5a5de7f4a2ac60773e4f3de78e1299998aceb7b',\n","  'title': 'Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange',\n","  'year': 2024,\n","  'referenceCount': 34,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-30'},\n"," {'paperId': '0d55fee4b3ed48f6b95be4ce16c48862d27bdbbd',\n","  'url': 'https://www.semanticscholar.org/paper/0d55fee4b3ed48f6b95be4ce16c48862d27bdbbd',\n","  'title': 'Low-Cost Language Models: Survey and Performance Evaluation on Python Code Generation',\n","  'year': 2024,\n","  'referenceCount': 0,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-17'},\n"," {'paperId': '960f95cac2ae7ab69356597fed8a5ad54929119c',\n","  'url': 'https://www.semanticscholar.org/paper/960f95cac2ae7ab69356597fed8a5ad54929119c',\n","  'title': 'Exploring the potential of large language models and generative artificial intelligence (GPT): Applications in Library and Information Science',\n","  'year': 2024,\n","  'referenceCount': 14,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-24'},\n"," {'paperId': 'f41baef16e08aaee020673339a56f4add722fb64',\n","  'url': 'https://www.semanticscholar.org/paper/f41baef16e08aaee020673339a56f4add722fb64',\n","  'title': 'SambaLingo: Teaching Large Language Models New Languages',\n","  'year': 2024,\n","  'referenceCount': 39,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-08'},\n"," {'paperId': '68e4b134021dff43b168f556711e7573636e994d',\n","  'url': 'https://www.semanticscholar.org/paper/68e4b134021dff43b168f556711e7573636e994d',\n","  'title': 'Fine-tuning Large Language Models with Sequential Instructions',\n","  'year': 2024,\n","  'referenceCount': 43,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-12'},\n"," {'paperId': '99d2c0b7aa04b3233a65e8302d97a12bfacad012',\n","  'url': 'https://www.semanticscholar.org/paper/99d2c0b7aa04b3233a65e8302d97a12bfacad012',\n","  'title': 'Perplexed: Understanding When Large Language Models are Confused',\n","  'year': 2024,\n","  'referenceCount': 45,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-09'},\n"," {'paperId': '6867e65205548c31e0832d41e5d3c511ea0d4db5',\n","  'url': 'https://www.semanticscholar.org/paper/6867e65205548c31e0832d41e5d3c511ea0d4db5',\n","  'title': 'Planning and Editing What You Retrieve for Enhanced Tool Learning',\n","  'year': 2024,\n","  'referenceCount': 35,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-30'},\n"," {'paperId': '2c9e99d5d7ba2ae411329e59c3523df6413d771e',\n","  'url': 'https://www.semanticscholar.org/paper/2c9e99d5d7ba2ae411329e59c3523df6413d771e',\n","  'title': 'Towards Generalist Prompting for Large Language Models by Mental Models',\n","  'year': 2024,\n","  'referenceCount': 32,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-28'},\n"," {'paperId': 'd0f062fc45267bdc89bc79aba19caed9c671739b',\n","  'url': 'https://www.semanticscholar.org/paper/d0f062fc45267bdc89bc79aba19caed9c671739b',\n","  'title': 'Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks',\n","  'year': 2024,\n","  'referenceCount': 21,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-14'},\n"," {'paperId': 'fcca8e6a17d6045ec241b9a35e141efaeb892f6f',\n","  'url': 'https://www.semanticscholar.org/paper/fcca8e6a17d6045ec241b9a35e141efaeb892f6f',\n","  'title': 'PURPLE: Making a Large Language Model a Better SQL Writer',\n","  'year': 2024,\n","  'referenceCount': 62,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-29'},\n"," {'paperId': '28bb68c387909f94588ac546738a4298888267ab',\n","  'url': 'https://www.semanticscholar.org/paper/28bb68c387909f94588ac546738a4298888267ab',\n","  'title': 'Forcing Diffuse Distributions out of Language Models',\n","  'year': 2024,\n","  'referenceCount': 26,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-16'},\n"," {'paperId': '451210e692c1377fe14a772aa1efa8aa8bfff7e0',\n","  'url': 'https://www.semanticscholar.org/paper/451210e692c1377fe14a772aa1efa8aa8bfff7e0',\n","  'title': 'Memory-Augmenting Decoder-Only Language Models through Encoders (Student Abstract)',\n","  'year': 2024,\n","  'referenceCount': 11,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-24'},\n"," {'paperId': '8f7ea3303b10758feb8ee688092d6a890677e635',\n","  'url': 'https://www.semanticscholar.org/paper/8f7ea3303b10758feb8ee688092d6a890677e635',\n","  'title': 'Are LLMs ready for Visualization?',\n","  'year': 2024,\n","  'referenceCount': 26,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-10'},\n"," {'paperId': 'cb26863dd5ee9e39ecfa4f383e61435756c337af',\n","  'url': 'https://www.semanticscholar.org/paper/cb26863dd5ee9e39ecfa4f383e61435756c337af',\n","  'title': 'ItD: Large Language Models Can Teach Themselves Induction through Deduction',\n","  'year': 2024,\n","  'referenceCount': 24,\n","  'citationCount': 2,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-09'},\n"," {'paperId': 'fa56be1738f4521d9b92c44c6dcfbedf5f6b95a3',\n","  'url': 'https://www.semanticscholar.org/paper/fa56be1738f4521d9b92c44c6dcfbedf5f6b95a3',\n","  'title': 'Cognition is All You Need - The Next Layer of AI Above Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 0,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-04'},\n"," {'paperId': 'cf7ab5df804575bad88a9fcf0fbf7707bf500944',\n","  'url': 'https://www.semanticscholar.org/paper/cf7ab5df804575bad88a9fcf0fbf7707bf500944',\n","  'title': 'Training-Free Long-Context Scaling of Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 43,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-27'},\n"," {'paperId': 'e6e15de9e3aa88c2690e2d3ec71b8a75a8002d55',\n","  'url': 'https://www.semanticscholar.org/paper/e6e15de9e3aa88c2690e2d3ec71b8a75a8002d55',\n","  'title': 'Can Language Models Solve Olympiad Programming?',\n","  'year': 2024,\n","  'referenceCount': 28,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-16'},\n"," {'paperId': '811dd8b5367f7acbac2f2e14f6ad452db38b8bef',\n","  'url': 'https://www.semanticscholar.org/paper/811dd8b5367f7acbac2f2e14f6ad452db38b8bef',\n","  'title': 'Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World',\n","  'year': 2024,\n","  'referenceCount': 30,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-30'},\n"," {'paperId': 'fdefb6a9b51c742d71740d25a76973116a2e0893',\n","  'url': 'https://www.semanticscholar.org/paper/fdefb6a9b51c742d71740d25a76973116a2e0893',\n","  'title': 'RAFT: Adapting Language Model to Domain Specific RAG',\n","  'year': 2024,\n","  'referenceCount': 55,\n","  'citationCount': 2,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-15'},\n"," {'paperId': '3567ddec4c3c2925f15f95010f9be658cf7fb50b',\n","  'url': 'https://www.semanticscholar.org/paper/3567ddec4c3c2925f15f95010f9be658cf7fb50b',\n","  'title': 'Eliciting Better Multilingual Structured Reasoning from LLMs through Code',\n","  'year': 2024,\n","  'referenceCount': 20,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-05'},\n"," {'paperId': '0550e63749f0270b5ff7655b50a902f614cfb281',\n","  'url': 'https://www.semanticscholar.org/paper/0550e63749f0270b5ff7655b50a902f614cfb281',\n","  'title': 'Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies',\n","  'year': 2024,\n","  'referenceCount': 26,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-27'},\n"," {'paperId': '1ab1d9ca27f31fab7c00e80ba1bf9b28ce75ca6e',\n","  'url': 'https://www.semanticscholar.org/paper/1ab1d9ca27f31fab7c00e80ba1bf9b28ce75ca6e',\n","  'title': 'Teaching Large Language Models an Unseen Language on the Fly',\n","  'year': 2024,\n","  'referenceCount': 36,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-29'},\n"," {'paperId': 'da3369c33ed94717166dba73dc0ebb5c9ddfe348',\n","  'url': 'https://www.semanticscholar.org/paper/da3369c33ed94717166dba73dc0ebb5c9ddfe348',\n","  'title': 'Large Language Model Can Continue Evolving From Mistakes',\n","  'year': 2024,\n","  'referenceCount': 35,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-11'},\n"," {'paperId': '819f30b2535a62b1d4342beadb7f3e4cab93ca51',\n","  'url': 'https://www.semanticscholar.org/paper/819f30b2535a62b1d4342beadb7f3e4cab93ca51',\n","  'title': 'Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs',\n","  'year': 2024,\n","  'referenceCount': 25,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-11'},\n"," {'paperId': 'd32764d479f338e0a1897cc3c35630f4ed0a39bf',\n","  'url': 'https://www.semanticscholar.org/paper/d32764d479f338e0a1897cc3c35630f4ed0a39bf',\n","  'title': 'SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection',\n","  'year': 2024,\n","  'referenceCount': 47,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-26'},\n"," {'paperId': '75bf5432ad95cd07b2d600b91d57cbcd4255befe',\n","  'url': 'https://www.semanticscholar.org/paper/75bf5432ad95cd07b2d600b91d57cbcd4255befe',\n","  'title': 'Guiding Enumerative Program Synthesis with Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 43,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-06'},\n"," {'paperId': 'fd50db0489828d0da2557a6da8d931d523479957',\n","  'url': 'https://www.semanticscholar.org/paper/fd50db0489828d0da2557a6da8d931d523479957',\n","  'title': 'BAGEL: Bootstrapping Agents by Guiding Exploration with Language',\n","  'year': 2024,\n","  'referenceCount': 41,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-12'},\n"," {'paperId': 'b548a4f8ecc95c252b2e1c039926db492b09066c',\n","  'url': 'https://www.semanticscholar.org/paper/b548a4f8ecc95c252b2e1c039926db492b09066c',\n","  'title': \"PromptSet: A Programmer's Prompting Dataset\",\n","  'year': 2024,\n","  'referenceCount': 31,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-26'},\n"," {'paperId': '369c9378ae72c4b72215c34766b8f5a2215212be',\n","  'url': 'https://www.semanticscholar.org/paper/369c9378ae72c4b72215c34766b8f5a2215212be',\n","  'title': 'generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation',\n","  'year': 2024,\n","  'referenceCount': 84,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-12'},\n"," {'paperId': 'e21758889543ca7ce958084078dc52ac2b0fba62',\n","  'url': 'https://www.semanticscholar.org/paper/e21758889543ca7ce958084078dc52ac2b0fba62',\n","  'title': 'Where does In-context Translation Happen in Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 48,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-07'},\n"," {'paperId': '56318a5ded349c27dbb1358ce1a19a013ec56724',\n","  'url': 'https://www.semanticscholar.org/paper/56318a5ded349c27dbb1358ce1a19a013ec56724',\n","  'title': 'Large Language Models are Parallel Multilingual Learners',\n","  'year': 2024,\n","  'referenceCount': 46,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-14'},\n"," {'paperId': '262866c56cfaf9dec21ee9049beb12c6c16f23ad',\n","  'url': 'https://www.semanticscholar.org/paper/262866c56cfaf9dec21ee9049beb12c6c16f23ad',\n","  'title': 'Guiding Large Language Models to Generate Computer-Parsable Content',\n","  'year': 2024,\n","  'referenceCount': 45,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-08'},\n"," {'paperId': '38270d319b74916ab0022cb740e3e1dd68fe6459',\n","  'url': 'https://www.semanticscholar.org/paper/38270d319b74916ab0022cb740e3e1dd68fe6459',\n","  'title': 'MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation',\n","  'year': 2024,\n","  'referenceCount': 39,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-14'},\n"," {'paperId': 'f2b5fb4f8318d17b8f4d99a5c778990b19ef5a09',\n","  'url': 'https://www.semanticscholar.org/paper/f2b5fb4f8318d17b8f4d99a5c778990b19ef5a09',\n","  'title': 'A Survey of using Large Language Models for Generating Infrastructure as Code',\n","  'year': 2024,\n","  'referenceCount': 46,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-30'},\n"," {'paperId': 'e4eaaaf1e435061bb2eed5f6dc187e453c3bb086',\n","  'url': 'https://www.semanticscholar.org/paper/e4eaaaf1e435061bb2eed5f6dc187e453c3bb086',\n","  'title': 'Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning',\n","  'year': 2024,\n","  'referenceCount': 51,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-15'},\n"," {'paperId': '33bc65cdcea955e4e2b60f854eb02ba92887c28f',\n","  'url': 'https://www.semanticscholar.org/paper/33bc65cdcea955e4e2b60f854eb02ba92887c28f',\n","  'title': 'An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 38,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-03'},\n"," {'paperId': 'e35c5f517e547bde2ae4d77a535d3e5584805f02',\n","  'url': 'https://www.semanticscholar.org/paper/e35c5f517e547bde2ae4d77a535d3e5584805f02',\n","  'title': 'ModeLing: A Novel Dataset for Testing Linguistic Reasoning in Language Models',\n","  'year': 2024,\n","  'referenceCount': 34,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': None},\n"," {'paperId': '0627590d66398578c04975d96bc7fa713d0b18f5',\n","  'url': 'https://www.semanticscholar.org/paper/0627590d66398578c04975d96bc7fa713d0b18f5',\n","  'title': 'KAUCUS - Knowledgeable User Simulators for Training Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 61,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-01-29'},\n"," {'paperId': '9dcecdf1b75e1a9bf9ec8dcee964d0e513a2b3d3',\n","  'url': 'https://www.semanticscholar.org/paper/9dcecdf1b75e1a9bf9ec8dcee964d0e513a2b3d3',\n","  'title': 'A Survey on Large Language Models from Concept to Implementation',\n","  'year': 2024,\n","  'referenceCount': 122,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-27'},\n"," {'paperId': '5775943977c5c525e1093a1070ea1bd50b84f6c6',\n","  'url': 'https://www.semanticscholar.org/paper/5775943977c5c525e1093a1070ea1bd50b84f6c6',\n","  'title': 'WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction',\n","  'year': 2024,\n","  'referenceCount': 13,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-05'},\n"," {'paperId': 'c41ff6575a0eb5942212de2cfdb09396bcb3601e',\n","  'url': 'https://www.semanticscholar.org/paper/c41ff6575a0eb5942212de2cfdb09396bcb3601e',\n","  'title': 'NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 41,\n","  'citationCount': 2,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-04'},\n"," {'paperId': '2be59a3c79288f771344995a6f0ab1dd4da35b0e',\n","  'url': 'https://www.semanticscholar.org/paper/2be59a3c79288f771344995a6f0ab1dd4da35b0e',\n","  'title': 'Audiences, automation, and AI: From structured news to language models',\n","  'year': 2024,\n","  'referenceCount': 7,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-05'},\n"," {'paperId': '27a6dc0300eb6a002e8f53c2dacd3495c034f9ff',\n","  'url': 'https://www.semanticscholar.org/paper/27a6dc0300eb6a002e8f53c2dacd3495c034f9ff',\n","  'title': 'RoT: Enhancing Large Language Models with Reflection on Search Trees',\n","  'year': 2024,\n","  'referenceCount': 39,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-08'},\n"," {'paperId': 'aa59536123b29599115cb28027d9ddb67fc1c613',\n","  'url': 'https://www.semanticscholar.org/paper/aa59536123b29599115cb28027d9ddb67fc1c613',\n","  'title': 'Telecom Language Models: Must They Be Large?',\n","  'year': 2024,\n","  'referenceCount': 19,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-07'},\n"," {'paperId': '27296a78c457c188c9f20f8205023a7bc77bb08d',\n","  'url': 'https://www.semanticscholar.org/paper/27296a78c457c188c9f20f8205023a7bc77bb08d',\n","  'title': 'Acquiring Linguistic Knowledge from Multimodal Input',\n","  'year': 2024,\n","  'referenceCount': 57,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-27'},\n"," {'paperId': 'ef1d74ddfc09a367050bc54b7be846769061d95e',\n","  'url': 'https://www.semanticscholar.org/paper/ef1d74ddfc09a367050bc54b7be846769061d95e',\n","  'title': 'Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments',\n","  'year': 2024,\n","  'referenceCount': 46,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-13'},\n"," {'paperId': '258ce7997260b355f4291703e8b74ae0e5e666f9',\n","  'url': 'https://www.semanticscholar.org/paper/258ce7997260b355f4291703e8b74ae0e5e666f9',\n","  'title': 'Towards Consistent Language Models Using Controlled Prompting and Decoding',\n","  'year': None,\n","  'referenceCount': 23,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': None},\n"," {'paperId': '363064388aeef51f916131e87916a48054435cb5',\n","  'url': 'https://www.semanticscholar.org/paper/363064388aeef51f916131e87916a48054435cb5',\n","  'title': 'Rethinking Generative Large Language Model Evaluation for Semantic Comprehension',\n","  'year': 2024,\n","  'referenceCount': 68,\n","  'citationCount': 2,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-12'},\n"," {'paperId': 'd2503ddd1bcd4b33ac1c703c3475d6aae8abf483',\n","  'url': 'https://www.semanticscholar.org/paper/d2503ddd1bcd4b33ac1c703c3475d6aae8abf483',\n","  'title': 'LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation',\n","  'year': 2024,\n","  'referenceCount': 91,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-02'},\n"," {'paperId': '4c9ea71ebca90fa902c718a40c19dcef5d6cdfbe',\n","  'url': 'https://www.semanticscholar.org/paper/4c9ea71ebca90fa902c718a40c19dcef5d6cdfbe',\n","  'title': 'PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset',\n","  'year': 2024,\n","  'referenceCount': 50,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-05'},\n"," {'paperId': '524fb3ee0cf8fd04570036cc8a1b31c9a8ddc534',\n","  'url': 'https://www.semanticscholar.org/paper/524fb3ee0cf8fd04570036cc8a1b31c9a8ddc534',\n","  'title': 'Evolving Knowledge Distillation with Large Language Models and Active Learning',\n","  'year': 2024,\n","  'referenceCount': 51,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-11'},\n"," {'paperId': 'd065dbe07e15593606d471a370f4f9ceaa6143ce',\n","  'url': 'https://www.semanticscholar.org/paper/d065dbe07e15593606d471a370f4f9ceaa6143ce',\n","  'title': 'On Languaging a Simulation Engine',\n","  'year': 2024,\n","  'referenceCount': 54,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-26'},\n"," {'paperId': '9fc4fa0e353c660691d2b660edf13684117cf5b4',\n","  'url': 'https://www.semanticscholar.org/paper/9fc4fa0e353c660691d2b660edf13684117cf5b4',\n","  'title': 'The Use of Generative Search Engines for Knowledge Work and Complex Tasks',\n","  'year': 2024,\n","  'referenceCount': 32,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-19'},\n"," {'paperId': '97352d95cc1fd9b7d9e959d61dd751a619975bfe',\n","  'url': 'https://www.semanticscholar.org/paper/97352d95cc1fd9b7d9e959d61dd751a619975bfe',\n","  'title': \"Arcee's MergeKit: A Toolkit for Merging Large Language Models\",\n","  'year': 2024,\n","  'referenceCount': 49,\n","  'citationCount': 3,\n","  'influentialCitationCount': 1,\n","  'publicationDate': '2024-03-20'},\n"," {'paperId': 'ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0',\n","  'url': 'https://www.semanticscholar.org/paper/ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0',\n","  'title': 'Advancing LLM Reasoning Generalists with Preference Trees',\n","  'year': 2024,\n","  'referenceCount': 58,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-02'},\n"," {'paperId': 'e13f9819b30007181b3fd9d7d1dd76aa26e8b4df',\n","  'url': 'https://www.semanticscholar.org/paper/e13f9819b30007181b3fd9d7d1dd76aa26e8b4df',\n","  'title': 'Can LLMs translate SATisfactorily? Assessing LLMs in Generating and Interpreting Formal Specifications',\n","  'year': None,\n","  'referenceCount': 5,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': None},\n"," {'paperId': '77e07e5542b450a2ee3193993c552700ad9ba82d',\n","  'url': 'https://www.semanticscholar.org/paper/77e07e5542b450a2ee3193993c552700ad9ba82d',\n","  'title': 'FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions',\n","  'year': 2024,\n","  'referenceCount': 46,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-22'},\n"," {'paperId': 'ed7fddff0bc8a0388446f0c1c1b65a8e1c346056',\n","  'url': 'https://www.semanticscholar.org/paper/ed7fddff0bc8a0388446f0c1c1b65a8e1c346056',\n","  'title': 'LLM-based Test-driven Interactive Code Generation: User Study and Empirical Evaluation',\n","  'year': 2024,\n","  'referenceCount': 47,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-15'},\n"," {'paperId': '7ad3a1d2ec3265164870cfca721e4d4914ecfbfb',\n","  'url': 'https://www.semanticscholar.org/paper/7ad3a1d2ec3265164870cfca721e4d4914ecfbfb',\n","  'title': 'Data Analysis Using Generative AI: Opportunities and Challenges',\n","  'year': 2023,\n","  'referenceCount': 0,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': None},\n"," {'paperId': '788cae44a021a7a5bbc8032c5626d980a6f7468a',\n","  'url': 'https://www.semanticscholar.org/paper/788cae44a021a7a5bbc8032c5626d980a6f7468a',\n","  'title': 'Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 38,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-01'},\n"," {'paperId': 'd0075dd5603c9d477edf0a41f59ab6dcf0e91976',\n","  'url': 'https://www.semanticscholar.org/paper/d0075dd5603c9d477edf0a41f59ab6dcf0e91976',\n","  'title': 'Simple and Scalable Strategies to Continually Pre-train Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 92,\n","  'citationCount': 3,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-13'},\n"," {'paperId': '4ac401216d84ede5ab613bf44676b3a968dcab83',\n","  'url': 'https://www.semanticscholar.org/paper/4ac401216d84ede5ab613bf44676b3a968dcab83',\n","  'title': 'Large language models and linguistic intentionality',\n","  'year': 2024,\n","  'referenceCount': 4,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-15'},\n"," {'paperId': '086c6252d9b0e81a541dbad3dc5ee1fbd01330b3',\n","  'url': 'https://www.semanticscholar.org/paper/086c6252d9b0e81a541dbad3dc5ee1fbd01330b3',\n","  'title': 'Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT',\n","  'year': 2024,\n","  'referenceCount': 41,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-03'},\n"," {'paperId': '303b4ae0e53cca8d007076778e5c17e28116ff7e',\n","  'url': 'https://www.semanticscholar.org/paper/303b4ae0e53cca8d007076778e5c17e28116ff7e',\n","  'title': 'Learning to Decode Collaboratively with Multiple Language Models',\n","  'year': 2024,\n","  'referenceCount': 40,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-06'},\n"," {'paperId': '1bdf6986738a32043bf6d267c214323b4fbef6e7',\n","  'url': 'https://www.semanticscholar.org/paper/1bdf6986738a32043bf6d267c214323b4fbef6e7',\n","  'title': 'AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent',\n","  'year': 2024,\n","  'referenceCount': 50,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-04'},\n"," {'paperId': '84d8850d985d514a851f292698bc66b86ba489ce',\n","  'url': 'https://www.semanticscholar.org/paper/84d8850d985d514a851f292698bc66b86ba489ce',\n","  'title': 'UvA-DARE (Digital Academic Repository) Do Instruction-tuned Large Language Models Help with Relation Extraction?',\n","  'year': None,\n","  'referenceCount': 16,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': None},\n"," {'paperId': 'cf51da61a045aa5fe6382b265cf913975cd00dd3',\n","  'url': 'https://www.semanticscholar.org/paper/cf51da61a045aa5fe6382b265cf913975cd00dd3',\n","  'title': 'Monotonic Paraphrasing Improves Generalization of Language Model Prompting',\n","  'year': 2024,\n","  'referenceCount': 75,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-24'},\n"," {'paperId': '5ec2aa6c84e4b7ee51c6cc3e4cd74ed3b21c2df1',\n","  'url': 'https://www.semanticscholar.org/paper/5ec2aa6c84e4b7ee51c6cc3e4cd74ed3b21c2df1',\n","  'title': \"FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability\",\n","  'year': 2024,\n","  'referenceCount': 35,\n","  'citationCount': 1,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-28'},\n"," {'paperId': '87f0c4a4d56d7265fa8deaa344547c54aa5d98c3',\n","  'url': 'https://www.semanticscholar.org/paper/87f0c4a4d56d7265fa8deaa344547c54aa5d98c3',\n","  'title': 'A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing',\n","  'year': 2024,\n","  'referenceCount': 17,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-04'},\n"," {'paperId': '212318b81f99a7dc83929b4fea679b096cdf513d',\n","  'url': 'https://www.semanticscholar.org/paper/212318b81f99a7dc83929b4fea679b096cdf513d',\n","  'title': 'Memory Sharing for Large Language Model based Agents',\n","  'year': 2024,\n","  'referenceCount': 28,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-15'},\n"," {'paperId': 'b31a0dec20b0185fdf260aa64c16f0d053a8f3e8',\n","  'url': 'https://www.semanticscholar.org/paper/b31a0dec20b0185fdf260aa64c16f0d053a8f3e8',\n","  'title': 'Can Large Language Models Automatically Score Proficiency of Written Essays?',\n","  'year': 2024,\n","  'referenceCount': 22,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-10'},\n"," {'paperId': '72c7de236f0f1c48173873dd53a441440a73d873',\n","  'url': 'https://www.semanticscholar.org/paper/72c7de236f0f1c48173873dd53a441440a73d873',\n","  'title': 'Augmenting Large Language Models with Symbolic Rule Learning for Robust Numerical Reasoning',\n","  'year': None,\n","  'referenceCount': 15,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': None},\n"," {'paperId': '306364a9854e63619c512cc4d5231eade9d55663',\n","  'url': 'https://www.semanticscholar.org/paper/306364a9854e63619c512cc4d5231eade9d55663',\n","  'title': 'Leveraging pre-trained language models for code generation',\n","  'year': 2024,\n","  'referenceCount': 27,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-02-29'},\n"," {'paperId': '006134d7d87ad8ad5836c20cac57b326da632672',\n","  'url': 'https://www.semanticscholar.org/paper/006134d7d87ad8ad5836c20cac57b326da632672',\n","  'title': 'S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document',\n","  'year': 2024,\n","  'referenceCount': 25,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-15'},\n"," {'paperId': '33c40aadd08802cfa4911dc99424922d538999c3',\n","  'url': 'https://www.semanticscholar.org/paper/33c40aadd08802cfa4911dc99424922d538999c3',\n","  'title': 'A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)',\n","  'year': 2024,\n","  'referenceCount': 44,\n","  'citationCount': 2,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-01'},\n"," {'paperId': '2e96c568a6e896e8fe82d2d6ee59475a255dd4d8',\n","  'url': 'https://www.semanticscholar.org/paper/2e96c568a6e896e8fe82d2d6ee59475a255dd4d8',\n","  'title': 'CHAT GPT ASSISTANT',\n","  'year': 2024,\n","  'referenceCount': 0,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-07'}]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# 영향력 높은 target paper와 related된 paper filtering >> citationcount가 너무 높다.\n","sorted_papers = sorted(response['recommendedPapers'], key=lambda x: x['influentialCitationCount'], reverse=True)[:5]\n","sorted_papers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ic-kDzP2Iv-9","executionInfo":{"status":"ok","timestamp":1713953389353,"user_tz":-540,"elapsed":3,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"1e0f517e-459c-4e95-a35c-5ac1d29fef8b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'paperId': '4ff598828d16fae431ca88045162720e905bb0ee',\n","  'url': 'https://www.semanticscholar.org/paper/4ff598828d16fae431ca88045162720e905bb0ee',\n","  'title': 'Teaching Machines to Code: Smart Contract Translation with LLMs',\n","  'year': 2024,\n","  'referenceCount': 0,\n","  'citationCount': 1,\n","  'influentialCitationCount': 1,\n","  'publicationDate': '2024-03-13'},\n"," {'paperId': '97352d95cc1fd9b7d9e959d61dd751a619975bfe',\n","  'url': 'https://www.semanticscholar.org/paper/97352d95cc1fd9b7d9e959d61dd751a619975bfe',\n","  'title': \"Arcee's MergeKit: A Toolkit for Merging Large Language Models\",\n","  'year': 2024,\n","  'referenceCount': 49,\n","  'citationCount': 3,\n","  'influentialCitationCount': 1,\n","  'publicationDate': '2024-03-20'},\n"," {'paperId': '7109683221040314999f4985c2483f7453a40dd9',\n","  'url': 'https://www.semanticscholar.org/paper/7109683221040314999f4985c2483f7453a40dd9',\n","  'title': 'Towards Practical Tool Usage for Continually Learning LLMs',\n","  'year': 2024,\n","  'referenceCount': 81,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-14'},\n"," {'paperId': 'e9b40e2cf481ebe5bf40b6958eebaf5ab1274481',\n","  'url': 'https://www.semanticscholar.org/paper/e9b40e2cf481ebe5bf40b6958eebaf5ab1274481',\n","  'title': 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n","  'year': 2024,\n","  'referenceCount': 65,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-29'},\n"," {'paperId': '29a56e1c377ac6a4457656b57ef7631ed2bdb509',\n","  'url': 'https://www.semanticscholar.org/paper/29a56e1c377ac6a4457656b57ef7631ed2bdb509',\n","  'title': 'LLMCRIT: Teaching Large Language Models to Use Criteria',\n","  'year': 2024,\n","  'referenceCount': 30,\n","  'citationCount': 0,\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-03-02'}]"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["# Next"],"metadata":{"id":"PgcNdbFk7Efm"}},{"cell_type":"markdown","source":["Citations"],"metadata":{"id":"un0vyj96-SrJ"}},{"cell_type":"code","source":["import requests\n","\n","# Define the API endpoint URL\n","url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n","query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n","\n","# paper name 기입\n","query_params = {'query': query,'fields': 'citations,citations.influentialCitationCount,citations.title,citations.publicationDate'}\n","\n","# semantic scholarship api 넣는다.\n","api_key = ''\n","headers = {'x-api-key': api_key}\n","\n","response = requests.get(url, params=query_params, headers=headers).json()\n","paper_id = response['data'][0]['paperId']\n","\n","# paper name에 대응되는 paper id\n","# print(response['data'][0]['title'], paper_id)\n","response['data'][0]['citations'][:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ckJaO9ayFsF","executionInfo":{"status":"ok","timestamp":1713953491897,"user_tz":-540,"elapsed":1596,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"a61422bb-9b17-48b5-cab9-00c5524ea072"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'paperId': 'b66f499b302e1bb5cd95b3c160b71d16c11451c9',\n","  'title': 'Navigating the Path of Writing: Outline-guided Text Generation with Large Language Models',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-22'},\n"," {'paperId': 'b6ab16c8eade03a39830493071d99fc48a736fac',\n","  'title': 'A Survey on the Memory Mechanism of Large Language Model based Agents',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-21'},\n"," {'paperId': 'eafadb9a56a18075d794617b6a821722fbabe117',\n","  'title': 'LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-19'},\n"," {'paperId': '877ae5ba1c614d597f821267e743393f1146f972',\n","  'title': 'Requirements Satisfiability with In-Context Learning',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-19'},\n"," {'paperId': 'f18e5a844c37e5342f8f3d409c74c1a9c91d1f8f',\n","  'title': 'The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-19'},\n"," {'paperId': 'fbf7e7f2f156443ee2e31297e8eb4a2819a18845',\n","  'title': 'Large Language Models Can Plan Your Travels Rigorously with Formal Verification Tools',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-18'},\n"," {'paperId': '47c8f0d7232f52f1a48e933e32309dc35ad85f49',\n","  'title': 'MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-17'},\n"," {'paperId': 'e5a74f0fda9f58917173759db58b00420983e11d',\n","  'title': 'Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-15'},\n"," {'paperId': '3ad3e240cabb3f3471770d25a7414a81175aa0db',\n","  'title': 'Explainable Generative AI (GenXAI): A Survey, Conceptualization, and Research Agenda',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-15'},\n"," {'paperId': '7109683221040314999f4985c2483f7453a40dd9',\n","  'title': 'Towards Practical Tool Usage for Continually Learning LLMs',\n","  'influentialCitationCount': 0,\n","  'publicationDate': '2024-04-14'}]"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# target paper가 citations 한 paper 중 influentialCitationCount가 높은 20개만\n","\n","# None 값을 처리하는 함수\n","def get_citation_count(item):\n","    influential_citation_count = item.get('influentialCitationCount')\n","    if influential_citation_count is not None:\n","        return influential_citation_count\n","    else:\n","        return 0\n","\n","# 정렬\n","sorted_papers = sorted(response['data'][0]['citations'], key=get_citation_count, reverse=True)[:20]\n","sorted_papers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-UkwBMm7D3j","executionInfo":{"status":"ok","timestamp":1713953430086,"user_tz":-540,"elapsed":749,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"0155a622-4088-4f14-da3f-ad8abefe7037"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'paperId': '104b0bb1da562d53cbda87aec79ef6a2827d191a',\n","  'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models',\n","  'influentialCitationCount': 566,\n","  'publicationDate': '2023-07-18'},\n"," {'paperId': 'c61d54644e9aedcfc756e5d6fe4cc8b78c87755d',\n","  'title': 'A Survey of Large Language Models',\n","  'influentialCitationCount': 70,\n","  'publicationDate': '2023-03-31'},\n"," {'paperId': '6c943670dca38bfc7c8b477ae7c2d1fba1ad3691',\n","  'title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks',\n","  'influentialCitationCount': 50,\n","  'publicationDate': '2022-11-22'},\n"," {'paperId': 'd1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43',\n","  'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face',\n","  'influentialCitationCount': 49,\n","  'publicationDate': None},\n"," {'paperId': '5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0',\n","  'title': 'Qwen Technical Report',\n","  'influentialCitationCount': 44,\n","  'publicationDate': '2023-09-28'},\n"," {'paperId': '0671fd553dd670a4e820553a974bc48040ba0819',\n","  'title': 'Reflexion: language agents with verbal reinforcement learning',\n","  'influentialCitationCount': 44,\n","  'publicationDate': '2023-03-20'},\n"," {'paperId': '0bfc804e31eecfd77f45e4ee7f4d629fffdcd628',\n","  'title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs',\n","  'influentialCitationCount': 38,\n","  'publicationDate': '2023-07-31'},\n"," {'paperId': '6e754273d54a91371efbc928cd6b156364d517da',\n","  'title': 'ViperGPT: Visual Inference via Python Execution for Reasoning',\n","  'influentialCitationCount': 30,\n","  'publicationDate': '2023-03-14'},\n"," {'paperId': '1733eb7792f7a43dd21f51f4d1017a1bffd217b5',\n","  'title': 'Lost in the Middle: How Language Models Use Long Contexts',\n","  'influentialCitationCount': 29,\n","  'publicationDate': '2023-07-06'},\n"," {'paperId': 'e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365',\n","  'title': 'Enabling Large Language Models to Generate Text with Citations',\n","  'influentialCitationCount': 27,\n","  'publicationDate': '2023-05-24'},\n"," {'paperId': '7d8905a1fd288068f12c8347caeabefd36d0dd6c',\n","  'title': 'Gorilla: Large Language Model Connected with Massive APIs',\n","  'influentialCitationCount': 25,\n","  'publicationDate': '2023-05-24'},\n"," {'paperId': '28c6ac721f54544162865f41c5692e70d61bccab',\n","  'title': 'A Survey on Large Language Model based Autonomous Agents',\n","  'influentialCitationCount': 22,\n","  'publicationDate': '2023-08-22'},\n"," {'paperId': '58f8925a8b87054ad0635a6398a7fe24935b1604',\n","  'title': 'Mind2Web: Towards a Generalist Agent for the Web',\n","  'influentialCitationCount': 22,\n","  'publicationDate': '2023-06-09'},\n"," {'paperId': '0819c1e60c13b9797f937282d06b54d252d9d6ec',\n","  'title': 'Segment Everything Everywhere All at Once',\n","  'influentialCitationCount': 21,\n","  'publicationDate': '2023-04-13'},\n"," {'paperId': '888728745dbb769e29ed475d4f7661eebe1a71cf',\n","  'title': 'A Survey on Evaluation of Large Language Models',\n","  'influentialCitationCount': 20,\n","  'publicationDate': '2023-07-06'},\n"," {'paperId': '003ef1cd670d01af05afa0d3c72d72228f494432',\n","  'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n","  'influentialCitationCount': 19,\n","  'publicationDate': '2023-04-22'},\n"," {'paperId': '5501d00310b06e00351295529498cc684187148d',\n","  'title': 'GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models',\n","  'influentialCitationCount': 19,\n","  'publicationDate': '2023-03-17'},\n"," {'paperId': '5dbffedcabe3fa43060ebbe2b1789500edfd871f',\n","  'title': 'Reasoning with Language Model is Planning with World Model',\n","  'influentialCitationCount': 18,\n","  'publicationDate': '2023-05-24'},\n"," {'paperId': '703035b483c181953de1b55b5fd59cd4cd4cf211',\n","  'title': 'MetaGPT: Meta Programming for Multi-Agent Collaborative Framework',\n","  'influentialCitationCount': 17,\n","  'publicationDate': '2023-08-01'},\n"," {'paperId': '170c97c7215f42edfb20c2248f954879e91ef86e',\n","  'title': 'Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models',\n","  'influentialCitationCount': 17,\n","  'publicationDate': '2023-04-19'}]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Reference"],"metadata":{"id":"HS5eEt-A-Q7J"}},{"cell_type":"code","source":["# Define the API endpoint URL\n","url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n","\n","# paper name 기입\n","query_params = {'query': query}\n","api_key = ''\n","headers = {'x-api-key': api_key}\n","\n","response = requests.get(url, params=query_params, headers=headers).json()\n","paper_id = response['data'][0]['paperId']\n","\n","fields = '?fields=title,publicationDate,influentialCitationCount,contexts,intents'\n","\"\"\"\n","  context ; snippets of text where the reference is mentioned\n","  intents ; intents derived from the contexts in which this citation is mentioned.\n","\"\"\"\n","\n","url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references'+ fields\n","\n","# Send the API request\n","response = requests.get(url=url, headers=headers).json()\n","response['data']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aR1waKC67Wep","executionInfo":{"status":"ok","timestamp":1713953444544,"user_tz":-540,"elapsed":1139,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"f9b4f8c9-7720-47d6-8332-03880347aee7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'contexts': ['Honovich et al., 2022; Wang et al., 2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': 'e65b346d442e9962a4276dc1c1af2956d9d5f1eb',\n","   'title': 'Self-Instruct: Aligning Language Models with Self-Generated Instructions',\n","   'influentialCitationCount': 117,\n","   'publicationDate': '2022-12-20'}},\n"," {'contexts': ['Honovich et al., 2022; Wang et al., 2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': '6f4cc536f9ed83d0dbf7e919dc609be12aa0848a',\n","   'title': 'Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor',\n","   'influentialCitationCount': 18,\n","   'publicationDate': '2022-12-19'}},\n"," {'contexts': ['…(Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., 2022) and Python interpreters (Gao et al., 2022).',\n","   'However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.',\n","   ', 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be used (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022).',\n","   'This is in contrast to prior work on tool use (e.g., Gao et al., 2022; Parisi et al., 2022), where models are provided with dataset-specific examples of how a tool can be used to solve a concrete task.',\n","   '…of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which needs to be used (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022).'],\n","  'intents': ['background', 'result', 'methodology'],\n","  'citedPaper': {'paperId': '6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7',\n","   'title': 'PAL: Program-aided Language Models',\n","   'influentialCitationCount': 45,\n","   'publicationDate': '2022-11-18'}},\n"," {'contexts': [', 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al.',\n","   '…information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'a938ff4539b09a785a66669844f1a35f76169218',\n","   'title': 'PEER: A Collaborative Language Model',\n","   'influentialCitationCount': 8,\n","   'publicationDate': '2022-08-24'}},\n"," {'contexts': ['Specifically, we use Atlas (Izacard et al., 2022), a retrieval-augmented LM finetuned on Natural Questions (Kwiatkowski et al.',\n","   ', 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).',\n","   '…information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).',\n","   'Specifically, we use Atlas (Izacard et al., 2022), a retrieval-augmented LM finetuned on Natural Questions (Kwiatkowski et al., 2019).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': '398e4061dde8f5c80606869cebfa2031de7b5b74',\n","   'title': 'Few-shot Learning with Retrieval Augmented Language Models',\n","   'influentialCitationCount': 44,\n","   'publicationDate': '2022-08-05'}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': 'a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd',\n","   'title': 'BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage',\n","   'influentialCitationCount': 22,\n","   'publicationDate': '2022-08-05'}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': 'e19b54ad4c1c8af045069e9cac350ffc2ce60e1a',\n","   'title': 'No Language Left Behind: Scaling Human-Centered Machine Translation',\n","   'influentialCitationCount': 44,\n","   'publicationDate': '2022-07-11'}},\n"," {'contexts': ['However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.',\n","   'This is in contrast to prior work on tool use (e.g., Gao et al., 2022; Parisi et al., 2022), where models are provided with dataset-specific examples of how a tool can be used to solve a concrete task.',\n","   'Perhaps most closely related to our work is TALM (Parisi et al., 2022), an approach that uses a similar self-supervised objective for teaching a model to use a calculator and a search engine, but explores this only in settings where a model is finetuned for downstream tasks.',\n","   'A potential solution to this problem might be to iteratively apply our approach, similar to how this is done in related bootstrapping approaches (Schick and Schütze, 2021a; Izacard and Grave, 2021; Parisi et al., 2022).'],\n","  'intents': ['result', 'methodology'],\n","  'citedPaper': {'paperId': '354bf043179e3e9f05df73e3f04517e53c326d1f',\n","   'title': 'TALM: Tool Augmented Language Models',\n","   'influentialCitationCount': 10,\n","   'publicationDate': '2022-05-24'}},\n"," {'contexts': ['We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'],\n","  'intents': ['result'],\n","  'citedPaper': {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221',\n","   'title': 'OPT: Open Pre-trained Transformer Language Models',\n","   'influentialCitationCount': 256,\n","   'publicationDate': '2022-05-02'}},\n"," {'contexts': ['Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb',\n","   'title': 'PaLM: Scaling Language Modeling with Pathways',\n","   'influentialCitationCount': 295,\n","   'publicationDate': '2022-04-05'}},\n"," {'contexts': ['…to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise…',\n","   ', 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al.'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '3def68bd0f856886d34272840a7f81588f2bc082',\n","   'title': 'Survey of Hallucination in Natural Language Generation',\n","   'influentialCitationCount': 55,\n","   'publicationDate': '2022-02-08'}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': 'b3848d32f7294ec708627897833c4097eb4d8778',\n","   'title': 'LaMDA: Language Models for Dialog Applications',\n","   'influentialCitationCount': 88,\n","   'publicationDate': '2022-01-20'}},\n"," {'contexts': ['…a tool in an interactive way – especially for tools such as search engines, that could potentially return hundreds of different results, enabling a LM to browse through these results or to refine its search query in a similar spirit to Nakano et al. (2021) can be crucial for certain applications.',\n","   '…such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., 2022) and Python interpreters…',\n","   '…models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '2f3efe44083af91cef562c1a3451eee2f8601d22',\n","   'title': 'WebGPT: Browser-assisted question-answering with human feedback',\n","   'influentialCitationCount': 71,\n","   'publicationDate': '2021-12-17'}},\n"," {'contexts': ['…information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '002c256d30d6be4b23d365a8de8ae0e67e4c9641',\n","   'title': 'Improving language models by retrieving from trillions of tokens',\n","   'influentialCitationCount': 53,\n","   'publicationDate': '2021-12-08'}},\n"," {'contexts': ['…(Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., 2022) and Python interpreters (Gao et al., 2022).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'd6045d2ccc9c09ca1671348de86d07da6bc28eea',\n","   'title': 'Training Verifiers to Solve Math Word Problems',\n","   'influentialCitationCount': 357,\n","   'publicationDate': '2021-10-27'}},\n"," {'contexts': ['Tool Use Several approaches aim to equip LMs with the ability to use external tools such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al.',\n","   'These limitations include an inability to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al.',\n","   'However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.',\n","   'These limitations include an inability to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical…',\n","   'The way these models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be used (Gao et al.',\n","   'The way these models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific…',\n","   'However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.',\n","   'Tool Use Several approaches aim to equip LMs with the ability to use external tools such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan…'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': 'de549c1592a62c129b8d49c8c0137aa6859b103f',\n","   'title': 'Internet-Augmented Dialogue Generation',\n","   'influentialCitationCount': 44,\n","   'publicationDate': '2021-07-15'}},\n"," {'contexts': ['…LMs with some form of additional textual information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020;…'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': 'e596b8adbffa546dbc163e817fb3de72744ec4f6',\n","   'title': 'HTLM: Hyper-Text Pre-Training and Prompting of Language Models',\n","   'influentialCitationCount': 2,\n","   'publicationDate': '2021-07-14'}},\n"," {'contexts': ['Even for bigger models, the gap between predictions with and without API calls remains high. closer inspection shows that improvements on T EMP LAMA can not be attributed to the calendar tool, which is only used for 0.2% of all examples, but mostly to the Wikipedia search and question answering tools.',\n","   'As LAMA is based on statements obtained directly from Wikipedia, we prevent Toolformer from using the Wikipedia Search API to avoid giving it an unfair advantage.',\n","   'However, Figure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of different sizes and GPT-J finetuned with our approach, both with and without API calls.',\n","   'T EMP LAMA contains cloze queries about facts that change with time.',\n","   'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.',\n","   '…tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time (Dhingra et al., 2022).',\n","   'For both tasks, we use the same evaluation as for the original LAMA dataset.',\n","   'Results shown in Table 4 (right) illustrate that Toolformer outperforms all baselines for both T EMP LAMA and D ATESET .',\n","   'We evaluate all models on T EMP LAMA (Dhingra et al., 2022) and a new dataset that we call D ATESET .',\n","   'The effect of these modifications is explored in Appendix E. LAMA We evaluate our models on the SQuAD, Google-RE and T-REx subsets of the LAMA benchmark (Petroni et al., 2019).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': 'ac8d33e4c0a45e227a47353f3f26fbb231482dc1',\n","   'title': 'Time-Aware Language Models as Temporal Knowledge Bases',\n","   'influentialCitationCount': 21,\n","   'publicationDate': '2021-06-29'}},\n"," {'contexts': ['…(Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006), sequence generation (He et al., 2020), few-shot text classification (Schick and Schütze, 2021a) and retrieval (Izacard and Grave, 2021) to reasoning (Zelikman et al., 2022).',\n","   '…to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero-and few-shot settings (Jiang et al., 2020; Schick and Schütze, 2021a).',\n","   '…these goals is based on the recent idea of using large LMs with in-context learning (Brown et al., 2020) to generate entire datasets from scratch (Schick and Schütze, 2021b; The New England Journal of Medicine is a registered trademark of [QA(“Who is the publisher of The New England Journal of…',\n","   'A potential solution to this problem might be to iteratively apply our approach, similar to how this is done in related bootstrapping approaches (Schick and Schütze, 2021a; Izacard and Grave, 2021; Parisi et al., 2022).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': 'b769b629c8de35b16735214251d6b4e99cb55762',\n","   'title': 'Generating Datasets with Pretrained Language Models',\n","   'influentialCitationCount': 18,\n","   'publicationDate': '2021-04-15'}},\n"," {'contexts': ['…tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time (Dhingra et al., 2022).',\n","   'We test mathematical abilities on ASDiv (Miao et al., 2020), SVAMP (Patel et al., 2021) and the MAWPS benchmark (Koncel-Kedziorski et al., 2016).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': '13c4e5a6122f3fa2663f63e49537091da6532f35',\n","   'title': 'Are NLP Models really able to Solve Simple Math Word Problems?',\n","   'influentialCitationCount': 113,\n","   'publicationDate': '2021-03-11'}},\n"," {'contexts': ['A potential reason for GPT-J not suffering from this problem is that it was trained on more multilingual data than both OPT and GPT-3, including the EuroParl corpus (Koehn, 2005; Gao et al., 2020).'],\n","  'intents': [],\n","  'citedPaper': {'paperId': 'db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e',\n","   'title': 'The Pile: An 800GB Dataset of Diverse Text for Language Modeling',\n","   'influentialCitationCount': 213,\n","   'publicationDate': '2020-12-31'}},\n"," {'contexts': ['…(Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006), sequence generation (He et al., 2020), few-shot text classification (Schick and Schütze, 2021a) and retrieval (Izacard and Grave, 2021) to reasoning (Zelikman et al., 2022).',\n","   'A potential solution to this problem might be to iteratively apply our approach, similar to how this is done in related bootstrapping approaches (Schick and Schütze, 2021a; Izacard and Grave, 2021; Parisi et al., 2022).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': '2b4bc49a3b23229a060609380752666b24b435fb',\n","   'title': 'Distilling Knowledge from Reader to Retriever for Question Answering',\n","   'influentialCitationCount': 44,\n","   'publicationDate': '2020-12-08'}},\n"," {'contexts': ['As our search engine, we use a BM25 retriever (Robertson et al., 1995; Baeza-Yates et al., 1999) that indexes the Wikipedia dump from KILT (Petroni et al., 2021).'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': '0b09448f7543453cc066416f547292dc1e4471f6',\n","   'title': 'KILT: a Benchmark for Knowledge Intensive Language Tasks',\n","   'influentialCitationCount': 54,\n","   'publicationDate': '2020-09-04'}},\n"," {'contexts': ['We test mathematical abilities on ASDiv (Miao et al., 2020), SVAMP (Patel et al., 2021) and the MAWPS benchmark (Koncel-Kedziorski et al., 2016).'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': 'f13e41d24e5d0a68ca662c1b49de398a6fb68251',\n","   'title': 'A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers',\n","   'influentialCitationCount': 39,\n","   'publicationDate': '2020-07-01'}},\n"," {'contexts': ['Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).',\n","   'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.',\n","   '…that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model (Brown et al., 2020) and several other baselines on various tasks.',\n","   'Our approach for achieving these goals is based on the recent idea of using large LMs with in-context learning (Brown et al., 2020) to generate entire datasets from scratch (Schick and Schütze, 2021b; The New England Journal of Medicine is a registered trademark of [QA(“Who is the publisher of The…'],\n","  'intents': ['background', 'result', 'methodology'],\n","  'citedPaper': {'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0',\n","   'title': 'Language Models are Few-Shot Learners',\n","   'influentialCitationCount': 2951,\n","   'publicationDate': '2020-05-28'}},\n"," {'contexts': ['…include an inability to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'dbeeca8466e0c177ec67c60d529899232415ca87',\n","   'title': 'On Faithfulness and Factuality in Abstractive Summarization',\n","   'influentialCitationCount': 99,\n","   'publicationDate': '2020-05-02'}},\n"," {'contexts': ['…information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '832fff14d2ed50eb7969c4c4b976c35776548f56',\n","   'title': 'REALM: Retrieval-Augmented Language Model Pre-Training',\n","   'influentialCitationCount': 165,\n","   'publicationDate': '2020-02-10'}},\n"," {'contexts': ['…(Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006), sequence generation (He et al., 2020), few-shot text classification (Schick and Schütze, 2021a) and retrieval (Izacard and Grave, 2021) to reasoning (Zelikman et al., 2022).',\n","   '…to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero-and few-shot settings (Jiang et al., 2020; Schick and Schütze, 2021a).',\n","   '…these goals is based on the recent idea of using large LMs with in-context learning (Brown et al., 2020) to generate entire datasets from scratch (Schick and Schütze, 2021b; The New England Journal of Medicine is a registered trademark of [QA(“Who is the publisher of The New England Journal of…',\n","   'A potential solution to this problem might be to iteratively apply our approach, similar to how this is done in related bootstrapping approaches (Schick and Schütze, 2021a; Izacard and Grave, 2021; Parisi et al., 2022).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': '8ae9a17c87a4518b513e860683a0ef7824be994d',\n","   'title': 'Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference',\n","   'influentialCitationCount': 143,\n","   'publicationDate': '2020-01-21'}},\n"," {'contexts': ['…to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero-and few-shot settings (Jiang et al., 2020; Schick and Schütze, 2021a).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'a75649771901a4881b44c0ceafa469fcc6e6f968',\n","   'title': 'How Can We Know What Language Models Know?',\n","   'influentialCitationCount': 49,\n","   'publicationDate': '2019-11-28'}},\n"," {'contexts': ['In our experiments, we mainly compare GPT-J and the following models: • GPT-J + CC : GPT-J finetuned on C , our subset of CCNet without any API calls.',\n","   'However, Toolformer does not consistently outperform GPT-J as finetuning on CCNet deteriorates performance for some languages.',\n","   'To this end, we evaluate our models on two language modeling datasets: WikiText (Merity et al., 2017) and a subset of 10,000 randomly selected documents from CCNet (Wenzek et al., 2020) that were not used during training.',\n","   'We use a subset of CCNet (Wenzek et al., 2020) as our dataset C and GPT-J (Wang and Komatsuzaki, 2021) as our language model M .',\n","   '• Toolformer : GPT-J finetuned on C ∗ , our subset of CCNet augmented with API calls.',\n","   'Finetuning on CCNet leads to slightly improved performance on the CCNet evaluation subset (perplexity improves from 10.6 to 10.5), but slightly deteriorates performance on WikiText (9.9 to 10.3), presumably because the original pretraining data for GPT-J is more similar to WikiText than our subset of CCNet.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': 'c20c68c45127439139a08adb0b1f2b8354a94d6c',\n","   'title': 'CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data',\n","   'influentialCitationCount': 49,\n","   'publicationDate': '2019-11-01'}},\n"," {'contexts': ['Multilingual QA We evaluate all models on MLQA (Lewis et al., 2019), a multilingual QA benchmark.',\n","   'In this setup, GPT-3 performs better than all other models, supporting our hypothesis that its subpar performance on MLQA is due to the task’s multilingual aspect.',\n","   'As an upper bound, we also evaluate GPT-J and GPT-3 on a variant of MLQA where both the context and the question are provided in English.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': '2e347a977f14eca7cc5bbbb4c71145b75637340c',\n","   'title': 'MLQA: Evaluating Cross-lingual Extractive Question Answering',\n","   'influentialCitationCount': 111,\n","   'publicationDate': '2019-10-16'}},\n"," {'contexts': ['…(Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006), sequence generation (He et al., 2020), few-shot text classification (Schick and Schütze, 2021a) and retrieval (Izacard and Grave, 2021) to reasoning (Zelikman et al.,…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '12442420adf1c36887fafd108f4b7f4fc822ae60',\n","   'title': 'Revisiting Self-Training for Neural Sequence Generation',\n","   'influentialCitationCount': 30,\n","   'publicationDate': '2019-09-30'}},\n"," {'contexts': ['…are various approaches that augment LMs with some form of additional textual information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '75acc731bdd2b626edc74672a30da3bc51010ae8',\n","   'title': 'CTRL: A Conditional Transformer Language Model for Controllable Generation',\n","   'influentialCitationCount': 147,\n","   'publicationDate': '2019-09-11'}},\n"," {'contexts': ['The effect of these modifications is explored in Appendix E. LAMA We evaluate our models on the SQuAD, Google-RE and T-REx subsets of the LAMA benchmark (Petroni et al., 2019).'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': 'd0086b86103a620a86bc918746df0aa642e2a8a3',\n","   'title': 'Language Models as Knowledge Bases?',\n","   'influentialCitationCount': 250,\n","   'publicationDate': '2019-09-01'}},\n"," {'contexts': ['Question Answering We look at Web Questions (Berant et al., 2013), Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017).',\n","   'Specifically, we use Atlas (Izacard et al., 2022), a retrieval-augmented LM finetuned on Natural Questions (Kwiatkowski et al., 2019).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': '17dbd7b72029181327732e4d11b52a08ed4630d0',\n","   'title': 'Natural Questions: A Benchmark for Question Answering Research',\n","   'influentialCitationCount': 308,\n","   'publicationDate': '2019-08-01'}},\n"," {'contexts': ['Question Answering We look at Web Questions (Berant et al., 2013), Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'f010affab57b5fcf1cd6be23df79d8ec98c7289c',\n","   'title': 'TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension',\n","   'influentialCitationCount': 336,\n","   'publicationDate': '2017-05-01'}},\n"," {'contexts': ['The source language is automatically detected using the fastText classifier (Joulin et al., 2016), while the target language is always set to English.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': '5feb32a73dd1bd9e13f84a7b3344497a5545106b',\n","   'title': 'FastText.zip: Compressing text classification models',\n","   'influentialCitationCount': 91,\n","   'publicationDate': '2016-12-12'}},\n"," {'contexts': ['Finetuning on CCNet leads to slightly improved performance on the CCNet evaluation subset (perplexity improves from 10.6 to 10.5), but slightly deteriorates performance on WikiText (9.9 to 10.3), presumably because the original pretraining data for GPT-J is more similar to WikiText than our subset of CCNet.',\n","   'To this end, we evaluate our models on two language modeling datasets: WikiText (Merity et al., 2017) and a subset of 10,000 randomly selected documents from CCNet (Wenzek et al., 2020) that were not used during training.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': 'efbd381493bb9636f489b965a2034d529cd56bcd',\n","   'title': 'Pointer Sentinel Mixture Models',\n","   'influentialCitationCount': 345,\n","   'publicationDate': '2016-09-26'}},\n"," {'contexts': ['We test mathematical abilities on ASDiv (Miao et al., 2020), SVAMP (Patel et al., 2021) and the MAWPS benchmark (Koncel-Kedziorski et al., 2016).'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': '2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9',\n","   'title': 'MAWPS: A Math Word Problem Repository',\n","   'influentialCitationCount': 52,\n","   'publicationDate': '2016-06-12'}},\n"," {'contexts': ['Question Answering We look at Web Questions (Berant et al., 2013), Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'b29447ba499507a259ae9d8f685d60cc1597d7d3',\n","   'title': 'Semantic Parsing on Freebase from Question-Answer Pairs',\n","   'influentialCitationCount': 243,\n","   'publicationDate': '2013-10-01'}},\n"," {'contexts': ['Bootstrapping The idea of using self-training and bootstrapping techniques to improve models has been investigated in various contexts, ranging from word sense disambiguation (Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006; Reichart and Rappoport, 2007), sequence generation (He et al.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': '8067a93316a34181fbb4b765310f15e11925d5f3',\n","   'title': 'Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets',\n","   'influentialCitationCount': 5,\n","   'publicationDate': '2007-06-01'}},\n"," {'contexts': ['…various contexts, ranging from word sense disambiguation (Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006), sequence generation (He et al., 2020), few-shot text classification (Schick and Schütze, 2021a) and retrieval (Izacard and…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '78a9513e70f596077179101f6cb6eadc51602039',\n","   'title': 'Effective Self-Training for Parsing',\n","   'influentialCitationCount': 58,\n","   'publicationDate': '2006-06-04'}},\n"," {'contexts': ['…improve models has been investigated in various contexts, ranging from word sense disambiguation (Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006), sequence generation (He et al., 2020), few-shot text classification (Schick and…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'cee045e890270abae65455667b292db355d53728',\n","   'title': 'Snowball: extracting relations from large plain-text collections',\n","   'influentialCitationCount': 112,\n","   'publicationDate': '2000-06-01'}},\n"," {'contexts': ['…to improve models has been investigated in various contexts, ranging from word sense disambiguation (Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006), sequence generation (He et al., 2020), few-shot text classification…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '92575a3c554353a27b2c0263ad7f8487d9102301',\n","   'title': 'Extracting Patterns and Relations from the World Wide Web',\n","   'influentialCitationCount': 99,\n","   'publicationDate': '1998-03-27'}},\n"," {'contexts': ['…self-training and bootstrapping techniques to improve models has been investigated in various contexts, ranging from word sense disambiguation (Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006), sequence generation (He et al.,…'],\n","  'intents': [],\n","  'citedPaper': {'paperId': '944cba683d10d8c1a902e05cd68e32a9f47b372e',\n","   'title': 'Unsupervised Word Sense Disambiguation Rivaling Supervised Methods',\n","   'influentialCitationCount': 181,\n","   'publicationDate': '1995-06-26'}},\n"," {'contexts': ['…ability to use external tools such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et…',\n","   '…of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which needs to be used (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '99832586d55f540f603637e458a292406a0ed75d',\n","   'title': 'LANGUAGE MODELS',\n","   'influentialCitationCount': 131,\n","   'publicationDate': None}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': None,\n","   'title': '2022) as our machine translation (MT) model. The source language is automatically detected using the fastText classifier (Joulin',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': ['In our experiments, we mainly compare GPT-J and the following models: • GPT-J + CC : GPT-J finetuned on C , our subset of CCNet without any API calls.',\n","   'However, Toolformer does not consistently outperform GPT-J as finetuning on CCNet deteriorates performance for some languages.',\n","   '…on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model (Brown et…',\n","   'To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively.',\n","   'Once again, Toolformer clearly outperforms all other models based on GPT-J, relying on the Wikipedia search API (99.3',\n","   'We conduct experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model (Brown et al., 2020) and several other baselines on various tasks.',\n","   'While GPT-J and GPT-J + CC perform about the same, Toolformer achieves stronger results even without API calls.',\n","   'However, Figure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of different sizes and GPT-J finetuned with our approach, both with and without API calls.',\n","   'Toolformer considerably improves zero-shot performance of a 6.7B parameter GPT-J model, enabling it to even outperform a much larger GPT-3 model on a range of different downstream tasks.',\n","   'As an upper bound, we also evaluate GPT-J and GPT-3 on a variant of MLQA where both the context and the question are provided in English.',\n","   'We use a subset of CCNet (Wenzek et al., 2020) as our dataset C and GPT-J (Wang and Komatsuzaki, 2021) as our language model M .',\n","   '• Toolformer : GPT-J finetuned on C ∗ , our subset of CCNet augmented with API calls.',\n","   'As shown in Table 3 (left), all GPT-J models without tool use achieve similar performance.',\n","   'Finetuning on CCNet leads to slightly improved performance on the CCNet evaluation subset (perplexity improves from 10.6 to 10.5), but slightly deteriorates performance on WikiText (9.9 to 10.3), presumably because the original pretraining data for GPT-J is more similar to WikiText than our subset of CCNet.',\n","   'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.',\n","   'A potential reason for GPT-J not suffering from this problem is that it was trained on more multilingual data than both OPT and GPT-3, including EuroParl (Koehn, 2005).'],\n","  'intents': ['result', 'methodology'],\n","  'citedPaper': {'paperId': None,\n","   'title': 'GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': ['…2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '1403e6b9adf7712c35ae56327d52fe54603b87e1',\n","   'title': 'Few-shot Learning with Multilingual Language Models',\n","   'influentialCitationCount': 19,\n","   'publicationDate': None}},\n"," {'contexts': ['As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n","   'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n","   'influentialCitationCount': 17772,\n","   'publicationDate': None}},\n"," {'contexts': ['To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': '9405cc0d6169988371b2755e573cc28650d14dfe',\n","   'title': 'Language Models are Unsupervised Multitask Learners',\n","   'influentialCitationCount': 2742,\n","   'publicationDate': None}},\n"," {'contexts': ['A potential reason for GPT-J not suffering from this problem is that it was trained on more multilingual data than both OPT and GPT-3, including EuroParl (Koehn, 2005).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '694b3c58712deefb59502847ba1b52b192c413e5',\n","   'title': 'Europarl: A Parallel Corpus for Statistical Machine Translation',\n","   'influentialCitationCount': 501,\n","   'publicationDate': None}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': None,\n","   'title': 'Modern information retrieval , volume 463',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': ['As our search engine, we use a BM25 retriever (Robertson et al., 1995; Baeza-Yates et al., 1999) that indexes the Wikipedia dump from KILT (Petroni et al., 2021).'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': 'd2071c1e4a6030dc0005dbfeefdd196a8b293e84',\n","   'title': 'GatfordCentre for Interactive Systems ResearchDepartment of Information',\n","   'influentialCitationCount': 272,\n","   'publicationDate': None}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': None,\n","   'title': 'Input: Metformin is the first-line drug for patients with type 2 diabetes and obesity',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': None,\n","   'title': 'Output: But what are the risks',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': None,\n","   'title': 'Output: The population is 658,893 people. This is 11.4% of the national average of [Calculator(658,893 / 11.4%)] 5,763,',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': ['We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'],\n","  'intents': ['result'],\n","  'citedPaper': {'paperId': None,\n","   'title': 'pre-trained',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': None,\n","   'title': ': 南 京 高 淳 县 住 房 和 城 乡 建 设 局 城 市 新 区 设 计 a plane of reference Gaochun is one of seven districts of the provincial capital Nanjing',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': None,\n","   'title': '16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': ['The effect of these modifications is explored in Appendix E. LAMA We evaluate our models on the SQuAD, Google-RE and T-REx subsets of the LAMA benchmark (Petroni et al., 2019).'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': '13167f9cd8c7906ca808b01d28dca6dd951da8a5',\n","   'title': 'of the Association for Computational Linguistics',\n","   'influentialCitationCount': 273,\n","   'publicationDate': None}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': None,\n","   'title': '2022. Star: Bootstrapping',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}},\n"," {'contexts': ['…of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which needs to be used (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022).',\n","   '…Use Several approaches aim to equip LMs with the ability to use external tools such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': None,\n","   'title': '2022. Internet-augmented language models through few-shot prompting for open-domain question answering',\n","   'influentialCitationCount': None,\n","   'publicationDate': None}}]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# target paper가 citations 한 paper 중 influentialCitationCount가 높은 20개만\n","\n","# None 값을 처리하는 함수\n","def get_citation_count(item):\n","    influential_citation_count = item.get('influentialCitationCount')\n","    if influential_citation_count is not None:\n","        return influential_citation_count\n","    else:\n","        return 0\n","\n","# 정렬\n","sorted_papers = sorted(response['data'], key=get_citation_count, reverse=True)[:20]\n","sorted_papers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FeudOif4--Tk","executionInfo":{"status":"ok","timestamp":1713953448184,"user_tz":-540,"elapsed":353,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"e64daa47-f9ac-4232-b119-33ec347f3abb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'contexts': ['Honovich et al., 2022; Wang et al., 2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': 'e65b346d442e9962a4276dc1c1af2956d9d5f1eb',\n","   'title': 'Self-Instruct: Aligning Language Models with Self-Generated Instructions',\n","   'influentialCitationCount': 117,\n","   'publicationDate': '2022-12-20'}},\n"," {'contexts': ['Honovich et al., 2022; Wang et al., 2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls.'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': '6f4cc536f9ed83d0dbf7e919dc609be12aa0848a',\n","   'title': 'Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor',\n","   'influentialCitationCount': 18,\n","   'publicationDate': '2022-12-19'}},\n"," {'contexts': ['…(Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., 2022) and Python interpreters (Gao et al., 2022).',\n","   'However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.',\n","   ', 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be used (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022).',\n","   'This is in contrast to prior work on tool use (e.g., Gao et al., 2022; Parisi et al., 2022), where models are provided with dataset-specific examples of how a tool can be used to solve a concrete task.',\n","   '…of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which needs to be used (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022).'],\n","  'intents': ['background', 'result', 'methodology'],\n","  'citedPaper': {'paperId': '6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7',\n","   'title': 'PAL: Program-aided Language Models',\n","   'influentialCitationCount': 45,\n","   'publicationDate': '2022-11-18'}},\n"," {'contexts': [', 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al.',\n","   '…information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'a938ff4539b09a785a66669844f1a35f76169218',\n","   'title': 'PEER: A Collaborative Language Model',\n","   'influentialCitationCount': 8,\n","   'publicationDate': '2022-08-24'}},\n"," {'contexts': ['Specifically, we use Atlas (Izacard et al., 2022), a retrieval-augmented LM finetuned on Natural Questions (Kwiatkowski et al.',\n","   ', 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).',\n","   '…information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).',\n","   'Specifically, we use Atlas (Izacard et al., 2022), a retrieval-augmented LM finetuned on Natural Questions (Kwiatkowski et al., 2019).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': '398e4061dde8f5c80606869cebfa2031de7b5b74',\n","   'title': 'Few-shot Learning with Retrieval Augmented Language Models',\n","   'influentialCitationCount': 44,\n","   'publicationDate': '2022-08-05'}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': 'a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd',\n","   'title': 'BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage',\n","   'influentialCitationCount': 22,\n","   'publicationDate': '2022-08-05'}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': 'e19b54ad4c1c8af045069e9cac350ffc2ce60e1a',\n","   'title': 'No Language Left Behind: Scaling Human-Centered Machine Translation',\n","   'influentialCitationCount': 44,\n","   'publicationDate': '2022-07-11'}},\n"," {'contexts': ['However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.',\n","   'This is in contrast to prior work on tool use (e.g., Gao et al., 2022; Parisi et al., 2022), where models are provided with dataset-specific examples of how a tool can be used to solve a concrete task.',\n","   'Perhaps most closely related to our work is TALM (Parisi et al., 2022), an approach that uses a similar self-supervised objective for teaching a model to use a calculator and a search engine, but explores this only in settings where a model is finetuned for downstream tasks.',\n","   'A potential solution to this problem might be to iteratively apply our approach, similar to how this is done in related bootstrapping approaches (Schick and Schütze, 2021a; Izacard and Grave, 2021; Parisi et al., 2022).'],\n","  'intents': ['result', 'methodology'],\n","  'citedPaper': {'paperId': '354bf043179e3e9f05df73e3f04517e53c326d1f',\n","   'title': 'TALM: Tool Augmented Language Models',\n","   'influentialCitationCount': 10,\n","   'publicationDate': '2022-05-24'}},\n"," {'contexts': ['We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'],\n","  'intents': ['result'],\n","  'citedPaper': {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221',\n","   'title': 'OPT: Open Pre-trained Transformer Language Models',\n","   'influentialCitationCount': 256,\n","   'publicationDate': '2022-05-02'}},\n"," {'contexts': ['Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb',\n","   'title': 'PaLM: Scaling Language Modeling with Pathways',\n","   'influentialCitationCount': 295,\n","   'publicationDate': '2022-04-05'}},\n"," {'contexts': ['…to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise…',\n","   ', 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al.'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '3def68bd0f856886d34272840a7f81588f2bc082',\n","   'title': 'Survey of Hallucination in Natural Language Generation',\n","   'influentialCitationCount': 55,\n","   'publicationDate': '2022-02-08'}},\n"," {'contexts': [],\n","  'intents': [],\n","  'citedPaper': {'paperId': 'b3848d32f7294ec708627897833c4097eb4d8778',\n","   'title': 'LaMDA: Language Models for Dialog Applications',\n","   'influentialCitationCount': 88,\n","   'publicationDate': '2022-01-20'}},\n"," {'contexts': ['…a tool in an interactive way – especially for tools such as search engines, that could potentially return hundreds of different results, enabling a LM to browse through these results or to refine its search query in a similar spirit to Nakano et al. (2021) can be crucial for certain applications.',\n","   '…such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., 2022) and Python interpreters…',\n","   '…models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where…'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '2f3efe44083af91cef562c1a3451eee2f8601d22',\n","   'title': 'WebGPT: Browser-assisted question-answering with human feedback',\n","   'influentialCitationCount': 71,\n","   'publicationDate': '2021-12-17'}},\n"," {'contexts': ['…information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': '002c256d30d6be4b23d365a8de8ae0e67e4c9641',\n","   'title': 'Improving language models by retrieving from trillions of tokens',\n","   'influentialCitationCount': 53,\n","   'publicationDate': '2021-12-08'}},\n"," {'contexts': ['…(Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., 2022) and Python interpreters (Gao et al., 2022).'],\n","  'intents': ['background'],\n","  'citedPaper': {'paperId': 'd6045d2ccc9c09ca1671348de86d07da6bc28eea',\n","   'title': 'Training Verifiers to Solve Math Word Problems',\n","   'influentialCitationCount': 357,\n","   'publicationDate': '2021-10-27'}},\n"," {'contexts': ['Tool Use Several approaches aim to equip LMs with the ability to use external tools such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al.',\n","   'These limitations include an inability to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al.',\n","   'However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.',\n","   'These limitations include an inability to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical…',\n","   'The way these models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be used (Gao et al.',\n","   'The way these models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific…',\n","   'However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.',\n","   'Tool Use Several approaches aim to equip LMs with the ability to use external tools such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan…'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': 'de549c1592a62c129b8d49c8c0137aa6859b103f',\n","   'title': 'Internet-Augmented Dialogue Generation',\n","   'influentialCitationCount': 44,\n","   'publicationDate': '2021-07-15'}},\n"," {'contexts': ['…LMs with some form of additional textual information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020;…'],\n","  'intents': ['methodology'],\n","  'citedPaper': {'paperId': 'e596b8adbffa546dbc163e817fb3de72744ec4f6',\n","   'title': 'HTLM: Hyper-Text Pre-Training and Prompting of Language Models',\n","   'influentialCitationCount': 2,\n","   'publicationDate': '2021-07-14'}},\n"," {'contexts': ['Even for bigger models, the gap between predictions with and without API calls remains high. closer inspection shows that improvements on T EMP LAMA can not be attributed to the calendar tool, which is only used for 0.2% of all examples, but mostly to the Wikipedia search and question answering tools.',\n","   'As LAMA is based on statements obtained directly from Wikipedia, we prevent Toolformer from using the Wikipedia Search API to avoid giving it an unfair advantage.',\n","   'However, Figure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of different sizes and GPT-J finetuned with our approach, both with and without API calls.',\n","   'T EMP LAMA contains cloze queries about facts that change with time.',\n","   'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.',\n","   '…tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time (Dhingra et al., 2022).',\n","   'For both tasks, we use the same evaluation as for the original LAMA dataset.',\n","   'Results shown in Table 4 (right) illustrate that Toolformer outperforms all baselines for both T EMP LAMA and D ATESET .',\n","   'We evaluate all models on T EMP LAMA (Dhingra et al., 2022) and a new dataset that we call D ATESET .',\n","   'The effect of these modifications is explored in Appendix E. LAMA We evaluate our models on the SQuAD, Google-RE and T-REx subsets of the LAMA benchmark (Petroni et al., 2019).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': 'ac8d33e4c0a45e227a47353f3f26fbb231482dc1',\n","   'title': 'Time-Aware Language Models as Temporal Knowledge Bases',\n","   'influentialCitationCount': 21,\n","   'publicationDate': '2021-06-29'}},\n"," {'contexts': ['…(Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006), sequence generation (He et al., 2020), few-shot text classification (Schick and Schütze, 2021a) and retrieval (Izacard and Grave, 2021) to reasoning (Zelikman et al., 2022).',\n","   '…to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero-and few-shot settings (Jiang et al., 2020; Schick and Schütze, 2021a).',\n","   '…these goals is based on the recent idea of using large LMs with in-context learning (Brown et al., 2020) to generate entire datasets from scratch (Schick and Schütze, 2021b; The New England Journal of Medicine is a registered trademark of [QA(“Who is the publisher of The New England Journal of…',\n","   'A potential solution to this problem might be to iteratively apply our approach, similar to how this is done in related bootstrapping approaches (Schick and Schütze, 2021a; Izacard and Grave, 2021; Parisi et al., 2022).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': 'b769b629c8de35b16735214251d6b4e99cb55762',\n","   'title': 'Generating Datasets with Pretrained Language Models',\n","   'influentialCitationCount': 18,\n","   'publicationDate': '2021-04-15'}},\n"," {'contexts': ['…tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time (Dhingra et al., 2022).',\n","   'We test mathematical abilities on ASDiv (Miao et al., 2020), SVAMP (Patel et al., 2021) and the MAWPS benchmark (Koncel-Kedziorski et al., 2016).'],\n","  'intents': ['background', 'methodology'],\n","  'citedPaper': {'paperId': '13c4e5a6122f3fa2663f63e49537091da6532f35',\n","   'title': 'Are NLP Models really able to Solve Simple Math Word Problems?',\n","   'influentialCitationCount': 113,\n","   'publicationDate': '2021-03-11'}}]"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Citations Paper"],"metadata":{"id":"GzpziWa4LElt"}},{"cell_type":"code","source":["def query2citations(query, num=20):\n","    # Define the API endpoint URL\n","    url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n","\n","    # paper name 기입\n","    query_params = {'query': query,'fields': 'citations,citations.influentialCitationCount,citations.title,citations.publicationDate,citations.abstract'}\n","\n","    # semantic scholarship api 넣는다.\n","    api_key = ''\n","    headers = {'x-api-key': api_key}\n","\n","    citations_response = requests.get(url, params=query_params, headers=headers).json()\n","    paper_id = citations_response['data'][0]['paperId']\n","\n","    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=abstract'\n","\n","    target_response = requests.get(url, params=query_params, headers=headers).json()\n","\n","    return target_response, sorted(citations_response['data'][0]['citations'], key=get_citation_count, reverse=True)[:num]\n","\n","def get_citation_count(item):\n","    influential_citation_count = item.get('influentialCitationCount')\n","    if influential_citation_count is not None:\n","        return influential_citation_count\n","    else:\n","        return 0\n","\n","query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n","num=20\n","target_response, citation_response = query2citations(query=query,num=num)"],"metadata":{"id":"3oGSOnwhJl_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MANNtlGtL0eg","executionInfo":{"status":"ok","timestamp":1713953516939,"user_tz":-540,"elapsed":343,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"ace2b44e-4f82-4185-c51b-da9259925f13"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'paperId': '53d128ea815bcc0526856eb5a9c42cc977cb36a7',\n"," 'abstract': 'Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.'}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["citation_response[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOhn9jLvMhpH","executionInfo":{"status":"ok","timestamp":1713953518112,"user_tz":-540,"elapsed":3,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"e0fd1da3-3888-4640-9f0b-7cffb575dfb0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'paperId': '104b0bb1da562d53cbda87aec79ef6a2827d191a',\n"," 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models',\n"," 'abstract': 'In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.',\n"," 'influentialCitationCount': 566,\n"," 'publicationDate': '2023-07-18'}"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["# 단어 임베딩 불러오기\n","word_embeddings = {}\n","\n","with open(\"/content/drive/MyDrive/consult-proj/glove.6B.100d.txt\", 'r', encoding='utf-8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        vector = np.asarray(values[1:], \"float32\")\n","        word_embeddings[word] = vector"],"metadata":{"id":"czUHhsQPNqA-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["description = target_response['abstract']\n","\n","description_tokens = word_tokenize(description.lower())\n","description_vectors = [word_embeddings[word] for word in description_tokens if word in word_embeddings]\n","\n","sim_dict = {}\n","for keyword in citation_response:\n","    # 토큰화 및 토큰 벡터화\n","    paper_id, title = keyword['paperId'], keyword['title']\n","    abstract = str(keyword['abstract'])\n","\n","    keyword_tokens = word_tokenize(abstract.lower())\n","    keyword_vectors = [word_embeddings[word] for word in keyword_tokens if word in word_embeddings]\n","\n","    # 벡터 평균 계산\n","    description_vector_avg = np.mean(description_vectors, axis=0)\n","    keyword_vector_avg = np.mean(keyword_vectors, axis=0)\n","\n","    # 코사인 유사도 계산\n","    similarity = cosine_similarity([description_vector_avg], [keyword_vector_avg])\n","    sim_dict[title] = similarity[0][0]\n","\n","sim_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kG_WeginPQNR","executionInfo":{"status":"ok","timestamp":1713956144948,"user_tz":-540,"elapsed":3,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"67280006-4c78-4dd6-a597-7bab76350695"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Llama 2: Open Foundation and Fine-Tuned Chat Models': 0.9852161,\n"," 'A Survey of Large Language Models': 0.9885709,\n"," 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks': 0.98989874,\n"," 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face': 0.99058336,\n"," 'Qwen Technical Report': 0.9888913,\n"," 'Reflexion: language agents with verbal reinforcement learning': 0.9855268,\n"," 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs': 0.98978454,\n"," 'ViperGPT: Visual Inference via Python Execution for Reasoning': 0.9814495,\n"," 'Lost in the Middle: How Language Models Use Long Contexts': 0.9844477,\n"," 'Enabling Large Language Models to Generate Text with Citations': 0.98905426,\n"," 'Gorilla: Large Language Model Connected with Massive APIs': 0.9928637,\n"," 'A Survey on Large Language Model based Autonomous Agents': 0.6698898,\n"," 'Mind2Web: Towards a Generalist Agent for the Web': 0.9879962,\n"," 'Segment Everything Everywhere All at Once': 0.9907112,\n"," 'A Survey on Evaluation of Large Language Models': 0.985506,\n"," 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency': 0.9923949,\n"," 'GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models': 0.980513,\n"," 'Reasoning with Language Model is Planning with World Model': 0.9832901,\n"," 'MetaGPT: Meta Programming for Multi-Agent Collaborative Framework': 0.9815379,\n"," 'Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models': 0.9871513}"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["def similarityBtn(target_response, citation_response):\n","    description = target_response['abstract']\n","\n","    description_tokens = word_tokenize(description.lower())\n","    description_vectors = [word_embeddings[word] for word in description_tokens if word in word_embeddings]\n","\n","    sim_dict = {}\n","    for keyword in citation_response:\n","        # 토큰화 및 토큰 벡터화\n","        paper_id, title = keyword['paperId'], keyword['title']\n","        abstract = str(keyword['abstract'])\n","\n","        keyword_tokens = word_tokenize(abstract.lower())\n","        keyword_vectors = [word_embeddings[word] for word in keyword_tokens if word in word_embeddings]\n","\n","        # 벡터 평균 계산\n","        description_vector_avg = np.mean(description_vectors, axis=0)\n","        keyword_vector_avg = np.mean(keyword_vectors, axis=0)\n","\n","        # 코사인 유사도 계산\n","        similarity = cosine_similarity([description_vector_avg], [keyword_vector_avg])\n","        sim_dict[paper_id, title] = similarity[0][0]\n","\n","    return sim_dict\n","\n","similarityBtn(target_response, citation_response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QclH873qP-WX","executionInfo":{"status":"ok","timestamp":1713956242452,"user_tz":-540,"elapsed":341,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"89ad8a93-fc2c-428b-c4ee-7b0854f7db03"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{('104b0bb1da562d53cbda87aec79ef6a2827d191a',\n","  'Llama 2: Open Foundation and Fine-Tuned Chat Models'): 0.9852161,\n"," ('c61d54644e9aedcfc756e5d6fe4cc8b78c87755d',\n","  'A Survey of Large Language Models'): 0.9885709,\n"," ('6c943670dca38bfc7c8b477ae7c2d1fba1ad3691',\n","  'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks'): 0.98989874,\n"," ('d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43',\n","  'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face'): 0.99058336,\n"," ('5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0',\n","  'Qwen Technical Report'): 0.9888913,\n"," ('0671fd553dd670a4e820553a974bc48040ba0819',\n","  'Reflexion: language agents with verbal reinforcement learning'): 0.9855268,\n"," ('0bfc804e31eecfd77f45e4ee7f4d629fffdcd628',\n","  'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs'): 0.98978454,\n"," ('6e754273d54a91371efbc928cd6b156364d517da',\n","  'ViperGPT: Visual Inference via Python Execution for Reasoning'): 0.9814495,\n"," ('1733eb7792f7a43dd21f51f4d1017a1bffd217b5',\n","  'Lost in the Middle: How Language Models Use Long Contexts'): 0.9844477,\n"," ('e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365',\n","  'Enabling Large Language Models to Generate Text with Citations'): 0.98905426,\n"," ('7d8905a1fd288068f12c8347caeabefd36d0dd6c',\n","  'Gorilla: Large Language Model Connected with Massive APIs'): 0.9928637,\n"," ('28c6ac721f54544162865f41c5692e70d61bccab',\n","  'A Survey on Large Language Model based Autonomous Agents'): 0.6698898,\n"," ('58f8925a8b87054ad0635a6398a7fe24935b1604',\n","  'Mind2Web: Towards a Generalist Agent for the Web'): 0.9879962,\n"," ('0819c1e60c13b9797f937282d06b54d252d9d6ec',\n","  'Segment Everything Everywhere All at Once'): 0.9907112,\n"," ('888728745dbb769e29ed475d4f7661eebe1a71cf',\n","  'A Survey on Evaluation of Large Language Models'): 0.985506,\n"," ('003ef1cd670d01af05afa0d3c72d72228f494432',\n","  'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency'): 0.9923949,\n"," ('5501d00310b06e00351295529498cc684187148d',\n","  'GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models'): 0.980513,\n"," ('5dbffedcabe3fa43060ebbe2b1789500edfd871f',\n","  'Reasoning with Language Model is Planning with World Model'): 0.9832901,\n"," ('703035b483c181953de1b55b5fd59cd4cd4cf211',\n","  'MetaGPT: Meta Programming for Multi-Agent Collaborative Framework'): 0.9815379,\n"," ('170c97c7215f42edfb20c2248f954879e91ef86e',\n","  'Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models'): 0.9871513}"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["# FURTHER MORE"],"metadata":{"id":"VJAo9DuFwkjS"}},{"cell_type":"code","source":["!pip install -U sentence-transformers"],"metadata":{"id":"WRTkOdZ6xXo6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","import torch.nn.functional as F\n","\n","#Mean Pooling - Take attention mask into account for correct averaging\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","\n","# Sentences we want sentence embeddings for\n","sentences = ['This is an example sentence', 'Each sentence is converted', 'Hello l am minyku Park']\n","\n","# Load model from HuggingFace Hub\n","tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n","model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n","\n","# Tokenize sentences\n","encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n","\n","# Compute token embeddings\n","with torch.no_grad():\n","    model_output = model(**encoded_input)\n","\n","# Perform pooling\n","sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","\n","# Normalize embeddings\n","sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n","\n","print(cosine_similarity(sentence_embeddings))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4P6RCkSNxtbv","executionInfo":{"status":"ok","timestamp":1713956746042,"user_tz":-540,"elapsed":878,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"d33af0f7-54bb-42d4-9b9d-8d7698fabd95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.0000002  0.40455925 0.17594095]\n"," [0.40455925 1.0000002  0.11156712]\n"," [0.17594095 0.11156712 1.0000002 ]]\n"]}]},{"cell_type":"code","source":["target_response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVTKPaDkyh-w","executionInfo":{"status":"ok","timestamp":1713956863821,"user_tz":-540,"elapsed":3,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"8e26013f-296e-4479-f985-04d9d10e82bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'paperId': '53d128ea815bcc0526856eb5a9c42cc977cb36a7',\n"," 'abstract': 'Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.'}"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["description = target_response['abstract']\n","\n","description_tokens = word_tokenize(description.lower())\n","description_vectors = [word_embeddings[word] for word in description_tokens if word in word_embeddings]\n","\n","sim_dict = {}\n","abs_dict = {}\n","abs_dict[query]\n","\n","for keyword in citation_response:\n","    # 토큰화 및 토큰 벡터화\n","    paper_id, title = keyword['paperId'], keyword['title']\n","    abstract = str(keyword['abstract'])\n","    abs_dict[title] = abstract\n","\n","    keyword_tokens = word_tokenize(abstract.lower())\n","    keyword_vectors = [word_embeddings[word] for word in keyword_tokens if word in word_embeddings]\n","\n","    # 벡터 평균 계산\n","    description_vector_avg = np.mean(description_vectors, axis=0)\n","    keyword_vector_avg = np.mean(keyword_vectors, axis=0)\n","\n","    # 코사인 유사도 계산\n","    similarity = cosine_similarity([description_vector_avg], [keyword_vector_avg])\n","    sim_dict[title] = similarity[0][0]\n","\n","sim_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5dququ1wL-9","executionInfo":{"status":"ok","timestamp":1713956389087,"user_tz":-540,"elapsed":328,"user":{"displayName":"박민규","userId":"12628046480347201618"}},"outputId":"e01d1dee-ab14-41bc-b61d-3ef41df50cd5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Llama 2: Open Foundation and Fine-Tuned Chat Models': 0.9852161,\n"," 'A Survey of Large Language Models': 0.9885709,\n"," 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks': 0.98989874,\n"," 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face': 0.99058336,\n"," 'Qwen Technical Report': 0.9888913,\n"," 'Reflexion: language agents with verbal reinforcement learning': 0.9855268,\n"," 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs': 0.98978454,\n"," 'ViperGPT: Visual Inference via Python Execution for Reasoning': 0.9814495,\n"," 'Lost in the Middle: How Language Models Use Long Contexts': 0.9844477,\n"," 'Enabling Large Language Models to Generate Text with Citations': 0.98905426,\n"," 'Gorilla: Large Language Model Connected with Massive APIs': 0.9928637,\n"," 'A Survey on Large Language Model based Autonomous Agents': 0.6698898,\n"," 'Mind2Web: Towards a Generalist Agent for the Web': 0.9879962,\n"," 'Segment Everything Everywhere All at Once': 0.9907112,\n"," 'A Survey on Evaluation of Large Language Models': 0.985506,\n"," 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency': 0.9923949,\n"," 'GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models': 0.980513,\n"," 'Reasoning with Language Model is Planning with World Model': 0.9832901,\n"," 'MetaGPT: Meta Programming for Multi-Agent Collaborative Framework': 0.9815379,\n"," 'Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models': 0.9871513}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":[],"metadata":{"id":"zioijN48xLG0"},"execution_count":null,"outputs":[]}]}