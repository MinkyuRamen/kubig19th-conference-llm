{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/minkyuramen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
    "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = '/Users/minkyuramen/Desktop/project/env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "# api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# sentence-transformers로 embedding 후 abstract간 cosine similarity를 측정하였다.\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query2references(query, num=20):\n",
    "    \"\"\"References\"\"\"\n",
    "    # Define the API endpoint URL\n",
    "    url = 'https://api.semanticscholar.org/graph/v1/paper/search?fields=paperId,title,abstract'\n",
    "\n",
    "    # paper name 기입\n",
    "    query_params = {'query': query}\n",
    "    headers = {'x-api-key': api_key}\n",
    "\n",
    "    response = requests.get(url, params=query_params, headers=headers).json()\n",
    "    paper_id = response['data'][0]['paperId']\n",
    "\n",
    "\n",
    "    fields = '?fields=title,publicationDate,influentialCitationCount,contexts,intents,abstract'\n",
    "    \"\"\"\n",
    "    context ; snippets of text where the reference is mentioned\n",
    "    intents ; intents derived from the contexts in which this citation is mentioned.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references'+ fields\n",
    "\n",
    "    # Send the API request\n",
    "    response2 = requests.get(url=url, headers=headers).json()\n",
    "    # response\n",
    "\n",
    "    threshold = 10\n",
    "    references = []\n",
    "\n",
    "    for elements in response2['data']:\n",
    "        try:\n",
    "            if elements['citedPaper']['influentialCitationCount'] > threshold:\n",
    "                references.append(elements)\n",
    "            else: pass\n",
    "        except: pass\n",
    "\n",
    "    return response['data'], sorted(references, key=lambda x: x['citedPaper']['influentialCitationCount'], reverse=True)[:num]\n",
    "\n",
    "# query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n",
    "# num = 20\n",
    "\n",
    "# target_response, references = query2references(query, num)\n",
    "# print(target_response)\n",
    "# print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_recommend(query, num=20, threshold=0.6, recommend=5):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    ### 상위 20개의 reference이 많은 논문들을 select\n",
    "    target_response, reference = query2references(query=query,num=num)\n",
    "    reference_response = [ref['citedPaper'] for ref in reference]\n",
    "    reference_context = [ref['contexts'] for ref in reference]\n",
    "    reference_intent = [ref['intents'] for ref in reference]\n",
    "\n",
    "\n",
    "    ## target 논문과 num개의 reference 사이의 유사도 계산\n",
    "    abs_dict = {}\n",
    "    abs_dict[target_response[0]['title']] = target_response[0]['abstract']\n",
    "\n",
    "    for keyword in reference_response:\n",
    "        paper_id, title = keyword['paperId'], keyword['title']\n",
    "        abstract = str(keyword['abstract'])\n",
    "        abs_dict[title] = abstract\n",
    "\n",
    "    sentences = list(abs_dict.values())\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    rec = cosine_similarity(sentence_embeddings)\n",
    "    \n",
    "    ## 유사도가 threshold 이상인 논문들을 추출\n",
    "    indices = np.where(rec[0][1:] > threshold)[0]\n",
    "    rec_lst = [reference_response[i] for i in indices]\n",
    "    rec_context = [reference_context[i] for i in indices]\n",
    "    rec_intent = [reference_intent[i] for i in indices]\n",
    "\n",
    "    for item in rec_lst:\n",
    "        if item['publicationDate'] is None:\n",
    "            item['publicationDate'] = datetime.max\n",
    "\n",
    "    # intent와 context를 list에 추가\n",
    "    for i in range(len(rec_lst)):\n",
    "        rec_lst[i]['intent'] = ' '.join(rec_intent[i])\n",
    "        rec_lst[i]['context'] = rec_context[i][0]\n",
    "    \n",
    "    if len(rec_lst) > recommend:\n",
    "        rec_lst = rec_lst[:recommend]\n",
    "\n",
    "    # 날짜순으로 정렬\n",
    "    return sorted(rec_lst, key=lambda x: datetime.strptime(x['publicationDate'], '%Y-%m-%d') if isinstance(x['publicationDate'], str) else x['publicationDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0',\n",
       "  'title': 'Language Models are Few-Shot Learners',\n",
       "  'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\",\n",
       "  'influentialCitationCount': 3000,\n",
       "  'publicationDate': '2020-05-28',\n",
       "  'intent': 'result methodology background',\n",
       "  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n",
       " {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb',\n",
       "  'title': 'PaLM: Scaling Language Modeling with Pathways',\n",
       "  'abstract': 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.',\n",
       "  'influentialCitationCount': 297,\n",
       "  'publicationDate': '2022-04-05',\n",
       "  'intent': 'background',\n",
       "  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n",
       " {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221',\n",
       "  'title': 'OPT: Open Pre-trained Transformer Language Models',\n",
       "  'abstract': 'Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.',\n",
       "  'influentialCitationCount': 262,\n",
       "  'publicationDate': '2022-05-02',\n",
       "  'intent': 'result',\n",
       "  'context': 'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'},\n",
       " {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       "  'influentialCitationCount': 17880,\n",
       "  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n",
       "  'intent': 'background',\n",
       "  'context': 'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.'},\n",
       " {'paperId': '9405cc0d6169988371b2755e573cc28650d14dfe',\n",
       "  'title': 'Language Models are Unsupervised Multitask Learners',\n",
       "  'abstract': 'Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.',\n",
       "  'influentialCitationCount': 2759,\n",
       "  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n",
       "  'intent': 'methodology',\n",
       "  'context': 'To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n",
    "num = 20\n",
    "threshold = 0.6\n",
    "recommend = 5\n",
    "\n",
    "# recommend 개의 가장 연관된 reference 추천\n",
    "reference_recommend(query=query, num=num, threshold=threshold, recommend=recommend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query2citations(query, num=20):\n",
    "    # Define the API endpoint URL\n",
    "    url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
    "\n",
    "    # paper name 기입\n",
    "    query_params = {'query': query,'fields': 'citations,citations.influentialCitationCount,citations.title,citations.publicationDate,citations.abstract'}\n",
    "\n",
    "    # semantic scholarship api 넣는다.\n",
    "    headers = {'x-api-key': api_key}\n",
    "\n",
    "    citations_response = requests.get(url, params=query_params, headers=headers).json()\n",
    "    paper_id = citations_response['data'][0]['paperId']\n",
    "\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=abstract,title'\n",
    "\n",
    "    target_response = requests.get(url, params=query_params, headers=headers).json()\n",
    "\n",
    "    return target_response, sorted(citations_response['data'][0]['citations'], key=get_citation_count, reverse=True)[:num]\n",
    "\n",
    "def get_citation_count(item):\n",
    "    influential_citation_count = item.get('influentialCitationCount')\n",
    "    if influential_citation_count is not None:\n",
    "        return influential_citation_count\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n",
    "# ## 상위 20개의 citation이 많은 논문들을 select\n",
    "# num=20\n",
    "\n",
    "# target_response, citation_response = query2citations(query=query,num=num)\n",
    "# print(target_response)\n",
    "# print(citation_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citation_recommend(query, num=20, threshold=0.6, recommend=5):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    ### 상위 20개의 citation이 많은 논문들을 select\n",
    "    target_response, citation_response = query2citations(query=query,num=num)\n",
    "\n",
    "    ## target 논문과 20개의 citation 사이의 유사도 계산\n",
    "    abs_dict = {}\n",
    "    abs_dict[target_response['title']] = target_response['abstract']\n",
    "\n",
    "    for keyword in citation_response:\n",
    "        paper_id, title = keyword['paperId'], keyword['title']\n",
    "        abstract = str(keyword['abstract'])\n",
    "        abs_dict[title] = abstract\n",
    "\n",
    "    sentences = list(abs_dict.values())\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    rec = cosine_similarity(sentence_embeddings)\n",
    "    \n",
    "    ## 유사도가 threshold 이상인 논문들을 추출\n",
    "    indices = np.where(rec[0][1:] > threshold)[0]\n",
    "    rec_lst = [citation_response[i] for i in indices]\n",
    "\n",
    "    for item in rec_lst:\n",
    "        if item['publicationDate'] is None:\n",
    "            item['publicationDate'] = datetime.max\n",
    "\n",
    "    if len(rec_lst) > recommend:\n",
    "        rec_lst = rec_lst[:recommend]\n",
    "\n",
    "    # 날짜순으로 정렬\n",
    "    return sorted(rec_lst, key=lambda x: datetime.strptime(x['publicationDate'], '%Y-%m-%d') if isinstance(x['publicationDate'], str) else x['publicationDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '7d8905a1fd288068f12c8347caeabefd36d0dd6c',\n",
       "  'title': 'Gorilla: Large Language Model Connected with Massive APIs',\n",
       "  'abstract': \"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\",\n",
       "  'influentialCitationCount': 25,\n",
       "  'publicationDate': '2023-05-24'},\n",
       " {'paperId': '58f8925a8b87054ad0635a6398a7fe24935b1604',\n",
       "  'title': 'Mind2Web: Towards a Generalist Agent for the Web',\n",
       "  'abstract': 'We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.',\n",
       "  'influentialCitationCount': 22,\n",
       "  'publicationDate': '2023-06-09'},\n",
       " {'paperId': '0bfc804e31eecfd77f45e4ee7f4d629fffdcd628',\n",
       "  'title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs',\n",
       "  'abstract': 'Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.',\n",
       "  'influentialCitationCount': 38,\n",
       "  'publicationDate': '2023-07-31'},\n",
       " {'paperId': '5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0',\n",
       "  'title': 'Qwen Technical Report',\n",
       "  'abstract': 'Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.',\n",
       "  'influentialCitationCount': 45,\n",
       "  'publicationDate': '2023-09-28'},\n",
       " {'paperId': 'd1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43',\n",
       "  'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face',\n",
       "  'abstract': 'Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.',\n",
       "  'influentialCitationCount': 49,\n",
       "  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n",
    "num = 20\n",
    "threshold = 0.6\n",
    "recommend = 5\n",
    "\n",
    "# recommend 개의 가장 연관된 citation 추천\n",
    "citation_recommend(query=query, num=num, threshold=threshold, recommend=recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
