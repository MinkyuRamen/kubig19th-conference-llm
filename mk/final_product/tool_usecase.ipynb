{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/minkyuramen/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/minkyuramen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import os\n",
    "from pprint import pprint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "import tool_pool as tp\n",
    "\n",
    "dotenv_path = '/Users/minkyuramen/Desktop/project/env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# load tool\n",
    "tools = [tp.loadpaper, tp.recommendpaper]\n",
    "# load Agent prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM that will drive the agent\n",
    "# Only certain models support this\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'query': 'Language Models Can Teach Themselves to Use Tools', 'start_page': 3}`\n",
      "\n",
      "\n",
      "\u001b[0m### Searched Paper ###\n",
      "Title: Toolformer: Language Models Can Teach Themselves to Use Tools\n",
      "Authors: Thomas Scialom\n",
      "Published: 2023-02-09T16:49:57Z\n",
      "ID: 2302.04761v1\n",
      "File ./papers_db/2302.04761v1.pdf already exists. Skipping download.\n",
      "Page limit reached at 4\n",
      "\u001b[36;1m\u001b[1;3mYour task is to add calls to a Question Answering API to a piece of text. The questions should help you get information required to complete the text. You can call the API by writing \"[QA(question)]\" where \"question\" is the question you want to ask. Here are some examples of API calls: Input: Joe Biden was born in Scranton, Pennsylvania. Output: Joe Biden was born in [QA(\"Where was Joe Biden born?\")] Scranton, [QA(\"In which state is Scranton?\")] Pennsylvania. Input: Coca-Cola, or Coke, is a carbonated soft drink manufactured by the Coca-Cola Company. Output: Coca-Cola, or [QA(\"What other name is Coca-Cola known by?\")] Coke, is a carbonated soft drink manufactured by [QA(\"Who manufactures Coca-Cola?\")] the Coca-Cola Company. Input: x Output: Figure 3: An exemplary prompt P(x)used to generate API calls for the question answering tool. Mitself on this dataset. Each of these steps is described in more detail below. Sampling API Calls For each API, we write a promptP(x)that encourages the LM to anno- tate an example x=x1;:::;x nwith API calls. An example of such a prompt for a question an- swering tool is shown in Figure 3; all prompts used are shown in Appendix A.2. Let pM(zn+1j z1;:::;z n)be the probability that Massigns to tokenzn+1as a continuation for the sequence z1;:::;z n. We ﬁrst sample up to kcandidate posi- tions for doing API calls by computing, for each i2f1;:::;ng, the probability pi=pM(<API>jP(x);x1:i\u00001) thatMassigns to starting an API call at position i. Given a sampling threshold s, we keep all po- sitionsI=fijpi> sg; if there are more than k such positions, we only keep the top k. For each position i2I, we then obtain up to m API callsc1 i;:::;cm iby sampling from Mgiven the sequence [P(x);x1;:::;x i\u00001;<API> ]as a preﬁx and</API> as an end-of-sequence token.2 2We discard all examples where Mdoes not generate the </API> token.Executing API Calls As a next step, we execute all API calls generated by Mto obtain the corre- sponding results. How this is done depends entirely on the API itself – for example, it can involve call- ing another neural network, executing a Python script or using a retrieval system to perform search over a large corpus. The response for each API call cineeds to be a single text sequence ri. Filtering API Calls Letibe the position of the API callciin the sequence x=x1;:::;x n, and let ribe the response from the API. Further, given a sequence (wiji2N)ofweights , let Li(z) =\u0000nX j=iwj\u0000i\u0001logpM(xjjz;x1:j\u00001) be the weighted cross entropy loss for Mover the tokensxi;:::;x nif the model is preﬁxed with z. We compare two different instantiations of this loss: L+ i=Li(e(ci;ri)) L\u0000 i= min (Li(\");Li(e(ci;\"))) where\"denotes an empty sequence. The former is the weighted loss over all tokens xi;:::;x nif the API call and its result are given to Mas a preﬁx;3 the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to Mif providing it with both the input andthe output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a ﬁltering threshold f, we thus only keep API calls for which L\u0000 i\u0000L+ i\u0015 f holds, i.e., adding the API call and its result reduces the loss by at least f, compared to not doing any API call or obtaining no result from it. Model Finetuning After sampling and ﬁltering calls for all APIs, we ﬁnally merge the remaining API calls and interleave them with the original inputs. That is, for an input text x=x1;:::;x n with a corresponding API call and result (ci;ri)at positioni, we construct the new sequence x\u0003= 3We provide e(ci;ri)as a preﬁx instead of inserting it at positionibecauseMis not yet ﬁnetuned on any examples containing API calls, so inserting it in the middle of xwould interrupt the ﬂow and not align with patterns in the pretraining corpus, thus hurting perplexity.\u001b[0m\u001b[32;1m\u001b[1;3m필터링 API 호출에 대한 수학 표현은 다음과 같습니다:\n",
      "\n",
      "주어진 수식에서, i는 API 호출 ci의 위치를 나타내고, ri는 API의 응답을 나타냅니다. 또한, 가중치의 시퀀스 (wi, i는 자연수)가 주어졌을 때, Li(z) = Σ(j=i)^(n) wj * log p_M(x_j | z; x_1:j-1)는 모델 M에 대한 가중 교차 엔트로피 손실을 나타냅니다. 여기서 z는 모델이 주어진 경우입니다.\n",
      "\n",
      "우리는 이 손실의 두 가지 다른 인스턴스를 비교합니다:\n",
      "- L^+_i = Li(e(ci, ri))\n",
      "- L^-_i = min(Li(\"\"), Li(e(ci, ri)))\n",
      "\n",
      "여기서 \"\"는 빈 시퀀스를 나타냅니다. 전자는 API 호출과 해당 결과가 모델에 주어진 경우의 모든 토큰 xi부터 xn까지의 가중 손실입니다. 후자는 (i) 전혀 API 호출을 하지 않는 경우와 (ii) API 호출을 하지만 결과를 제공하지 않는 경우에서 얻은 손실의 최솟값입니다.\n",
      "\n",
      "직관적으로, API 호출은 모델이 미래 토큰을 예측하는 데 도움이 되는 경우입니다. 즉, API 호출과 해당 결과를 제공하는 것이 전혀 API 호출을 하지 않는 것이나 결과를 얻지 못하는 것보다 모델이 미래 토큰을 예측하는 데 더 쉽게 만드는 경우에 해당합니다.\n",
      "\n",
      "마지막으로, 필터링 임계값 f가 주어지면, 손실이 f 이상 감소하는 API 호출만 유지됩니다. 즉, API 호출과 해당 결과를 추가하면 손실이 f 이상 감소하므로, 전혀 API 호출을 하지 않거나 결과를 얻지 못하는 것보다 더 나은 경우에 해당합니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_executor.invoke({\"input\": \"explain about the math expression about the Filtering api calls in the 3 page of the paper 'Language Models Can Teach Themselves to Use Tools' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('필터링 API 호출에 대한 수학 표현은 다음과 같습니다:\\n'\n",
      " '\\n'\n",
      " '주어진 수식에서, i는 API 호출 ci의 위치를 나타내고, ri는 API의 응답을 나타냅니다. 또한, 가중치의 시퀀스 (wi, i는 '\n",
      " '자연수)가 주어졌을 때, Li(z) = Σ(j=i)^(n) wj * log p_M(x_j | z; x_1:j-1)는 모델 M에 대한 가중 '\n",
      " '교차 엔트로피 손실을 나타냅니다. 여기서 z는 모델이 주어진 경우입니다.\\n'\n",
      " '\\n'\n",
      " '우리는 이 손실의 두 가지 다른 인스턴스를 비교합니다:\\n'\n",
      " '- L^+_i = Li(e(ci, ri))\\n'\n",
      " '- L^-_i = min(Li(\"\"), Li(e(ci, ri)))\\n'\n",
      " '\\n'\n",
      " '여기서 \"\"는 빈 시퀀스를 나타냅니다. 전자는 API 호출과 해당 결과가 모델에 주어진 경우의 모든 토큰 xi부터 xn까지의 가중 '\n",
      " '손실입니다. 후자는 (i) 전혀 API 호출을 하지 않는 경우와 (ii) API 호출을 하지만 결과를 제공하지 않는 경우에서 얻은 손실의 '\n",
      " '최솟값입니다.\\n'\n",
      " '\\n'\n",
      " '직관적으로, API 호출은 모델이 미래 토큰을 예측하는 데 도움이 되는 경우입니다. 즉, API 호출과 해당 결과를 제공하는 것이 전혀 '\n",
      " 'API 호출을 하지 않는 것이나 결과를 얻지 못하는 것보다 모델이 미래 토큰을 예측하는 데 더 쉽게 만드는 경우에 해당합니다.\\n'\n",
      " '\\n'\n",
      " '마지막으로, 필터링 임계값 f가 주어지면, 손실이 f 이상 감소하는 API 호출만 유지됩니다. 즉, API 호출과 해당 결과를 추가하면 '\n",
      " '손실이 f 이상 감소하므로, 전혀 API 호출을 하지 않거나 결과를 얻지 못하는 것보다 더 나은 경우에 해당합니다.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'query': 'Large Language Model Connected with Massive APIs', 'start_page': 1}`\n",
      "\n",
      "\n",
      "\u001b[0m### Searched Paper ###\n",
      "Title: Gorilla: Large Language Model Connected with Massive APIs\n",
      "Authors: ['Shishir G. Patil', 'Tianjun Zhang', 'Xin Wang', 'Joseph E. Gonzalez']\n",
      "Published: 2023-05-24\n",
      "{'paperId': '7d8905a1fd288068f12c8347caeabefd36d0dd6c', 'externalIds': {'DBLP': 'journals/corr/abs-2305-15334', 'ArXiv': '2305.15334', 'DOI': '10.48550/arXiv.2305.15334', 'CorpusId': 258865184}}\n",
      "ArXiv_id: 2305.15334\n",
      "File ./papers_db/2305.15334.pdf already exists. Skipping download.\n",
      "Page limit reached at 4\n",
      "\u001b[36;1m\u001b[1;3mGorilla: Large Language Model Connected with Massive APIs Shishir G. Patil1∗Tianjun Zhang1,∗Xin Wang2Joseph E. Gonzalez1 1UC Berkeley2Microsoft Research sgp@berkeley.edu Abstract Large Language Models (LLMs) have seen an impressive wave of advances re- cently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of- the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model’s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla’s code, model, data, and demo are available at https://gorilla.cs.berkeley.edu 1 Introduction Recent advances in large language models (LLMs) [ 10,5,32,6,29,30] have enabled significant new capabilities including natural dialogue, mathematical reasoning, and program synthesis. However, despite these advances, LLMs are still fundamentally limited by the information they can store in a fixed set of weights and the things they can compute using a static computation graph and limited context. Furthermore, as the world changes, LLMs require retraining to update their knowledge and reasoning capabilities. By empowering LLMs to use tools [ 33], we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks. By providing access to search technologies and databases, [ 26,39,37] demonstrated that we can augment LLMs to address a significantly larger and more dynamic knowledge space. Similarly, by providing access to computational tools, [ 39,2] demonstrated that LLMs can accomplish complex computational tasks. Consequently, leading LLM providers[ 29], have started to integrate plugins to allow LLMs to invoke external tools through APIs. This transition from a small set of hand-coded tools, to the ability to invoke a vast space of changing cloud APIs could transform LLMs into the primary interface to computing infrastructure and the web. Tasks ranging from booking an entire vacation to hosting a conference, could become as simple as talking to an LLM that has access to the flight, car rental, hotel, catering, and entertainment web APIs. However, much of the prior work [ 35,24] integrating tools into LLMs considered a small well documented set of APIs that can be easily injected into the prompt. ∗Equal contribution. Preprint. Under review.arXiv:2305.15334v1 [cs.CL] 24 May 2023GPT-4<domain>:Speech-to-Text<api_provider>:TorchHub<code>:asr_model= torch.hub.load('snakers4/silero-models', 'asr', source='local')result = asr_model.transcribe(audio_path)Claude<domain>:Audio-Translation<api_provider>:Pytorch<code>:import torchaudiotranslation = Torchaudio.pipelines.WAV2VEC2_ASR_PIPELINE(\"audio.wav\")Gorilla<domain>:Speech-to-Text<api_provider>:TorchHub<code>:asr_model = torch.hub.load('snakers4/silero-models', 'silero_sst’)result = asr_model.transcribe(audio_path) Hallucinate!Wrong library!Good to go!Prompt: Help me find an API to convert the spoken language in a recorded audio to text using Torch Hub. Figure 1: Examples of API calls . Example API calls generated by GPT-4 [ 29], Claude [ 3], and Gorilla for the given prompt. In this example, GPT-4 presents a model that doesn’t exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call. Better Figure 2: Accuracy (vs) hallucination in four settings, that is, zero-shot (i.e., without any retriever), and with retrievers .BM25 andGPTare commonly used retrievers and the oracle retriever returns relevant documents at 100%, indicating an upper bound. Higher in the graph (higher accuracy) and to the left is better (lower hallucination). Across the entire dataset, our model, Gorilla, improves accuracy while reducing hallucination. Supporting a web scale collection of potentially millions of changing APIs requires rethinking our approach to how we integrate tools. It is not longer possible to describe the full set of APIs in a single context. Many of the APIs will have overlapping functionality with nuanced limitations and constraints. Simply evaluating LLMs in this new setting requires new benchmarks. In this paper, we explore the use of self-instruct fine-tuning and retrieval to enable LLMs to accu- rately select from a large, overlapping, and changing set tools expressed using their APIs and API documentation. We construct, APIBench, a large corpus of APIs with complex and often overlapping functionality by scraping ML APIs (models) from public model hubs. We choose three major model hubs for dataset construction: TorchHub, TensorHub and HuggingFace. We exhaustively include every API call in TorchHub (94 API calls) and TensorHub (696 API calls); For HuggingFace, since the models come in a large number and lots of the models don’t have a specification, we choose the most downloaded 20 models per task category (in a total of 925). We also generate 10 synthetic user question prompts per API using Self-Instruct [ 42]. Thus, each entry in the dataset becomes an instruction reference API pair. We adopt a common AST sub-tree matching technique to evaluate the functional correctness of the generated API. We first parse the generated code into an AST tree, then find a sub-tree whose root node is the API call that we care about (e.g., torch.hub.load ) and use it to index our dataset. We check the functional correctness and hallucination problem for the LLMs, reporting the corresponding accuracy. We then finetune Gorilla, a LLaMA-7B-based model with document retrieval using our dataset. We find that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy as well as reducing hallucination errors. We show an example output in Fig. 1. Further, our retrieval-aware training of Gorilla enables the model to adapt to changes in the API documentation. Finally, we demonstrate Gorilla’s ability to understand and reason about constraints. 22 Related Work Large Language Models Recent strides in the field of LLMs have renovated many downstream domains [ 10,40,48,47], not only in traditional natural language processing tasks but also in program synthesis. Many of these advances are achieved by augmenting pre-trained LLMs by prompting [ 44,14] and instruction fine-tuning [ 11,31,43,15]. Recent open-sourced models like LLaMa [ 40], Alpaca [ 38], and Vicuna [ 9] have furthered the understanding of LLMs and facilitated their experimentation. While our approach, Gorilla, incorporates techniques akin to those mentioned, its primary emphasis is on enhancing the LLMs’ ability to utilize millions of tools, as opposed to refining their conversational skills. Additionally, we pioneer the study of fine-tuning a base model by supplementing it with information retrieval - a first, to the best of our knowledge. Tool Usage The discussion of tool usage within LLMs has seen an upsurge, with models like Toolformer taking the lead [ 33,19,21,26]. Tools often incorporated include web-browsing [ 34], calculators [ 12,39], translation systems [ 39], and Python interpreters [ 14]. While these efforts can be seen as preliminary explorations of marrying LLMs with tool usage, they generally focus on specific tools. Our paper, in contrast, aims to explore a vast array of tools (i.e., API calls) in an open-ended fashion, potentially covering a wide range of applications. With the recent launch of Toolformer [ 33] and GPT-4 [ 29], the importance of API calls has been highlighted, encouraging many works in employing API calls as tooling [ 35,24]. Moreover, the application of API calls in robotics has been explored to some extent [ 41,1]. However, these works primarily aim at showcasing the potential of “prompting” LLMs rather than establishing a systematic method for evaluation and training (including fine-tuning). Our work, on the other hand, concentrates on systematic evaluation and building a pipeline for future use. LLMs for Program Synthesis Harnessing LLMs for program synthesis has historically been a challenging task [ 23,7,45,16,13,20]. Researchers have proposed an array of strategies to prompt LLMs to perform better in coding tasks, including in-context learning [ 44,18,7], task decomposition [ 17,46], and self-debugging [ 8,36]. Besides prompting, there have also been efforts to pretrain language models specifically for code generation [28, 22, 27]. However, these strategies focus on prompting large language models or pre-training them for general program synthesis. In our research, in contrast, we focus on a much restricted domain: the synthesis of linear programs using API calls. General program synthesis, not only is complex, but is also hard to verify and evaluate. API calls, on the other hand, function more like tool usage. This allows the LLM to significantly expand its capabilities without grappling with low-level implementation details. 3 Methodology In this section, we describe APIBench, a comprehensive benchmark constructed from TorchHub, TensorHub, and HuggingFace API Model Cards. We begin by outlining the process of collecting the API dataset and how we generated instruction-answer pairs. We then introduce Gorilla, a novel training paradigm with a information–retriever incorporated into the training and inference pipelines. Finally, we present our AST tree matching evaluation metric. 3.1 Dataset Collection To collect the dataset, we meticulously recorded all online model cards for HuggingFace’s “The Model Hub”, PyTorch Hub, and TensorFlow Hub Models. Throughout the rest of the paper, we call these HuggingFace, Torch Hub, and TensorFlow Hub respectively for brevity. API Documentation The HuggingFace platform hosts and servers about 203,681 models. However, many of them have poor documentation, lack dependencies, have no information in their model card, etc. To filter these out, we pick the top 20 models from each domain. We consider 7 domains in multimodal data, 8 in CV , 12 in NLP, 5 in Audio, 2 in tabular data, and 2 in reinforcement learning. Post filtering, we got a total of 925 models from HuggingFace. TensorFlow Hub is versioned into v1 and v2. The latest version (v2) has 801 models in total, and we process all of them. Post filtering out models, whose mode cards had little to no information, we are left with 626 models. Similar to 3\u001b[0m\u001b[32;1m\u001b[1;3m'Large Language Model Connected with Massive APIs' 논문의 메소드를 요약하면 다음과 같습니다:\n",
      "\n",
      "- Gorilla 모델을 소개하며, 이 모델은 GPT-4보다 API 호출 작성 성능이 우수하다.\n",
      "- Gorilla는 문서 검색기와 결합하여 테스트 시간 문서 변경에 적응하는 강력한 능력을 보여준다.\n",
      "- Hallucination 문제를 크게 완화시키며, 직접 LLMs에 프롬프팅할 때 흔히 발생하는 문제를 줄인다.\n",
      "- APIBench라는 포괄적인 데이터셋을 소개하며, HuggingFace, TorchHub 및 TensorHub API를 포함한다.\n",
      "- Gorilla 모델을 APIBench 데이터셋을 사용하여 finetune하고, API 기능 정확성에서 GPT-4를 크게 능가하며 Hallucination 오류를 줄인다.\n",
      "- Gorilla의 검색 인식 훈련은 API 문서의 변경에 적응할 수 있도록 모델을 가능하게 한다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_executor.invoke({\"input\": \"summary the method of the paper 'Large Language Model Connected with Massive APIs' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'Large Language Model Connected with Massive APIs' 논문의 메소드를 요약하면 다음과 같습니다:\\n\"\n",
      " '\\n'\n",
      " '- Gorilla 모델을 소개하며, 이 모델은 GPT-4보다 API 호출 작성 성능이 우수하다.\\n'\n",
      " '- Gorilla는 문서 검색기와 결합하여 테스트 시간 문서 변경에 적응하는 강력한 능력을 보여준다.\\n'\n",
      " '- Hallucination 문제를 크게 완화시키며, 직접 LLMs에 프롬프팅할 때 흔히 발생하는 문제를 줄인다.\\n'\n",
      " '- APIBench라는 포괄적인 데이터셋을 소개하며, HuggingFace, TorchHub 및 TensorHub API를 포함한다.\\n'\n",
      " '- Gorilla 모델을 APIBench 데이터셋을 사용하여 finetune하고, API 기능 정확성에서 GPT-4를 크게 능가하며 '\n",
      " 'Hallucination 오류를 줄인다.\\n'\n",
      " '- Gorilla의 검색 인식 훈련은 API 문서의 변경에 적응할 수 있도록 모델을 가능하게 한다.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recompendpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'Facilitating Large Language Models to Master 16000+ Real-world APIs', 'type': 'citation'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'paperId': '19c222d1f18317d58cc85491f37479bc0dc49f41', 'title': 'API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs', 'abstract': \"Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.\", 'influentialCitationCount': 8, 'publicationDate': '2023-04-14'}, {'paperId': '455866ca838f356b53a7e3e5b344834f9e93dbbc', 'title': 'ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases', 'abstract': 'Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models.', 'influentialCitationCount': 15, 'publicationDate': '2023-06-08'}, {'paperId': '12b233752c7097ea6525622bed238ae2d2193c5a', 'title': 'MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback', 'abstract': \"To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation. Our analysis of 20 open- and closed-source LLMs offers intriguing findings. (a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural language feedback. (b) Better single-turn performance does not guarantee better multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.\", 'influentialCitationCount': 7, 'publicationDate': '2023-09-19'}, {'paperId': '67daf8c4fe1958d20ebdf95c2a36dd490c73836f', 'title': 'FireAct: Toward Language Agent Fine-tuning', 'abstract': 'Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.', 'influentialCitationCount': 7, 'publicationDate': '2023-10-09'}, {'paperId': '0725b276e351bba6b2a52ecb64f3c964b9acc2f9', 'title': 'ADaPT: As-Needed Decomposition and Planning with Language Models', 'abstract': 'Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.', 'influentialCitationCount': 6, 'publicationDate': '2023-11-08'}]\u001b[0m\u001b[32;1m\u001b[1;3m'Facilitating Large Language Models to Master 16000+ Real-world APIs' 논문의 미래 연구를 추천드릴게요. \n",
      "\n",
      "1. **API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs**\n",
      "   - 발표일: 2023년 4월 14일\n",
      "   - 초록: 최근 연구에서 대형 언어 모델(Large Language Models, LLMs)이 외부 도구를 활용하여 능력을 향상시킬 수 있다는 것이 입증되었습니다. 현재 LLMs가 도구를 활용하는 능력이 얼마나 효과적인지, LLMs의 도구 활용 능력을 향상시키는 방법은 무엇인지, 도구를 활용하기 위해 극복해야 할 장애물은 무엇인지에 대한 세 가지 중요한 질문이 남아 있습니다. 이 논문에서는 이러한 질문에 대답하기 위해 API-Bank를 소개합니다.\n",
      "   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064782)\n",
      "\n",
      "2. **ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases**\n",
      "   - 발표일: 2023년 6월 8일\n",
      "   - 초록: 대형 언어 모델이 실제 도구를 효과적으로 활용할 수 있도록 하는 것은 몸으로 인식 지능을 달성하는 데 중요합니다. 이 논문은 ToolAlpaca를 소개하여 소형 언어 모델에서도 특정 훈련 없이 일반화된 도구 사용 능력을 학습할 수 있는지에 대해 탐구합니다.\n",
      "   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064783)\n",
      "\n",
      "3. **MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback**\n",
      "   - 발표일: 2023년 9월 19일\n",
      "   - 초록: MINT는 LLMs의 다중 턴 상호 작용 능력을 평가하는 벤치마크로, 도구를 사용하고 자연어 피드백을 활용합니다. 이를 통해 실제 사용 사례에 대한 연구 벤치마크 평가와의 차이를 줄이고 LLMs의 다중 턴 상호 작용 능력을 향상시키는 데 도움이 될 것으로 기대됩니다.\n",
      "   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064784)\n",
      "\n",
      "4. **FireAct: Toward Language Agent Fine-tuning**\n",
      "   - 발표일: 2023년 10월 9일\n",
      "   - 초록: 이 논문은 LMs를 세밀하게 조정하여 언어 에이전트를 얻는 방향을 조사하고 주장합니다. 다양한 실험 결과를 통해 세부적인 실험 설계, 통찰, 그리고 언어 에이전트 세밀 조정에 대한 여러 질문을 제시합니다.\n",
      "   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064785)\n",
      "\n",
      "5. **ADaPT: As-Needed Decomposition and Planning with Language Models**\n",
      "   - 발표일: 2023년 11월 8일\n",
      "   - 초록: ADaPT는 복잡한 작업을 해결하기 위해 LLMs를 사용하는 방법을 소개하며, 이를 통해 작업 복잡성과 LLM 능력에 동적으로 적응할 수 있음을 입증합니다.\n",
      "   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064786)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_executor.invoke({\"input\": \"recommend the future works of the paper 'Facilitating Large Language Models to Master 16000+ Real-world APIs' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'Facilitating Large Language Models to Master 16000+ Real-world APIs' 논문의 미래 \"\n",
      " '연구를 추천드릴게요. \\n'\n",
      " '\\n'\n",
      " '1. **API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs**\\n'\n",
      " '   - 발표일: 2023년 4월 14일\\n'\n",
      " '   - 초록: 최근 연구에서 대형 언어 모델(Large Language Models, LLMs)이 외부 도구를 활용하여 능력을 향상시킬 '\n",
      " '수 있다는 것이 입증되었습니다. 현재 LLMs가 도구를 활용하는 능력이 얼마나 효과적인지, LLMs의 도구 활용 능력을 향상시키는 방법은 '\n",
      " '무엇인지, 도구를 활용하기 위해 극복해야 할 장애물은 무엇인지에 대한 세 가지 중요한 질문이 남아 있습니다. 이 논문에서는 이러한 질문에 '\n",
      " '대답하기 위해 API-Bank를 소개합니다.\\n'\n",
      " '   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064782)\\n'\n",
      " '\\n'\n",
      " '2. **ToolAlpaca: Generalized Tool Learning for Language Models with 3000 '\n",
      " 'Simulated Cases**\\n'\n",
      " '   - 발표일: 2023년 6월 8일\\n'\n",
      " '   - 초록: 대형 언어 모델이 실제 도구를 효과적으로 활용할 수 있도록 하는 것은 몸으로 인식 지능을 달성하는 데 중요합니다. 이 '\n",
      " '논문은 ToolAlpaca를 소개하여 소형 언어 모델에서도 특정 훈련 없이 일반화된 도구 사용 능력을 학습할 수 있는지에 대해 '\n",
      " '탐구합니다.\\n'\n",
      " '   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064783)\\n'\n",
      " '\\n'\n",
      " '3. **MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language '\n",
      " 'Feedback**\\n'\n",
      " '   - 발표일: 2023년 9월 19일\\n'\n",
      " '   - 초록: MINT는 LLMs의 다중 턴 상호 작용 능력을 평가하는 벤치마크로, 도구를 사용하고 자연어 피드백을 활용합니다. 이를 '\n",
      " '통해 실제 사용 사례에 대한 연구 벤치마크 평가와의 차이를 줄이고 LLMs의 다중 턴 상호 작용 능력을 향상시키는 데 도움이 될 것으로 '\n",
      " '기대됩니다.\\n'\n",
      " '   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064784)\\n'\n",
      " '\\n'\n",
      " '4. **FireAct: Toward Language Agent Fine-tuning**\\n'\n",
      " '   - 발표일: 2023년 10월 9일\\n'\n",
      " '   - 초록: 이 논문은 LMs를 세밀하게 조정하여 언어 에이전트를 얻는 방향을 조사하고 주장합니다. 다양한 실험 결과를 통해 세부적인 '\n",
      " '실험 설계, 통찰, 그리고 언어 에이전트 세밀 조정에 대한 여러 질문을 제시합니다.\\n'\n",
      " '   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064785)\\n'\n",
      " '\\n'\n",
      " '5. **ADaPT: As-Needed Decomposition and Planning with Language Models**\\n'\n",
      " '   - 발표일: 2023년 11월 8일\\n'\n",
      " '   - 초록: ADaPT는 복잡한 작업을 해결하기 위해 LLMs를 사용하는 방법을 소개하며, 이를 통해 작업 복잡성과 LLM 능력에 '\n",
      " '동적으로 적응할 수 있음을 입증합니다.\\n'\n",
      " '   - [더 읽어보기](https://doi.org/10.1109/ACCESS.2023.3064786)')\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'type': 'reference'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m([{'paperId': '0bfc804e31eecfd77f45e4ee7f4d629fffdcd628', 'title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'abstract': 'Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.'}, {'paperId': 'b4798b374f5064476f838545f75569c22e682a03', 'title': 'Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia', 'abstract': 'Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act\"reasonably\", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data.'}], [{'paperId': '99832586d55f540f603637e458a292406a0ed75d', 'title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'abstract': 'While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io', 'influentialCitationCount': 180, 'publicationDate': '2022-10-06', 'intent': 'background', 'context': 'ReACT (Yao et al., 2022) proposes to better integrate reasoning with acting by allowing LLMs to give a proper reason for an action and incorporating environmental feedback for reasoning.'}, {'paperId': 'e65b346d442e9962a4276dc1c1af2956d9d5f1eb', 'title': 'Self-Instruct: Aligning Language Models with Self-Generated Instructions', 'abstract': 'Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.', 'influentialCitationCount': 133, 'publicationDate': '2022-12-20', 'intent': 'None', 'context': 'None'}, {'paperId': '53d128ea815bcc0526856eb5a9c42cc977cb36a7', 'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools', 'abstract': 'Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.', 'influentialCitationCount': 68, 'publicationDate': '2023-02-09', 'intent': 'background', 'context': 'Tool Learning Recent studies have shed light on the burgeoning capabilities of LLMs in mastering tools and making decisions within complex environments (Qin et al., 2023b; Vemprala et al., 2023; Nakano et al., 2021; Qin et al., 2023a; Shen et al., 2023; Wu et al., 2023; Schick et al., 2023; Hao et al., 2023; Qian et al., 2023; Song et al., 2023).'}, {'paperId': '131f499e4d3503da93022d07fcf804a18483bea9', 'title': 'WizardLM: Empowering Large Language Models to Follow Complex Instructions', 'abstract': \"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM\", 'influentialCitationCount': 91, 'publicationDate': '2023-04-24', 'intent': 'None', 'context': 'None'}, {'paperId': 'd1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43', 'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face', 'abstract': 'Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.', 'influentialCitationCount': 50, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'None', 'context': 'None'}])\u001b[0m\u001b[32;1m\u001b[1;3m다음은 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs' 논문을 읽기 전에 추천하는 이전 작품 목록입니다:\n",
      "\n",
      "1. **Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia**\n",
      "   - **요약**: 에이전트 기반 모델링은 수십 년 동안 존재해 왔으며 사회 및 자연 과학 분야에서 널리 적용되었습니다. 이 연구 방법의 범위는 이제 대규모 언어 모델(Large Language Models, LLMs)이 제공하는 새로운 기회를 흡수함으로써 급격히 확대될 전망입니다. 생성적 에이전트 기반 모델(GABM)은 에이전트 간의 대화가 아닌 LLM을 사용하여 상황에 상식을 적용하는 것입니다. Concordia는 물리적 또는 디지털 기반 환경의 언어 중심 시뮬레이션을 구축하고 작업하는 데 도움이 되는 라이브러리입니다.\n",
      "   \n",
      "2. **ReAct: Synergizing Reasoning and Acting in Language Models**\n",
      "   - **요약**: 대규모 언어 모델(LLMs)은 언어 이해 및 상호 작용적 의사 결정 작업에서 놀라운 능력을 보여 주었지만 추론(예: 사고 연쇄 유도) 및 행동(예: 행동 계획 생성) 능력은 주로 별도의 주제로 연구되었습니다. ReAct는 LLMs를 사용하여 추론 트레이스와 작업별 작업을 교차로 생성하여 두 가지 사이의 더 큰 시너지를 가능하게 합니다.\n",
      "\n",
      "3. **Self-Instruct: Aligning Language Models with Self-Generated Instructions**\n",
      "   - **요약**: Self-Instruct는 사전 훈련된 언어 모델의 지시 따르기 능력을 향상시키기 위한 프레임워크로, 자체 생성 지침을 활용합니다. 이 방법을 적용하여 GPT3를 튜닝하면 InstructGPT-001과 유사한 성능을 보입니다.\n",
      "\n",
      "4. **Toolformer: Language Models Can Teach Themselves to Use Tools**\n",
      "   - **요약**: Toolformer는 LLMs가 간단한 API를 통해 외부 도구를 사용하는 방법을 자체 학습하고 최적화하는 모델입니다. 이는 손쉽게 구현되며 다양한 하위 작업에서 탁월한 성능을 보입니다.\n",
      "\n",
      "5. **WizardLM: Empowering Large Language Models to Follow Complex Instructions**\n",
      "   - **요약**: WizardLM은 LLMs를 고도로 복잡한 지침을 따르도록 강화하는 방법을 제시합니다. 이 모델은 인간이 생성한 지침보다 더 복잡한 지침을 생성하고 이를 통해 LLM을 튜닝하여 높은 성능을 달성합니다.\n",
      "\n",
      "이러한 이전 작품들을 통해 'ToolLLM' 논문을 더 잘 이해할 수 있을 것입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_executor.invoke({\"input\": \"Could you recommend previous works to read before this papaer : 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"다음은 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world \"\n",
      " \"APIs' 논문을 읽기 전에 추천하는 이전 작품 목록입니다:\\n\"\n",
      " '\\n'\n",
      " '1. **Generative agent-based modeling with actions grounded in physical, '\n",
      " 'social, or digital space using Concordia**\\n'\n",
      " '   - **요약**: 에이전트 기반 모델링은 수십 년 동안 존재해 왔으며 사회 및 자연 과학 분야에서 널리 적용되었습니다. 이 연구 '\n",
      " '방법의 범위는 이제 대규모 언어 모델(Large Language Models, LLMs)이 제공하는 새로운 기회를 흡수함으로써 급격히 '\n",
      " '확대될 전망입니다. 생성적 에이전트 기반 모델(GABM)은 에이전트 간의 대화가 아닌 LLM을 사용하여 상황에 상식을 적용하는 것입니다. '\n",
      " 'Concordia는 물리적 또는 디지털 기반 환경의 언어 중심 시뮬레이션을 구축하고 작업하는 데 도움이 되는 라이브러리입니다.\\n'\n",
      " '   \\n'\n",
      " '2. **ReAct: Synergizing Reasoning and Acting in Language Models**\\n'\n",
      " '   - **요약**: 대규모 언어 모델(LLMs)은 언어 이해 및 상호 작용적 의사 결정 작업에서 놀라운 능력을 보여 주었지만 '\n",
      " '추론(예: 사고 연쇄 유도) 및 행동(예: 행동 계획 생성) 능력은 주로 별도의 주제로 연구되었습니다. ReAct는 LLMs를 사용하여 '\n",
      " '추론 트레이스와 작업별 작업을 교차로 생성하여 두 가지 사이의 더 큰 시너지를 가능하게 합니다.\\n'\n",
      " '\\n'\n",
      " '3. **Self-Instruct: Aligning Language Models with Self-Generated '\n",
      " 'Instructions**\\n'\n",
      " '   - **요약**: Self-Instruct는 사전 훈련된 언어 모델의 지시 따르기 능력을 향상시키기 위한 프레임워크로, 자체 생성 '\n",
      " '지침을 활용합니다. 이 방법을 적용하여 GPT3를 튜닝하면 InstructGPT-001과 유사한 성능을 보입니다.\\n'\n",
      " '\\n'\n",
      " '4. **Toolformer: Language Models Can Teach Themselves to Use Tools**\\n'\n",
      " '   - **요약**: Toolformer는 LLMs가 간단한 API를 통해 외부 도구를 사용하는 방법을 자체 학습하고 최적화하는 '\n",
      " '모델입니다. 이는 손쉽게 구현되며 다양한 하위 작업에서 탁월한 성능을 보입니다.\\n'\n",
      " '\\n'\n",
      " '5. **WizardLM: Empowering Large Language Models to Follow Complex '\n",
      " 'Instructions**\\n'\n",
      " '   - **요약**: WizardLM은 LLMs를 고도로 복잡한 지침을 따르도록 강화하는 방법을 제시합니다. 이 모델은 인간이 생성한 '\n",
      " '지침보다 더 복잡한 지침을 생성하고 이를 통해 LLM을 튜닝하여 높은 성능을 달성합니다.\\n'\n",
      " '\\n'\n",
      " \"이러한 이전 작품들을 통해 'ToolLLM' 논문을 더 잘 이해할 수 있을 것입니다.\")\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
