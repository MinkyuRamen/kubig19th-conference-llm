{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import sys\n",
    "from semanticscholar import SemanticScholar\n",
    "\n",
    "import requests\n",
    "\n",
    "dotenv_path = '.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "ss_api_key = os.getenv(\"SS_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "def get_paper_info_by_title(title, ss_api_key):\n",
    "    \"\"\"\n",
    "    논문의 제목으로 정보를 가져오는 함수\n",
    "    논문의 제목을 입력하여 paper_id와 abstract 정보를 가져오기 위함\n",
    "    ex) toolformer 논문의 paper id 반환\n",
    "    \"\"\"\n",
    "    # Define the API endpoint URL\n",
    "    url = 'https://api.semanticscholar.org/graph/v1/paper/search?query={}&fields=paperId,title,abstract,tldr'\n",
    "    \n",
    "    headers = {'x-api-key': ss_api_key}\n",
    "\n",
    "    if isinstance(title, str):\n",
    "        title = [title]\n",
    "    results = []\n",
    "    for t in title:\n",
    "        response = requests.get(url.format(t), headers=headers).json()\n",
    "        if response.get('data'):\n",
    "            paper = response['data'][0]\n",
    "            results.append(paper)\n",
    "        else:\n",
    "            results.append(None)\n",
    "    return results[0] if len(results) == 1 else results\n",
    "\n",
    "def get_citing_papers(paper_id, api_key):\n",
    "    \"\"\"\n",
    "    입력받은 paper id 논문을 인용한 논문의 정보 가져오는 함수\n",
    "    ex) toolformer 논문을 인용한 논문들의 제목, 년도, 인용수, 초록, tldr을 반환\n",
    "    \"\"\"\n",
    "    # Define the API endpoint URL\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations'\n",
    "\n",
    "    # Define the query parameters\n",
    "    # query_params = {'fields': 'title,authors,year,abstract,influentialCitationCount'}\n",
    "    query_params = {'fields': 'title,year,influentialCitationCount,abstract'}\n",
    "\n",
    "    # Define the headers\n",
    "    headers = {'x-api-key': api_key}\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(url, params=query_params, headers=headers).json()\n",
    "\n",
    "    real_data = [data[\"citingPaper\"] for data in response['data']]\n",
    "\n",
    "    return real_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'tldr@v2.0.0',\n",
       " 'text': 'This paper introduces Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction, which achieves substantially improved zero-shot performance across a variety of downstream tasks.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_paper_tldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paperId': '53d128ea815bcc0526856eb5a9c42cc977cb36a7', 'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools', 'abstract': 'Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.', 'tldr': {'model': 'tldr@v2.0.0', 'text': 'This paper introduces Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction, which achieves substantially improved zero-shot performance across a variety of downstream tasks.'}}\n",
      "Toolformer: Language Models Can Teach Themselves to Use Tools\n",
      "Abstract: Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n",
      "\n",
      "Toolformer: Language Models Can Teach Themselves to Use Tools\n",
      "TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper introduces Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction, which achieves substantially improved zero-shot performance across a variety of downstream tasks.'}\n",
      "\n",
      "Papers citing 'Toolformer: Language Models Can Teach Themselves to Use Tools' sorted by influential citation count:\n",
      "\n",
      "Title: CRAG -- Comprehensive RAG Benchmark\n",
      "Year: 2024\n",
      "Influential Citation Count: 1\n",
      "Abstract: Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.\n",
      "TLDR: No tldr available\n",
      "\n",
      "Title: Adaptive In-conversation Team Building for Language Model Agents\n",
      "Year: 2024\n",
      "Influential Citation Count: 1\n",
      "Abstract: Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively? Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent. It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. It allows for a flexible yet structured approach to problem-solving and can help reduce redundancy and enhance output diversity. A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering.\n",
      "TLDR: No tldr available\n",
      "\n",
      "Title: Stress-Testing Capability Elicitation With Password-Locked Models\n",
      "Year: 2024\n",
      "Influential Citation Count: 1\n",
      "Abstract: To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM's full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models, but may be unreliable when high-quality demonstrations are not available, e.g. as may be the case when models' (hidden) capabilities exceed those of human demonstrators.\n",
      "TLDR: No tldr available\n",
      "\n",
      "Title: Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows\n",
      "Year: 2024\n",
      "Influential Citation Count: 0\n",
      "Abstract: We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback. Website: https://microsoft.github.io/Trace\n",
      "TLDR: No tldr available\n",
      "\n",
      "Title: TDC-2: Multimodal Foundation for Therapeutic Science\n",
      "Year: 2024\n",
      "Influential Citation Count: 0\n",
      "Abstract: Therapeutics Data Commons (tdcommons.ai) is an open science initiative with unified datasets, AI models, and benchmarks to support research across therapeutic modalities and drug discovery and development stages. The Commons 2.0 (TDC-2) is a comprehensive overhaul of Therapeutic Data Commons to catalyze research in multimodal models for drug discovery by unifying single-cell biology of diseases, biochemistry of molecules, and effects of drugs through multimodal datasets, AI-powered API endpoints, new multimodal tasks and model frameworks, and comprehensive benchmarks. TDC-2 introduces over 1,000 multimodal datasets spanning approximately 85 million cells, pre-calculated embeddings from 5 state-of-the-art single-cell models, and a biomedical knowledge graph. TDC-2 drastically expands the coverage of ML tasks across therapeutic pipelines and 10+ new modalities, spanning but not limited to single-cell gene expression data, clinical trial data, peptide sequence data, peptidomimetics protein-peptide interaction data regarding newly discovered ligands derived from AS-MS spectroscopy, novel 3D structural data for proteins, and cell-type-specific protein-protein interaction networks at single-cell resolution. TDC-2 introduces multimodal data access under an API-first design using the model-view-controller paradigm. TDC-2 introduces 7 novel ML tasks with fine-grained biological contexts: contextualized drug-target identification, single-cell chemical/genetic perturbation response prediction, protein-peptide binding affinity prediction task, and clinical trial outcome prediction task, which introduce antigen-processing-pathway-specific, cell-type-specific, peptide-specific, and patient-specific biological contexts. TDC-2 also releases benchmarks evaluating 15+ state-of-the-art models across 5+ new learning tasks evaluating models on diverse biological contexts and sampling approaches. Among these, TDC-2 provides the first benchmark for context-specific learning. TDC-2, to our knowledge, is also the first to introduce a protein-peptide binding interaction benchmark.\n",
      "TLDR: No tldr available\n"
     ]
    }
   ],
   "source": [
    "query = \"Toolformer: Language Models Can Teach Themselves to Use Tools\"\n",
    "paper_count = 5\n",
    "paper_info = get_paper_info_by_title(query, ss_api_key)\n",
    "print(paper_info)\n",
    "\n",
    "paper_id = paper_info[\"paperId\"]\n",
    "query_paper_abstract = paper_info[\"abstract\"]\n",
    "query_paper_tldr = paper_info[\"tldr\"]\n",
    "citation_paper_abstract = []\n",
    "citation_paper_tldr = []\n",
    "\n",
    "if paper_id:\n",
    "    # Get the papers that cite the given paper ID\n",
    "    citing_papers = get_citing_papers(paper_id, ss_api_key)\n",
    "    \n",
    "    # paper_id 논문을 인용한 논문들을 influentialCitationCount를 기준으로 내림차순 정렬(유명한 것부터 보기)\n",
    "    sorted_papers = sorted(citing_papers, key=lambda x: x.get('influentialCitationCount', 0), reverse=True)[:paper_count]\n",
    "\n",
    "    print(f\"{query}\\nAbstract: {query_paper_abstract}\\n\")\n",
    "    print(f\"{query}\\nTLDR: {query_paper_tldr}\\n\")\n",
    "    \n",
    "    print(f\"Papers citing '{query}' sorted by influential citation count:\")\n",
    "    for paper in sorted_papers:\n",
    "        title = paper['title']\n",
    "        # authors = ', '.join([author['name'] for author in paper['authors']])\n",
    "        year = paper['year']\n",
    "        abstract = paper.get('abstract', 'No abstract available')\n",
    "        tldr = paper.get('tldr', 'No tldr available')\n",
    "        citation_paper_abstract.append(abstract)\n",
    "        citation_paper_tldr.append(tldr)\n",
    "        influential_citation_count = paper.get('influentialCitationCount', 0)\n",
    "        print(f\"\\nTitle: {title}\\nYear: {year}\\nInfluential Citation Count: {influential_citation_count}\\nAbstract: {abstract}\\nTLDR: {tldr}\")\n",
    "else:\n",
    "    print(\"No paper found with the given title.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
