{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import sys\n",
    "from semanticscholar import SemanticScholar\n",
    "\n",
    "import requests\n",
    "\n",
    "dotenv_path = '.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "ss_api_key = os.getenv(\"SS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전략 1\n",
    "- 논문을 인용(citation)한 다른 논문들의 초록에서 한계점을 언급한 것이 있는지 찾아보기\n",
    "- 논문을 참조(reference)한 다른 논문들의 초록에서 한계점을 언급한 것이 있는지 찾아보기\n",
    "- 찾게 된다면 언급이 된 그 논문이 개선할 수 있는 아이디어일 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_info_by_title(title, ss_api_key):\n",
    "    \"\"\"논문의 제목으로 정보를 가져오는 함수\"\"\"\n",
    "    # Define the API endpoint URL\n",
    "    url = 'https://api.semanticscholar.org/graph/v1/paper/search?query={}&fields=paperId,title,abstract,authors,citations,fieldsOfStudy,influentialCitationCount,isOpenAccess,openAccessPdf,publicationDate,publicationTypes,references,venue'\n",
    "    \n",
    "    headers = {'x-api-key': ss_api_key}\n",
    "    response = requests.get(url.format(title), headers=headers).json()\n",
    "\n",
    "    if response.get('data'):\n",
    "        paper = response['data'][0]\n",
    "        return paper\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_citing_papers(paper_id, ss_api_key):\n",
    "    \"\"\"입력한 id에 해당하는 논문을 인용한 논문들의 제목과 초록을 가져오는 함수\"\"\"\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations?fields=title,abstract'\n",
    "    \n",
    "    headers = {'x-api-key': ss_api_key}\n",
    "    response = requests.get(url, headers=headers).json()\n",
    "    \n",
    "    if response.get('data'):\n",
    "        return response['data']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_referenced_papers(paper_id, ss_api_key):\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references?fields=title,abstract'\n",
    "    \n",
    "    headers = {'x-api-key': ss_api_key}\n",
    "    response = requests.get(url, headers=headers).json()\n",
    "    \n",
    "    if response.get('data'):\n",
    "        return response['data']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def find_limitations_in_paper(texts):\n",
    "    \"\"\"한계점이라고 언급된 부분을 감지하는 함수\"\"\"\n",
    "    keywords = [\n",
    "        'limitation', 'limitations', 'drawback', 'shortcoming', 'constraint', 'weakness',\n",
    "        'future work', 'future direction', 'challenge', 'issue', 'problem', 'restriction',\n",
    "        'flaw', 'hurdle', 'barrier', 'difficulty', 'risk', 'pitfall', 'concern', 'obstacle',\n",
    "        'gap', 'disadvantage', 'incompleteness', 'improvement'\n",
    "    ]\n",
    "    limitations = []\n",
    "    \n",
    "    for text in texts:\n",
    "        abstract = text.get('abstract', '').lower()\n",
    "        if any(keyword in abstract for keyword in keywords):\n",
    "            limitations.append((text['title'], text['abstract']))\n",
    "    \n",
    "    return limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No limitations found in citing papers.\n",
      "\n",
      "No limitations found in referenced papers.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "title = \"Toolformer: Language Models Can Teach Themselves to Use Tools\"\n",
    "paper = get_paper_info_by_title(title, ss_api_key)\n",
    "\n",
    "if paper:\n",
    "    paper_id = paper['paperId']\n",
    "    citing_papers = get_citing_papers(paper_id, ss_api_key)\n",
    "    limitations_in_citations = find_limitations_in_paper(citing_papers)\n",
    "    \n",
    "    if limitations_in_citations:\n",
    "        print(\"\\nLimitations found in citing papers:\")\n",
    "        for title, limitations in limitations_in_citations:\n",
    "            print(f\"Title: {title}\")\n",
    "            for limitation in limitations:\n",
    "                print(f\" - {limitation}\")\n",
    "    else:\n",
    "        print(\"\\nNo limitations found in citing papers.\")\n",
    "    \n",
    "    # Get referenced papers and find limitations\n",
    "    referenced_papers = get_referenced_papers(paper_id, ss_api_key)\n",
    "    limitations_in_references = find_limitations_in_paper(referenced_papers)\n",
    "    \n",
    "    if limitations_in_references:\n",
    "        print(\"\\nLimitations found in referenced papers:\")\n",
    "        for title, limitations in limitations_in_references:\n",
    "            print(f\"Title: {title}\")\n",
    "            for limitation in limitations:\n",
    "                print(f\" - {limitation}\")\n",
    "    else:\n",
    "        print(\"\\nNo limitations found in referenced papers.\")\n",
    "else:\n",
    "    print(\"No paper found with the given title.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전략 2\n",
    "- 논문의 내용을 직접 가져와서 거기서 한계점 및 향후 연구 언급 부분을 찾아보기\n",
    "- pdf 다운로드 방식보다는 웹에서 긁어올 수 있으면 좋겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'limitation', 'limitations', 'drawback', 'shortcoming', 'constraint', 'weakness',\n",
    "    'future work', 'future direction', 'challenge', 'issue', 'problem', 'restriction',\n",
    "    'flaw', 'hurdle', 'barrier', 'difficulty', 'risk', 'pitfall', 'concern', 'obstacle',\n",
    "    'gap', 'disadvantage', 'incompleteness', 'improvement'\n",
    "]\n",
    "\n",
    "def get_arxiv_url(paper):\n",
    "    \"논문의 ar5iv 주소를 받아오는 함수\"\n",
    "    external_ids = paper.get('openAccessPdf', {})\n",
    "    arxiv_id = external_ids.get('url')\n",
    "    if 'http' in arxiv_id:\n",
    "        arxiv_id = arxiv_id.split('/')[-1]\n",
    "        return f\"https://ar5iv.org/abs/{arxiv_id}\"\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def find_limitations_in_arxiv(url, sections:list=None):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    all_sections = soup.find_all('section')\n",
    "    \n",
    "    limitations = []\n",
    "    for section in all_sections:\n",
    "        section_title_tag = section.find('h2', class_='ltx_title_section')\n",
    "        if section_title_tag:\n",
    "            section_title = section_title_tag.get_text().strip().lower()\n",
    "            if sections and not any(s.lower() in section_title for s in sections):\n",
    "                print(section_title)\n",
    "                continue\n",
    "\n",
    "        paragraphs = section.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            text = paragraph.get_text().lower()\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                limitations.append(paragraph.get_text())\n",
    "    \n",
    "    if limitations:\n",
    "        return list(set(limitations))\n",
    "    else:\n",
    "        return \"No limitations found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Toolformer: Language Models Can Teach Themselves to Use Tools\n",
      "Abstract: Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n",
      "arXiv URL: https://ar5iv.org/abs/2302.04761\n",
      "2 approach\n",
      "3 tools\n",
      "4 experiments\n",
      "5 analysis\n",
      "6 related work\n",
      "7 limitations\n",
      "8 conclusion\n",
      "Limitations found in the paper:\n",
      "- For Dateset, on the other hand, the considerable improvement of Toolformer compared to other models can be fully accredited to the calendar tool, which it makes use of for 54.8% of all examples.\n",
      "- Large language models achieve impressive zero- and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent capabilities (Wei et al., 2022). However, all of these models have several inherent limitations that can at best be partially addressed by further scaling. These limitations include an inability to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time (Dhingra et al., 2022).\n",
      "- A simple way to overcome these limitations of today’s language models is to give them the ability to use external tools such as search engines, calculators, or calendars.\n",
      "However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.\n",
      "Therefore, we propose Toolformer, a model that learns to use tools in a novel way, which fulfills the following desiderata:\n",
      "- Results shown in Table 7 illustrate that Toolformer outperforms all baselines for both TempLAMA and Dateset. However, closer inspection shows that improvements on TempLAMA can not be attributed to the calendar tool, which is only used for 0.2% of all examples, but mostly to the Wikipedia search and question answering tools, which Toolformer calls the most. This makes sense given that\n",
      "named entities in TempLama are often so specific and rare that even knowing the exact date alone would be of little help. The best course of action for this dataset – first querying the calendar API to get the current date, and then querying the question answering system with this date – is not only prohibited by our restriction of using at most one API call per example, but also hard to learn for Toolformer given that all API calls in its training data are sampled independently.\n",
      "- Results are shown in Table 5. Once again, Toolformer clearly outperforms all other models based on GPT-J, this time mostly relying on the Wikipedia search API (99.3%) to find relevant information. However, Toolformer still lags behind the much larger GPT-3 (175B) model. This is likely due to both the simplicity of our search engine (in many cases, it returns results that are clearly not a good match for a given query) and the inability of Toolformer to interact with it, e.g., by reformulating its query if results are not helpful or by browsing through multiple of the top results. We believe that adding this functionality is an exciting direction for future work.\n",
      "- Figure 4 shows that the ability to leverage the provided tools only emerges at around 775M parameters: smaller models achieve similar performance both with and without tools. An exception to this is the Wikipedia search engine used mostly for QA benchmarks; we hypothesize that this is because the API is comparably easy to use.\n",
      "While models become better at solving tasks without API calls as they grow in size, their ability to make good use of the provided API improves at the same time. As a consequence, there remains a large gap between predictions with and without API calls even for our biggest model.\n",
      "- We investigate the effect of our modified decoding strategy introduced in Section 4.2, where instead of always generating the most likely token, we generate the <API> token if it is one of the k𝑘k most likely tokens. Table 9 shows performance on the T-REx subset of LAMA and on WebQS for different values of k𝑘k. As expected, increasing k𝑘k leads to the model doing API calls for more examples – from 40.3% and 8.5% with k=1𝑘1k=1 (i.e., regular greedy decoding) to 98.1% and 100% for k=10𝑘10k=10. While for T-REx, there is already a clear improvement in performance with greedy decoding, on WebQS our model only starts to make a substantial number of API calls as we slightly increase k𝑘k. Interestingly, for k=1𝑘1k=1 the model is calibrated to some extent: It decides to call APIs for examples that it would perform particularly badly on without making API calls. This can be seen from the fact that performance on examples where it decides not to make an API call (44.3 and 19.9) is higher than average performance if no API calls are made at all (34.9 and 18.9). However, this calibration is lost for higher values of k𝑘k.\n",
      "- OPT and GPT-3 perform surprisingly weak across all languages, mostly because they fail to provide an answer in English despite being instructed to do so. A potential reason for GPT-J not suffering from this problem is that it was trained on more multilingual data than both OPT and GPT-3, including the EuroParl corpus (Koehn, 2005; Gao et al., 2020). As an upper bound, we also evaluate GPT-J and GPT-3 on a variant of MLQA where both the context and the question are provided in English. In this setup, GPT-3 performs better than all other models, supporting our hypothesis that its subpar performance on MLQA is due to the multilingual aspect of the task.\n",
      "- Input: But what are the risks during production of nanomaterials? Some nanomaterials may give rise to various kinds of lung damage.\n",
      "Output: But what are the risks during production of nanomaterials? [WikiSearch(”nanomaterial production risks”)] Some nanomaterials may give rise to various kinds of lung damage.\n",
      "- We evaluate all models on a variety of downstream tasks. In all cases, we consider a prompted zero-shot setup – i.e., models are instructed to solve each task in natural language, but we do not provide any in-context examples. This is in contrast to prior work on tool use (e.g., Gao et al., 2022; Parisi et al., 2022), where models are provided with dataset-specific examples of how a tool can be used to solve a concrete task. We choose the more challenging zero-shot setup as we are interested in seeing whether Toolformer works in precisely those cases where a user does not specify in advance which tools should be used in which way for solving a specific problem.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "title = \"Toolformer: Language Models Can Teach Themselves to Use Tools\"\n",
    "paper = get_paper_info_by_title(title, ss_api_key)\n",
    "\n",
    "if paper:\n",
    "    abstract = paper.get('abstract', '')\n",
    "    print(f\"Title: {paper['title']}\")\n",
    "    print(f\"Abstract: {abstract}\")\n",
    "    \n",
    "    arxiv_url = get_arxiv_url(paper)\n",
    "    if arxiv_url:\n",
    "        print(f\"arXiv URL: {arxiv_url}\")\n",
    "        limitations = find_limitations_in_arxiv(arxiv_url, ['introduction'])\n",
    "        if isinstance(limitations, list):\n",
    "            print(\"Limitations found in the paper:\")\n",
    "            for limitation in limitations:\n",
    "                print(f\"- {limitation}\")\n",
    "        else:\n",
    "            print(limitations)\n",
    "    else:\n",
    "        print(\"No arXiv ID available for this paper.\")\n",
    "else:\n",
    "    print(\"No paper found with the given title.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(limitations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n",
    "num = 20\n",
    "threshold = 0.6\n",
    "recommend = 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
