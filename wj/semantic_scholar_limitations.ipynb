{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import sys\n",
    "from semanticscholar import SemanticScholar\n",
    "\n",
    "import requests\n",
    "\n",
    "dotenv_path = '.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "ss_api_key = os.getenv(\"SS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전략 1\n",
    "- 논문을 인용(citation)한 다른 논문들의 초록에서 한계점을 언급한 것이 있는지 찾아보기\n",
    "- 논문을 참조(reference)한 다른 논문들의 초록에서 한계점을 언급한 것이 있는지 찾아보기\n",
    "- 찾게 된다면 언급이 된 그 논문이 개선할 수 있는 아이디어일 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_info_by_title(title, ss_api_key):\n",
    "    \"\"\"논문의 제목으로 정보를 가져오는 함수\"\"\"\n",
    "    # Define the API endpoint URL\n",
    "    url = 'https://api.semanticscholar.org/graph/v1/paper/search?query={}&fields=paperId,title,abstract,authors,citations,fieldsOfStudy,influentialCitationCount,isOpenAccess,openAccessPdf,publicationDate,publicationTypes,references,venue'\n",
    "    \n",
    "    headers = {'x-api-key': ss_api_key}\n",
    "    response = requests.get(url.format(title), headers=headers).json()\n",
    "\n",
    "    if response.get('data'):\n",
    "        paper = response['data'][0]\n",
    "        return paper\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_citing_papers(paper_id, ss_api_key):\n",
    "    \"\"\"입력한 id에 해당하는 논문을 인용한 논문들의 제목과 초록을 가져오는 함수\"\"\"\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations?fields=title,abstract'\n",
    "    \n",
    "    headers = {'x-api-key': ss_api_key}\n",
    "    response = requests.get(url, headers=headers).json()\n",
    "    \n",
    "    if response.get('data'):\n",
    "        return response['data']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_referenced_papers(paper_id, ss_api_key):\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references?fields=title,abstract'\n",
    "    \n",
    "    headers = {'x-api-key': ss_api_key}\n",
    "    response = requests.get(url, headers=headers).json()\n",
    "    \n",
    "    if response.get('data'):\n",
    "        return response['data']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def find_limitations_in_paper(texts):\n",
    "    \"\"\"한계점이라고 언급된 부분을 감지하는 함수\"\"\"\n",
    "    keywords = [\n",
    "        'limitation', 'limitations', 'drawback', 'shortcoming', 'constraint', 'weakness',\n",
    "        'future work', 'future direction', 'challenge', 'issue', 'problem', 'restriction',\n",
    "        'flaw', 'hurdle', 'barrier', 'difficulty', 'risk', 'pitfall', 'concern', 'obstacle',\n",
    "        'gap', 'disadvantage', 'incompleteness', 'improvement'\n",
    "    ]\n",
    "    limitations = []\n",
    "    \n",
    "    for text in texts:\n",
    "        abstract = text.get('abstract', '').lower()\n",
    "        if any(keyword in abstract for keyword in keywords):\n",
    "            limitations.append((text['title'], text['abstract']))\n",
    "    \n",
    "    return limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No limitations found in citing papers.\n",
      "\n",
      "No limitations found in referenced papers.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "title = \"Toolformer: Language Models Can Teach Themselves to Use Tools\"\n",
    "paper = get_paper_info_by_title(title, ss_api_key)\n",
    "\n",
    "if paper:\n",
    "    paper_id = paper['paperId']\n",
    "    citing_papers = get_citing_papers(paper_id, ss_api_key)\n",
    "    limitations_in_citations = find_limitations_in_paper(citing_papers)\n",
    "    \n",
    "    if limitations_in_citations:\n",
    "        print(\"\\nLimitations found in citing papers:\")\n",
    "        for title, limitations in limitations_in_citations:\n",
    "            print(f\"Title: {title}\")\n",
    "            for limitation in limitations:\n",
    "                print(f\" - {limitation}\")\n",
    "    else:\n",
    "        print(\"\\nNo limitations found in citing papers.\")\n",
    "    \n",
    "    # Get referenced papers and find limitations\n",
    "    referenced_papers = get_referenced_papers(paper_id, ss_api_key)\n",
    "    limitations_in_references = find_limitations_in_paper(referenced_papers)\n",
    "    \n",
    "    if limitations_in_references:\n",
    "        print(\"\\nLimitations found in referenced papers:\")\n",
    "        for title, limitations in limitations_in_references:\n",
    "            print(f\"Title: {title}\")\n",
    "            for limitation in limitations:\n",
    "                print(f\" - {limitation}\")\n",
    "    else:\n",
    "        print(\"\\nNo limitations found in referenced papers.\")\n",
    "else:\n",
    "    print(\"No paper found with the given title.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전략 2\n",
    "- 논문의 내용을 직접 가져와서 거기서 한계점 및 향후 연구 언급 부분을 찾아보기\n",
    "- pdf 다운로드 방식보다는 웹에서 긁어올 수 있으면 좋겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'limitation', 'limitations', 'drawback', 'shortcoming', 'constraint', 'weakness',\n",
    "    'future work', 'future direction', 'challenge', 'issue', 'problem', 'restriction',\n",
    "    'flaw', 'hurdle', 'barrier', 'difficulty', 'risk', 'pitfall', 'concern', 'obstacle',\n",
    "    'gap', 'disadvantage', 'incompleteness', 'improvement'\n",
    "]\n",
    "\n",
    "def get_arxiv_url(paper):\n",
    "    \"논문의 ar5iv 주소를 받아오는 함수\"\n",
    "    external_ids = paper.get('openAccessPdf', {})\n",
    "    arxiv_id = external_ids.get('url')\n",
    "    if 'http' in arxiv_id:\n",
    "        arxiv_id = arxiv_id.split('/')[-1]\n",
    "        return f\"https://ar5iv.org/abs/{arxiv_id}\"\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def find_limitations_in_arxiv(url, sections:list=None):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    all_sections = soup.find_all('section')\n",
    "    \n",
    "    limitations = []\n",
    "    for section in all_sections:\n",
    "        # section_title_tag = section.find('h2', class_='ltx_title_section')\n",
    "        # if section_title_tag:\n",
    "        #     section_title = section_title_tag.get_text().strip().lower()\n",
    "        #     if sections and not any(s.lower() in section_title for s in sections):\n",
    "        #         print(section_title)\n",
    "        #         continue\n",
    "\n",
    "        paragraphs = section.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            text = paragraph.get_text().lower()\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                limitations.append(paragraph.get_text())\n",
    "    \n",
    "    if limitations:\n",
    "        return list(set(limitations))\n",
    "    else:\n",
    "        return \"No limitations found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Toolformer: Language Models Can Teach Themselves to Use Tools\n",
      "Abstract: Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n",
      "arXiv URL: https://ar5iv.org/abs/2302.04761\n",
      "Limitations found in the paper:\n",
      "- While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limitation is the inability of Toolformer to use tools in a chain (i.e., using the output of one tool as an input for another tool). This is due to the fact that API calls for each tool are generated independently; as a consequence, there are no examples of chained tool use in the finetuning dataset. Our current approach also does not allow the LM to use a tool in an interactive way – especially for tools such as search engines, that could potentially return hundreds of different results, enabling a LM to browse through these results or to refine its search query in a similar spirit to Nakano et al. (2021) can be crucial for certain applications. Beyond this, we found models trained with Toolformer to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero-and few-shot settings (Jiang et al., 2020; Schick and\n",
      "Schütze, 2021a). Depending on the tool, our method is also very sample-inefficient; for example, processing more than a million documents results in only a few thousand examples of useful calls to the calculator API. A potential solution to this problem might be to iteratively apply our approach, similar to how this is done in related bootstrapping approaches (Schick and\n",
      "Schütze, 2021a; Izacard and Grave, 2021; Parisi et al., 2022). Finally, when deciding whether or not to make an API call, Toolformer currently does not take into account the tool-dependent, computational cost incurred from making an API call.\n",
      "- OPT and GPT-3 perform surprisingly weak across all languages, mostly because they fail to provide an answer in English despite being instructed to do so. A potential reason for GPT-J not suffering from this problem is that it was trained on more multilingual data than both OPT and GPT-3, including the EuroParl corpus (Koehn, 2005; Gao et al., 2020). As an upper bound, we also evaluate GPT-J and GPT-3 on a variant of MLQA where both the context and the question are provided in English. In this setup, GPT-3 performs better than all other models, supporting our hypothesis that its subpar performance on MLQA is due to the multilingual aspect of the task.\n",
      "- For Dateset, on the other hand, the considerable improvement of Toolformer compared to other models can be fully accredited to the calendar tool, which it makes use of for 54.8% of all examples.\n",
      "- Input: But what are the risks during production of nanomaterials? Some nanomaterials may give rise to various kinds of lung damage.\n",
      "Output: But what are the risks during production of nanomaterials? [WikiSearch(”nanomaterial production risks”)] Some nanomaterials may give rise to various kinds of lung damage.\n",
      "- We investigate the effect of our modified decoding strategy introduced in Section 4.2, where instead of always generating the most likely token, we generate the <API> token if it is one of the k𝑘k most likely tokens. Table 9 shows performance on the T-REx subset of LAMA and on WebQS for different values of k𝑘k. As expected, increasing k𝑘k leads to the model doing API calls for more examples – from 40.3% and 8.5% with k=1𝑘1k=1 (i.e., regular greedy decoding) to 98.1% and 100% for k=10𝑘10k=10. While for T-REx, there is already a clear improvement in performance with greedy decoding, on WebQS our model only starts to make a substantial number of API calls as we slightly increase k𝑘k. Interestingly, for k=1𝑘1k=1 the model is calibrated to some extent: It decides to call APIs for examples that it would perform particularly badly on without making API calls. This can be seen from the fact that performance on examples where it decides not to make an API call (44.3 and 19.9) is higher than average performance if no API calls are made at all (34.9 and 18.9). However, this calibration is lost for higher values of k𝑘k.\n",
      "- Large language models achieve impressive zero- and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent capabilities (Wei et al., 2022). However, all of these models have several inherent limitations that can at best be partially addressed by further scaling. These limitations include an inability to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time (Dhingra et al., 2022).\n",
      "- Figure 4 shows that the ability to leverage the provided tools only emerges at around 775M parameters: smaller models achieve similar performance both with and without tools. An exception to this is the Wikipedia search engine used mostly for QA benchmarks; we hypothesize that this is because the API is comparably easy to use.\n",
      "While models become better at solving tasks without API calls as they grow in size, their ability to make good use of the provided API improves at the same time. As a consequence, there remains a large gap between predictions with and without API calls even for our biggest model.\n",
      "- A simple way to overcome these limitations of today’s language models is to give them the ability to use external tools such as search engines, calculators, or calendars.\n",
      "However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.\n",
      "Therefore, we propose Toolformer, a model that learns to use tools in a novel way, which fulfills the following desiderata:\n",
      "- We evaluate all models on a variety of downstream tasks. In all cases, we consider a prompted zero-shot setup – i.e., models are instructed to solve each task in natural language, but we do not provide any in-context examples. This is in contrast to prior work on tool use (e.g., Gao et al., 2022; Parisi et al., 2022), where models are provided with dataset-specific examples of how a tool can be used to solve a concrete task. We choose the more challenging zero-shot setup as we are interested in seeing whether Toolformer works in precisely those cases where a user does not specify in advance which tools should be used in which way for solving a specific problem.\n",
      "- Results shown in Table 7 illustrate that Toolformer outperforms all baselines for both TempLAMA and Dateset. However, closer inspection shows that improvements on TempLAMA can not be attributed to the calendar tool, which is only used for 0.2% of all examples, but mostly to the Wikipedia search and question answering tools, which Toolformer calls the most. This makes sense given that\n",
      "named entities in TempLama are often so specific and rare that even knowing the exact date alone would be of little help. The best course of action for this dataset – first querying the calendar API to get the current date, and then querying the question answering system with this date – is not only prohibited by our restriction of using at most one API call per example, but also hard to learn for Toolformer given that all API calls in its training data are sampled independently.\n",
      "- Results are shown in Table 5. Once again, Toolformer clearly outperforms all other models based on GPT-J, this time mostly relying on the Wikipedia search API (99.3%) to find relevant information. However, Toolformer still lags behind the much larger GPT-3 (175B) model. This is likely due to both the simplicity of our search engine (in many cases, it returns results that are clearly not a good match for a given query) and the inability of Toolformer to interact with it, e.g., by reformulating its query if results are not helpful or by browsing through multiple of the top results. We believe that adding this functionality is an exciting direction for future work.\n",
      "- We explore a variety of tools to address different shortcomings of regular LMs. The only constraints we impose on these tools is that (i) both their inputs and outputs can be represented as text sequences, and (ii) we can obtain a few demonstrations of their intended use. Concretely, we explore the following five tools: a question answering system, a Wikipedia search engine, a calculator, a calendar, and a machine translation system. Some examples of potential calls and return strings for the APIs associated with each of these tools are shown in Table 1. We briefly discuss all tools below; further details can be found in Appendix A.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "title = \"Toolformer: Language Models Can Teach Themselves to Use Tools\"\n",
    "paper = get_paper_info_by_title(title, ss_api_key)\n",
    "\n",
    "if paper:\n",
    "    abstract = paper.get('abstract', '')\n",
    "    print(f\"Title: {paper['title']}\")\n",
    "    print(f\"Abstract: {abstract}\")\n",
    "    \n",
    "    arxiv_url = get_arxiv_url(paper)\n",
    "    if arxiv_url:\n",
    "        print(f\"arXiv URL: {arxiv_url}\")\n",
    "        limitations = find_limitations_in_arxiv(arxiv_url)\n",
    "        if isinstance(limitations, list):\n",
    "            print(\"Limitations found in the paper:\")\n",
    "            for limitation in limitations:\n",
    "                print(f\"- {limitation}\")\n",
    "        else:\n",
    "            print(limitations)\n",
    "    else:\n",
    "        print(\"No arXiv ID available for this paper.\")\n",
    "else:\n",
    "    print(\"No paper found with the given title.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전략 3\n",
    "- 전략 2처럼 하면 참조한 논문들의 한계나 선택하지 않은 방법론들의 한계점 등도 너무 많이 잡힌다.\n",
    "- 그래서 이 논문의 abstract, conclusion과 함께 이 논문을 인용한 논문들의 abstract를 주고 발전 방향에 대해 생각해보라고 지시하면 어떨까?\n",
    "- 그럼 명확히 '한계점'은 아니라도 더 개선할 여지가 있는 지점을 얘기해줄 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98408e1956554ba8b8a9820d478a2e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d49d82e9d74fc789d2a0a61fcd8c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795170f6cf4a4f82849e32d9b4c76df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9563ce7107142bdb9bedbf16ee519f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86733e34fc3e463d8855f36870dd892a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c96d100e414df7b2d2121d31eede90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import semantic_scholoar_api as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toolformer: Language Models Can Teach Themselves to Use Tools\n",
      "Abstract: Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n",
      "\n",
      "Papers citing 'Toolformer: Language Models Can Teach Themselves to Use Tools' sorted by influential citation count:\n",
      "\n",
      "Title: CRAG -- Comprehensive RAG Benchmark\n",
      "Year: 2024\n",
      "Influential Citation Count: 1\n",
      "Abstract: Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.\n",
      "\n",
      "\n",
      "Title: Adaptive In-conversation Team Building for Language Model Agents\n",
      "Year: 2024\n",
      "Influential Citation Count: 1\n",
      "Abstract: Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively? Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent. It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. It allows for a flexible yet structured approach to problem-solving and can help reduce redundancy and enhance output diversity. A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering.\n",
      "\n",
      "\n",
      "Title: Stress-Testing Capability Elicitation With Password-Locked Models\n",
      "Year: 2024\n",
      "Influential Citation Count: 1\n",
      "Abstract: To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM's full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models, but may be unreliable when high-quality demonstrations are not available, e.g. as may be the case when models' (hidden) capabilities exceed those of human demonstrators.\n",
      "\n",
      "\n",
      "Title: EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms\n",
      "Year: 2024\n",
      "Influential Citation Count: 0\n",
      "Abstract: The rise of powerful large language models (LLMs) has spurred a new trend in building LLM-based autonomous agents for solving complex tasks, especially multi-agent systems. Despite the remarkable progress, we notice that existing works are heavily dependent on human-designed frameworks, which greatly limits the functional scope and scalability of agent systems. How to automatically extend the specialized agent to multi-agent systems to improve task-solving capability still remains a significant challenge. In this paper, we introduce EvoAgent, a generic method to automatically extend expert agents to multi-agent systems via the evolutionary algorithm, thereby improving the effectiveness of LLM-based agents in solving tasks. Specifically, we consider the existing agent frameworks as the initial individual and then apply a series of evolutionary operators (e.g., mutation, crossover, selection, etc.) to generate multiple agents with diverse agent settings. EvoAgent can be generalized to any LLM-based agent framework, and can automatically extend the existing agent framework to multi-agent systems without any extra human designs. Experimental results across various tasks have shown that EvoAgent can automatically generate multiple expert agents and significantly enhance the task-solving capabilities of LLM-based agents.\n",
      "\n",
      "\n",
      "Title: Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities\n",
      "Year: 2024\n",
      "Influential Citation Count: 0\n",
      "Abstract: When presented with questions involving visual thinking, humans naturally switch reasoning modalities, often forming mental images or drawing visual aids. Large language models have shown promising results in arithmetic and symbolic reasoning by expressing intermediate reasoning in text as a chain of thought, yet struggle to extend this capability to answer text queries that are easily solved by visual reasoning, even with extensive multimodal pretraining. We introduce a simple method, whiteboard-of-thought prompting, to unlock the visual reasoning capabilities of multimodal large language models across modalities. Whiteboard-of-thought prompting provides multimodal large language models with a metaphorical `whiteboard' to draw out reasoning steps as images, then returns these images back to the model for further processing. We find this can be accomplished with no demonstrations or specialized modules, instead leveraging models' existing ability to write code with libraries such as Matplotlib and Turtle. This simple approach shows state-of-the-art results on four difficult natural language tasks that involve visual and spatial reasoning. We identify multiple settings where GPT-4o using chain-of-thought fails dramatically, including more than one where it achieves $0\\%$ accuracy, while whiteboard-of-thought enables up to $92\\%$ accuracy in these same settings. We present a detailed exploration of where the technique succeeds as well as its sources of error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def get_paper_info_by_title(title, ss_api_key):\n",
    "    \"\"\"논문의 제목으로 정보를 가져오는 함수\"\"\"\n",
    "    # Define the API endpoint URL\n",
    "    url = 'https://api.semanticscholar.org/graph/v1/paper/search?query={}&fields=paperId,title,abstract,authors,citations,fieldsOfStudy,influentialCitationCount,isOpenAccess,openAccessPdf,publicationDate,publicationTypes,references,venue'\n",
    "    \n",
    "    headers = {'x-api-key': ss_api_key}\n",
    "    response = requests.get(url.format(title), headers=headers).json()\n",
    "\n",
    "    if response.get('data'):\n",
    "        paper = response['data'][0]\n",
    "        return paper\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_citing_papers(paper_id, api_key):\n",
    "    # Define the API endpoint URL\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations'\n",
    "\n",
    "    # Define the query parameters\n",
    "    # query_params = {'fields': 'title,authors,year,abstract,influentialCitationCount'}\n",
    "    query_params = {'fields': 'title,year,influentialCitationCount,abstract'}\n",
    "\n",
    "    # Define the headers\n",
    "    headers = {'x-api-key': api_key}\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(url, params=query_params, headers=headers).json()\n",
    "    try:\n",
    "        real_data = [data[\"citingPaper\"] for data in response['data']]\n",
    "        return real_data\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "query = \"Toolformer: Language Models Can Teach Themselves to Use Tools\"\n",
    "paper_count = 5\n",
    "paper_info = get_paper_info_by_title(query, ss_api_key)\n",
    "paper_id = paper_info[\"paperId\"]\n",
    "query_paper_abstract = paper_info[\"abstract\"]\n",
    "citation_paper_abstract = []\n",
    "if paper_id:\n",
    "    # Get the papers that cite the given paper ID\n",
    "    citing_papers = get_citing_papers(paper_id, ss_api_key)\n",
    "    \n",
    "    sorted_papers = sorted(citing_papers, key=lambda x: x.get('influentialCitationCount', 0), reverse=True)[:paper_count]\n",
    "\n",
    "    print(f\"{query}\\nAbstract: {query_paper_abstract}\\n\")\n",
    "    \n",
    "    print(f\"Papers citing '{query}' sorted by influential citation count:\")\n",
    "    for paper in sorted_papers:\n",
    "        title = paper['title']\n",
    "        # authors = ', '.join([author['name'] for author in paper['authors']])\n",
    "        year = paper['year']\n",
    "        abstract = paper.get('abstract', 'No abstract available')\n",
    "        citation_paper_abstract.append(abstract)\n",
    "        influential_citation_count = paper.get('influentialCitationCount', 0)\n",
    "        print(f\"\\nTitle: {title}\\nYear: {year}\\nInfluential Citation Count: {influential_citation_count}\\nAbstract: {abstract}\\n\")\n",
    "else:\n",
    "    print(\"No paper found with the given title.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\" \n",
    "내가 쓴 논문의 초록과, 내 논문을 인용한 논문들의 초록을 몇가지 보여주겠다. \n",
    "내 논문을 바탕으로 어떤 점을 개선시켰는지, 또는 어떤 점에서 아이디어를 얻었는지 추론하라. \n",
    "그리고 그것을 종합하여 내 논문에서 영감을 주는 포인트를 보기 좋게 정리하라.\n",
    "\"\"\"\n",
    "user_prompt = \"\"\n",
    "user_prompt += f\"\"\"\n",
    "### 내 논문의 초록 \n",
    "{query_paper_abstract}\n",
    "\n",
    "\"\"\"\n",
    "for idx, citabs in enumerate(citation_paper_abstract):\n",
    "    user_prompt += f\"\"\"### 내 논문을 인용한 논문의 초록 {idx + 1}\\n\"\"\"\n",
    "    user_prompt += f\"\"\"{citation_paper_abstract[idx]}\\n\\n\"\"\"\n",
    "\n",
    "user_prompt += \"\"\"\n",
    "### 답변 형식 예시\n",
    "# \"이 논문은 A(인용 논문 1에 대한 내용)라는 부분에서 영감을 줄 수 있고, B(인용 논문 2에 대한 내용) 부분에서 추가적인 시도가 있을 수 있습니다. C(인용 논문 3에 대한 내용) 부분에서는...\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "내가 쓴 논문의 초록과, 내 논문을 인용한 논문들의 초록을 몇가지 보여주겠다. \n",
      "내 논문을 바탕으로 어떤 점을 개선시켰는지, 또는 어떤 점에서 아이디어를 얻었는지 추론하라. \n",
      "그리고 그것을 종합하여 내 논문에서 영감을 주는 포인트를 보기 좋게 정리하라.\n",
      "\n",
      "\n",
      "### 내 논문의 초록 \n",
      "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n",
      "\n",
      "### 내 논문을 인용한 논문의 초록 1\n",
      "Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.\n",
      "\n",
      "### 내 논문을 인용한 논문의 초록 2\n",
      "Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively? Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent. It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. It allows for a flexible yet structured approach to problem-solving and can help reduce redundancy and enhance output diversity. A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering.\n",
      "\n",
      "### 내 논문을 인용한 논문의 초록 3\n",
      "To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM's full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models, but may be unreliable when high-quality demonstrations are not available, e.g. as may be the case when models' (hidden) capabilities exceed those of human demonstrators.\n",
      "\n",
      "### 내 논문을 인용한 논문의 초록 4\n",
      "The rise of powerful large language models (LLMs) has spurred a new trend in building LLM-based autonomous agents for solving complex tasks, especially multi-agent systems. Despite the remarkable progress, we notice that existing works are heavily dependent on human-designed frameworks, which greatly limits the functional scope and scalability of agent systems. How to automatically extend the specialized agent to multi-agent systems to improve task-solving capability still remains a significant challenge. In this paper, we introduce EvoAgent, a generic method to automatically extend expert agents to multi-agent systems via the evolutionary algorithm, thereby improving the effectiveness of LLM-based agents in solving tasks. Specifically, we consider the existing agent frameworks as the initial individual and then apply a series of evolutionary operators (e.g., mutation, crossover, selection, etc.) to generate multiple agents with diverse agent settings. EvoAgent can be generalized to any LLM-based agent framework, and can automatically extend the existing agent framework to multi-agent systems without any extra human designs. Experimental results across various tasks have shown that EvoAgent can automatically generate multiple expert agents and significantly enhance the task-solving capabilities of LLM-based agents.\n",
      "\n",
      "### 내 논문을 인용한 논문의 초록 5\n",
      "When presented with questions involving visual thinking, humans naturally switch reasoning modalities, often forming mental images or drawing visual aids. Large language models have shown promising results in arithmetic and symbolic reasoning by expressing intermediate reasoning in text as a chain of thought, yet struggle to extend this capability to answer text queries that are easily solved by visual reasoning, even with extensive multimodal pretraining. We introduce a simple method, whiteboard-of-thought prompting, to unlock the visual reasoning capabilities of multimodal large language models across modalities. Whiteboard-of-thought prompting provides multimodal large language models with a metaphorical `whiteboard' to draw out reasoning steps as images, then returns these images back to the model for further processing. We find this can be accomplished with no demonstrations or specialized modules, instead leveraging models' existing ability to write code with libraries such as Matplotlib and Turtle. This simple approach shows state-of-the-art results on four difficult natural language tasks that involve visual and spatial reasoning. We identify multiple settings where GPT-4o using chain-of-thought fails dramatically, including more than one where it achieves $0\\%$ accuracy, while whiteboard-of-thought enables up to $92\\%$ accuracy in these same settings. We present a detailed exploration of where the technique succeeds as well as its sources of error.\n",
      "\n",
      "\n",
      "### 답변 형식 예시\n",
      "# \"이 논문은 A(인용 논문 1에 대한 내용)라는 부분에서 영감을 줄 수 있고, B(인용 논문 2에 대한 내용) 부분에서 추가적인 시도가 있을 수 있습니다. C(인용 논문 3에 대한 내용) 부분에서는...\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\Desktop\\새 폴더 (2)\\kubig19th-conference-llm\\wj\\semantic_scholar_limitations.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m user_prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan you explain how to use OpenAI\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms GPT-4 API for generating text?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# 응답 생성\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m response \u001b[39m=\u001b[39m generate_response(system_prompt, user_prompt)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "\u001b[1;32mc:\\Users\\PC\\Desktop\\새 폴더 (2)\\kubig19th-conference-llm\\wj\\semantic_scholar_limitations.ipynb Cell 14\u001b[0m in \u001b[0;36mgenerate_response\u001b[1;34m(system_prompt, user_prompt)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_response\u001b[39m(system_prompt, user_prompt):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4-turbo-2024-04-09\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: system_prompt},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: user_prompt}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%282%29/kubig19th-conference-llm/wj/semantic_scholar_limitations.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "def generate_response(system_prompt, user_prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo-2024-04-09\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# 시스템 프롬프트와 사용자 프롬프트 설정\n",
    "user_prompt = \"Can you explain how to use OpenAI's GPT-4 API for generating text?\"\n",
    "\n",
    "# 응답 생성\n",
    "response = generate_response(system_prompt, user_prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
