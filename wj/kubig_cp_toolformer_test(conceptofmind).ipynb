{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- toolformer datageneration test\n",
    "- [toolformer-conceptofmind](https://github.com/conceptofmind/toolformer)를 clone 해 놓은 뒤 그 폴더 안에서 실행하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhakai2/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jinhakai2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from data_generation.retrieval import RetrievalPostprocessing\n",
    "from data_generation.calendar import CalendarPostprocessing\n",
    "from data_generation.calculator import CalculatorPostprocessing\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# ipynb에서 args 사용하려면 이렇게 해야 함.\n",
    "sys.argv = ['script_name.py', '--device_id', '0', '--num_devices', '1']\n",
    "\n",
    "parser = argparse.ArgumentParser(description='do some continuations')\n",
    "parser.add_argument('--device_id', type=int, default=0)\n",
    "parser.add_argument(\"--num_devices\", type=int, default=8)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"beomi/Llama-3-Open-Ko-8B-Instruct-preview\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "start_tokens = [\n",
    "        tokenizer(\"[\")[\"input_ids\"][0],\n",
    "        tokenizer(\" [\")[\"input_ids\"][0],\n",
    "    ]\n",
    "end_tokens = [\n",
    "        tokenizer(\"]\")[\"input_ids\"][0],\n",
    "        tokenizer(\" ]\")[\"input_ids\"][0],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jinhakai2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from tools import Retriever\n",
    "from prompts import retrieval_prompt\n",
    "from typing import List\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "MAX_BATCH_SIZE = 1  # My 3090 is weak 😔\n",
    "N = 128  # SEQ Len\n",
    "MAX_LEN = 1024  # Maximum retrieval length\n",
    "M = 16  # Min Loss Span To Consider\n",
    "\n",
    "import json\n",
    "from typing import List\n",
    "import torch\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    pipeline,\n",
    "    PreTrainedModel,\n",
    "    TextGenerationPipeline,\n",
    ")\n",
    "from torch import nn\n",
    "\n",
    "MAX_BATCH_SIZE = 1  # My 3090 is weak 😔\n",
    "N = 64  # SEQ Len\n",
    "M = 16  # Min Loss Span To Consider\n",
    "\n",
    "\n",
    "class APICallPostprocessing:\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_tokens: List[int],\n",
    "        end_tokens: List[int],\n",
    "        minimum_percentage: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Base API Postprocesing class\n",
    "\n",
    "        :param start_tokens: token representation for [ or other tokens\n",
    "        :param end_tokens:  token representation for ] or other tokens\n",
    "        :param minimum_percentage: pass percentage for candidate generation, less than this are ignored.\n",
    "        \"\"\"\n",
    "        self.start_tokens = start_tokens\n",
    "        self.end_tokens = end_tokens\n",
    "        self.minimum_percentage = minimum_percentage\n",
    "        self.api_text = \"\"  # API text, might be better to pass it in\n",
    "        self.k_values = 5  # Default topk generation, might be better to pass it in\n",
    "\n",
    "    def filter_continuations(\n",
    "        self,\n",
    "        input_tokens: torch.Tensor,\n",
    "        input_logits: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        input_start: int,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "    ) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        Grab continuations that are valid\n",
    "\n",
    "        :param input_tokens: tokenized inputs\n",
    "        :param input_logits: input logits\n",
    "        :param labels: labels for input logits\n",
    "        :param input_start: start of real input\n",
    "        :param tokenizer:\n",
    "        :return: Values and Indices\n",
    "        \"\"\"\n",
    "        # First, figure out locations...\n",
    "        probs = torch.softmax(input_logits, dim=-1)\n",
    "        # Make sure we don't keep any tokens that are supposed to be [\n",
    "        remove_tokens = 1.0 - torch.sum(\n",
    "            torch.stack([labels == start_token for start_token in self.start_tokens]),\n",
    "            dim=0,\n",
    "        )\n",
    "        # Get maximum probability... Should be sufficient. Maybe switch to sum if there's issues later\n",
    "        max_start_tokens = torch.amax(\n",
    "            torch.stack(\n",
    "                [probs[:, :, start_token] for start_token in self.start_tokens]\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        max_start_tokens = max_start_tokens * remove_tokens\n",
    "        return torch.topk(max_start_tokens[:, : -(M + 1)], k=self.k_values, dim=1)\n",
    "\n",
    "    def create_candidates(\n",
    "        self,\n",
    "        indices: torch.Tensor,\n",
    "        values: torch.Tensor,\n",
    "        input_tokens: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        input_start: int,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        generator: TextGenerationPipeline,\n",
    "        criterion: nn.CrossEntropyLoss,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates continuations of valid API calls\n",
    "\n",
    "        :param indices: index to start\n",
    "        :param values: values for filtering\n",
    "        :param input_tokens: tokenized input\n",
    "        :param labels: labels for input\n",
    "        :param input_start: real start for base loss calculation\n",
    "        :param model:\n",
    "        :param tokenizer:\n",
    "        :param generator: pipeline for text generation\n",
    "        :param criterion: Should just be CE loss\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Setup lists...\n",
    "        outputs = list()\n",
    "        num_to_keeps = list()\n",
    "        texts_to_test = list()\n",
    "        max_index = 0\n",
    "        for i, batch in enumerate(indices):\n",
    "            for j, index in enumerate(batch):\n",
    "                if values[i][j] < self.minimum_percentage:\n",
    "                    continue\n",
    "                # Get base output\n",
    "                base_outputs = model(input_tokens[:, input_start:].cuda()).logits[\n",
    "                    :, index : index + M\n",
    "                ]\n",
    "                # Find starting location...\n",
    "                num_keep = int(input_tokens[:, input_start:].shape[1] - index)\n",
    "                # Calculate loss without API\n",
    "                base_loss = criterion(\n",
    "                    base_outputs.view(-1, base_outputs.size(-1)),\n",
    "                    labels[:, index : index + M].cuda().view(-1),\n",
    "                )\n",
    "                # For padding later\n",
    "                max_index = max(max_index, index)\n",
    "                # API Text\n",
    "                texts_to_test.append(\n",
    "                    tokenizer.decode(input_tokens[:, : input_start + index][i])\n",
    "                    + f\" [{self.api_text}\"\n",
    "                )\n",
    "                # grab 5 generations\n",
    "                outputs.append(\n",
    "                    generator(\n",
    "                        texts_to_test[-1], max_new_tokens=28, num_return_sequences=5\n",
    "                    )\n",
    "                )\n",
    "                # Add additional items to generation outputs...\n",
    "                for k in range(5):\n",
    "                    outputs[-1][k][\"index\"] = int(index)\n",
    "                    outputs[-1][k][\"base_loss\"] = float(base_loss.item())\n",
    "                    outputs[-1][k][\"base_outputs\"] = base_outputs\n",
    "                # So we know where to look\n",
    "                num_to_keeps.append(num_keep)\n",
    "        return outputs, num_to_keeps, texts_to_test, max_index\n",
    "\n",
    "    def add_api_calls(\n",
    "        self,\n",
    "        candidate: int,\n",
    "        outputs: dict,\n",
    "        texts_to_test: List[str],\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        input_tokens: torch.Tensor,\n",
    "        input_start: int,\n",
    "        nums_to_keep: List[int],\n",
    "        base_loss: float,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add API calls here.\n",
    "\n",
    "        :param candidate: which candidate is being parsed\n",
    "        :param outputs: individual candidate outputs\n",
    "        :param texts_to_test: text for candidates\n",
    "        :param tokenizer:\n",
    "        :param input_tokens:\n",
    "        :param input_start:\n",
    "        :param nums_to_keep: values kept after generation\n",
    "        :param base_loss: base loss value for candidate\n",
    "        :param args: args to pass to subclass\n",
    "        :param kwargs: kwargs to pass to subclass\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill this in with your API code please!\")\n",
    "\n",
    "    def generate_continuations(\n",
    "        self,\n",
    "        input_tokens: torch.Tensor,\n",
    "        input_logits: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate continuations\n",
    "\n",
    "        :param input_tokens: input to model\n",
    "        :param input_logits: output from model\n",
    "        :param labels: labels for logits\n",
    "        :param model:\n",
    "        :param tokenizer:\n",
    "        :param args: args to pass to add_api_calls\n",
    "        :param kwargs: kwargs to pass to add_api_calls\n",
    "        :return: individual candidate outputs\n",
    "        \"\"\"\n",
    "        # Setup token stuff...\n",
    "        input_start = input_tokens.shape[1] - input_logits.shape[1]\n",
    "        start_str = tokenizer.decode(input_tokens[:, :input_start][0])\n",
    "        # Find top tokens...\n",
    "        values, indices = self.filter_continuations(\n",
    "            input_tokens, input_logits, labels, input_start, tokenizer\n",
    "        )\n",
    "        # setup generation calls...\n",
    "        # generator = pipeline(\n",
    "        #     \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
    "        # )  # type: TextGenerationPipeline\n",
    "        generator = pipeline(\n",
    "            \"text-generation\", model=model, tokenizer=tokenizer\n",
    "        )  # type: TextGenerationPipeline\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        with torch.no_grad():\n",
    "            outputs, num_to_keeps, texts_to_test, max_index = self.create_candidates(\n",
    "                indices,\n",
    "                values,\n",
    "                input_tokens,\n",
    "                labels,\n",
    "                input_start,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                generator,\n",
    "                criterion,\n",
    "            )\n",
    "            for i in range(len(outputs)):\n",
    "                generated_texts, max_token_len, max_token_len_base = self.add_api_calls(\n",
    "                    i,\n",
    "                    outputs[i],\n",
    "                    texts_to_test,\n",
    "                    tokenizer,\n",
    "                    input_tokens,\n",
    "                    input_start,\n",
    "                    num_to_keeps,\n",
    "                    outputs[i][0][\"base_loss\"],\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                if len(generated_texts) == 0:\n",
    "                    outputs[i] = None\n",
    "                    continue\n",
    "                # shape the batches...\n",
    "                for j in range(len(generated_texts)):\n",
    "                    generated_texts[j].append(\n",
    "                        max_token_len - generated_texts[j][0].shape[1]\n",
    "                    )\n",
    "                    if generated_texts[j][-1] != 0:\n",
    "                        generated_texts[j][0] = torch.cat(\n",
    "                            (\n",
    "                                generated_texts[j][0],\n",
    "                                torch.zeros(\n",
    "                                    (1, generated_texts[j][-1]),\n",
    "                                    dtype=generated_texts[j][0].dtype,\n",
    "                                    device=generated_texts[j][0].device,\n",
    "                                ),\n",
    "                            ),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                    generated_texts[j].append(\n",
    "                        max_token_len_base - generated_texts[j][1].shape[1]\n",
    "                    )\n",
    "                    if generated_texts[j][-1] != 0:\n",
    "                        generated_texts[j][1] = torch.cat(\n",
    "                            (\n",
    "                                generated_texts[j][1],\n",
    "                                torch.zeros(\n",
    "                                    (1, generated_texts[j][-1]),\n",
    "                                    dtype=generated_texts[j][1].dtype,\n",
    "                                    device=generated_texts[j][1].device,\n",
    "                                ),\n",
    "                            ),\n",
    "                            dim=1,\n",
    "                        )\n",
    "\n",
    "                test_outputs = model(\n",
    "                    torch.cat(\n",
    "                        list(generated_text[0] for generated_text in generated_texts),\n",
    "                        dim=0,\n",
    "                    )\n",
    "                ).logits\n",
    "                base_outputs = model(\n",
    "                    torch.cat(\n",
    "                        list(generated_text[1] for generated_text in generated_texts),\n",
    "                        dim=0,\n",
    "                    )\n",
    "                ).logits\n",
    "                best_loss = -99.0\n",
    "                best_output = outputs[i][0]\n",
    "                for j in range(len(generated_texts)):\n",
    "                    num_to_keep = generated_texts[j][2]\n",
    "                    if generated_texts[j][-2] != 0:\n",
    "                        test = test_outputs[j][: -generated_texts[j][-2]]\n",
    "                        test_loss = criterion(\n",
    "                            test[-num_to_keep : -(num_to_keep - M)].view(\n",
    "                                -1, generated_texts[j][-3][\"base_outputs\"].size(-1)\n",
    "                            ),\n",
    "                            labels[:, -num_to_keep : -(num_to_keep - M)]\n",
    "                            .cuda()\n",
    "                            .view(-1),\n",
    "                        )\n",
    "                    else:\n",
    "                        test_loss = criterion(\n",
    "                            test_outputs[j][-num_to_keep : -(num_to_keep - M)].view(\n",
    "                                -1, generated_texts[j][-3][\"base_outputs\"].size(-1)\n",
    "                            ),\n",
    "                            labels[:, -num_to_keep : -(num_to_keep - M)]\n",
    "                            .cuda()\n",
    "                            .view(-1),\n",
    "                        )\n",
    "                    if generated_texts[j][-1] != 0:\n",
    "                        base = base_outputs[j][: -generated_texts[j][-1]]\n",
    "                        base_loss = criterion(\n",
    "                            base[-num_to_keep : -(num_to_keep - M)].view(\n",
    "                                -1, generated_texts[j][-3][\"base_outputs\"].size(-1)\n",
    "                            ),\n",
    "                            labels[:, -num_to_keep : -(num_to_keep - M)]\n",
    "                            .cuda()\n",
    "                            .view(-1),\n",
    "                        )\n",
    "                    else:\n",
    "                        base_loss = criterion(\n",
    "                            base_outputs[j][-num_to_keep : -(num_to_keep - M)].view(\n",
    "                                -1, generated_texts[j][-3][\"base_outputs\"].size(-1)\n",
    "                            ),\n",
    "                            labels[:, -num_to_keep : -(num_to_keep - M)]\n",
    "                            .cuda()\n",
    "                            .view(-1),\n",
    "                        )\n",
    "                    generated_texts[j][-3][\"generated_text\"] = generated_texts[j][-3][\n",
    "                        \"generated_text\"\n",
    "                    ].replace(start_str, \"\")\n",
    "                    if (\n",
    "                        min(base_loss.item(), generated_texts[j][-3][\"base_loss\"])\n",
    "                        - test_loss\n",
    "                        > best_loss\n",
    "                    ):\n",
    "                        best_output = generated_texts[j][-3]\n",
    "                        best_loss = generated_texts[j][-3][\"base_loss\"] - test_loss\n",
    "                if len(generated_texts) > 0:\n",
    "                    outputs[i] = best_output\n",
    "                    outputs[i][\"Score\"] = float(best_loss.item())\n",
    "                    outputs[i][\"base_api_loss\"] = float(base_loss.item())\n",
    "                    del outputs[i][\"base_outputs\"]\n",
    "                else:\n",
    "                    outputs[i] = None\n",
    "        # print(json.dumps(outputs, indent=2))\n",
    "        return outputs\n",
    "\n",
    "    def parse_article(\n",
    "        self, data: dict, model: PreTrainedModel, tokenizer: PreTrainedTokenizerBase\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes in data dict and parses it into API continuations\n",
    "        :param data: data, assuming it's from load_dataset and has a text field\n",
    "        :param model:\n",
    "        :param tokenizer:\n",
    "        :return: outputs for the input data, should have index of API call insertion, API, and score value at minimum.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill this in for what you need to do please!\")\n",
    "\n",
    "class RetrievalPostprocessing(APICallPostprocessing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_tokens: List[int],\n",
    "        end_tokens: List[int],\n",
    "        minimum_percentage: float = 0.1,\n",
    "    ):\n",
    "        self.retriever = Retriever()\n",
    "        self.api_text = \"Retrieval(\"\n",
    "        super().__init__(start_tokens, end_tokens, minimum_percentage)\n",
    "\n",
    "    def add_api_calls(\n",
    "        self,\n",
    "        candidate: int,\n",
    "        outputs: dict,\n",
    "        texts_to_test: List[str],\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        input_tokens: torch.Tensor,\n",
    "        input_start: int,\n",
    "        nums_to_keep: List[int],\n",
    "        base_loss: float,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        retrieval_strings = args[0]\n",
    "        generated_texts = list()\n",
    "        max_token_len = N\n",
    "        max_token_len_base = N\n",
    "        for j in range(len(outputs)):\n",
    "            outputs[j][\"Retrieval\"] = outputs[j][\"generated_text\"].replace(\n",
    "                texts_to_test[candidate], \"\"\n",
    "            )\n",
    "            outputs[j][\"Generated\"] = outputs[j][\"generated_text\"].split(\"Output:\")[-1]\n",
    "            if \"]\" in outputs[j][\"Retrieval\"]:\n",
    "                outputs[j][\"Retrieval\"] = (\n",
    "                    outputs[j][\"Retrieval\"].replace(\"Retrieval(\", \"\").split(\"]\")[0]\n",
    "                )\n",
    "                if \")\" in outputs[j][\"Retrieval\"]:\n",
    "                    outputs[j][\"Retrieval\"] = outputs[j][\"Retrieval\"].split(\")\")[0]\n",
    "                outputs[j][\"Retrieval_text\"] = (\n",
    "                    \"[Retrieval(\" + outputs[j][\"Retrieval\"] + \")\"\n",
    "                )\n",
    "                base_inputs = tokenizer(\n",
    "                    outputs[j][\"Retrieval_text\"] + \"]\" + \"\\n\",\n",
    "                    return_tensors=\"pt\",\n",
    "                )[\"input_ids\"].cuda()\n",
    "                outputs[j][\"Retrieval\"] = self.retriever.retrieval(\n",
    "                    retrieval_strings, outputs[j][\"Retrieval\"], 3\n",
    "                )\n",
    "                outputs[j][\"Retrieval_output\"] = [outputs[j][\"Retrieval_text\"][1:], \", \".join(outputs[j][\"Retrieval\"])]\n",
    "                outputs[j][\"Retrieval_text\"] = (\n",
    "                    outputs[j][\"Retrieval_text\"]\n",
    "                    + \"->\"\n",
    "                    + \", \".join(outputs[j][\"Retrieval\"])\n",
    "                    + \"]\"\n",
    "                )\n",
    "                test_inputs = tokenizer(\n",
    "                    outputs[j][\"Retrieval_text\"] + \"\\n\",\n",
    "                    return_tensors=\"pt\",\n",
    "                )[\"input_ids\"].cuda()\n",
    "                test_inputs = torch.concat(\n",
    "                    [\n",
    "                        test_inputs.cuda(),\n",
    "                        input_tokens[:, input_start:].cuda(),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                if test_inputs.shape[1] > MAX_LEN:\n",
    "                    continue\n",
    "                base_inputs = torch.concat(\n",
    "                    [\n",
    "                        base_inputs.cuda(),\n",
    "                        input_tokens[:, input_start:].cuda(),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                max_token_len = max(max_token_len, test_inputs.shape[1])\n",
    "                max_token_len_base = max(max_token_len_base, test_inputs.shape[1])\n",
    "                generated_texts.append(\n",
    "                    [\n",
    "                        test_inputs,\n",
    "                        base_inputs,\n",
    "                        nums_to_keep[candidate],\n",
    "                        base_loss,\n",
    "                        outputs[j],\n",
    "                    ]\n",
    "                )\n",
    "        return generated_texts, max_token_len, max_token_len_base\n",
    "\n",
    "    def parse_article(\n",
    "        self, data: dict, model: PreTrainedModel, tokenizer: PreTrainedTokenizerBase\n",
    "    ):\n",
    "        outputs = list()\n",
    "        # tokens = tokenizer(data[\"text\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "        tokens = tokenizer(data[text_field], return_tensors=\"pt\")[\"input_ids\"]\n",
    "        print(tokens)\n",
    "        start_step = 2048//N\n",
    "        ret_skip = 1024//N  # naively assuming the model should be able to look back if it's less than this.\n",
    "        total_steps = tokens.shape[1]//N\n",
    "        for i in range(start_step, total_steps):\n",
    "            input_tokens = tokens[:, (-N * (i + 1) - 1) : (-N * (i) - 1)]\n",
    "            labels = tokens[\n",
    "                :,\n",
    "                int(tokens.shape[1] + (-N * (i + 1))) : int(tokens.shape[1] + (-N * i)),\n",
    "            ]\n",
    "            ret_tokens = tokens[:, : (-(N) * ((i - ret_skip) + 1) - 1)]\n",
    "            # print(tokens.shape)\n",
    "            string = tokenizer.decode(input_tokens[0])\n",
    "            print(string)\n",
    "            ret_strings = tokenize.sent_tokenize(tokenizer.decode(ret_tokens[0]))\n",
    "            print(ret_strings)\n",
    "            # print(ret_strings)\n",
    "            model_input = tokenizer(\n",
    "                retrieval_prompt.replace(\"<REPLACEGPT>\", string) + string,\n",
    "                return_tensors=\"pt\",\n",
    "            )[\"input_ids\"]\n",
    "            # print(string)\n",
    "            # print(model_input.shape)\n",
    "            with torch.no_grad():\n",
    "                output = model(model_input.cuda()).logits.cpu()[:, -N:]\n",
    "            new_outputs = self.generate_continuations(\n",
    "                model_input,\n",
    "                output,\n",
    "                labels,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                ret_strings,\n",
    "            )\n",
    "            for output in new_outputs:\n",
    "                if output is None:\n",
    "                    continue\n",
    "                output[\"index\"] += int(tokens.shape[1] + (-N * (i + 1)))\n",
    "                # filter by score\n",
    "                if output[\"Score\"] > 1.0:\n",
    "                    outputs.append([output[\"Score\"], output[\"index\"]] + output[\"Retrieval_output\"])\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "api_handler = RetrievalPostprocessing(start_tokens, end_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# beomi/Llama-3-Open-Ko-8B-Instruct-preview 모델을 4bit로 load\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=False,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=\"float16\",\n",
    "            )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhakai2/.local/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for c4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/c4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jinhakai2/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 한국어 데이터셋을 가져옴. dp는 문장 내에서 단어 간의 의존 관계를 분석하는 데이터셋.\n",
    "# 일반 평문을 가져오기 위해 dp로 선택\n",
    "# dataset = load_dataset(\"klue\", \"dp\", split=\"train\", streaming=True)\n",
    "# dataset = load_dataset(\"squad_kor_v1\", split=\"train\", streaming=True)\n",
    "dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
    "iter_data = iter(dataset)\n",
    "test = False\n",
    "counter = 0\n",
    "file_counter = 0\n",
    "found_examples = 0\n",
    "output_dataset = list()\n",
    "start_time = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter_data)\n",
    "text_field = \"text\"\n",
    "data[text_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성할 개수 목표\n",
    "\n",
    "num_examples = int(3/float(args.num_devices))\n",
    "start_count = -1\n",
    "if os.path.isfile(f\"retrieval_data_{args.device_id}.json\"):\n",
    "    with open(f\"retrieval_data_{args.device_id}.json\") as f:\n",
    "        output_dataset = json.load(f)\n",
    "        start_count = output_dataset[-1]['file_index']\n",
    "        for item in output_dataset:\n",
    "            num_examples -= len(item['retrieval_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "import dateutil.parser as dparser\n",
    "import random\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class AvailableAPIs:\n",
    "    \"\"\"Keeps track of available APIs\"\"\"\n",
    "\n",
    "    retrieval: bool = True\n",
    "    calendar: bool = True\n",
    "    calculator: bool = True\n",
    "    llmchain: bool = True\n",
    "\n",
    "    def check_any_available(self):\n",
    "        return any([self.retrieval, self.calendar, self.calculator])\n",
    "\n",
    "def check_apis_available(\n",
    "    data: dict, tokenizer: PreTrainedTokenizerBase\n",
    ") -> AvailableAPIs:\n",
    "    \"\"\"\n",
    "    Returns available APIs with boolean flags\n",
    "\n",
    "    :param data: from load_dataset, assumes ['text'] is available\n",
    "    :param tokenizer: Tokenizer to tokenize data\n",
    "    :return: AvailableAPIs\n",
    "    \"\"\"\n",
    "    # tokenized_data = tokenizer(data[\"text\"])[\"input_ids\"]\n",
    "    tokenized_data = tokenizer(data[text_field])[\"input_ids\"]\n",
    "    available = AvailableAPIs()\n",
    "    # In case we need a different version, found this here:\n",
    "    # https://stackoverflow.com/questions/28198370/regex-for-validating-correct-input-for-calculator\n",
    "    calc_pattern = re.compile(\"^(\\d+[\\+\\-\\*\\/]{1})+\\d+$\")\n",
    "    # if len(tokenized_data) < 4096:\n",
    "    #     available.retrieval = False\n",
    "    # try:\n",
    "        # date = dparser.parse(data[\"url\"], fuzzy=True)\n",
    "    # except (ValueError, OverflowError):\n",
    "    #     available.calendar = False\n",
    "    available.retrieval = True\n",
    "    available.calendar = False\n",
    "    available.calculator = False\n",
    "    tried_rand = False\n",
    "    for i in range(len(tokenized_data) // 100):\n",
    "        text = tokenizer.decode(tokenized_data[i * 100 : (i + 1) * 100])\n",
    "\n",
    "        operators = bool(re.search(calc_pattern, text))\n",
    "        equals = any(\n",
    "            [\"=\" in text, \"equal to\" in text, \"total of\" in text, \"average of\" in text]\n",
    "        )\n",
    "        if not (operators and equals) and not tried_rand:\n",
    "            tried_rand = True\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            text = text.split(\" \")\n",
    "            text = [item for item in text if item.replace(\".\", \"\", 1).isnumeric()]\n",
    "            if len(text) >= 3:\n",
    "                if random.randint(0, 99) == 0:\n",
    "                    available.calculator = True\n",
    "        else:\n",
    "            available.calculator = True\n",
    "\n",
    "    return available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import datetime\n",
    "iter_count = 0\n",
    "max_iter = 300\n",
    "korea_time_zone = pytz.timezone('Asia/Seoul')\n",
    "current_time_kst = datetime.datetime.now(korea_time_zone)\n",
    "filename = f\"retrieval_data_{args.device_id}_{current_time_kst.strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n",
    "\n",
    "while found_examples < num_examples:\n",
    "    if iter_count == max_iter:\n",
    "        break\n",
    "    iter_count +=1\n",
    "    try:\n",
    "        data = next(iter_data)\n",
    "    except StopIteration:\n",
    "        break  # 더 이상 데이터가 없으므로 루프를 종료합니다.\n",
    "    if file_counter < start_count: # 0 < -1\n",
    "        file_counter += 1\n",
    "        continue\n",
    "    if file_counter % args.num_devices != args.device_id: # 0 != 0\n",
    "        file_counter += 1\n",
    "        continue\n",
    "    available = check_apis_available(data, tokenizer)\n",
    "    test = available.retrieval\n",
    "    if test:\n",
    "        data_outputs = api_handler.parse_article(data, model, tokenizer)\n",
    "        output_dataset.append(\n",
    "            {\n",
    "                \"file_index\": file_counter,\n",
    "                \"text\": data[text_field],\n",
    "                \"retrieval_outputs\": data_outputs\n",
    "            }\n",
    "        )\n",
    "        prev_found = found_examples\n",
    "        found_examples += len(output_dataset[-1][\"retrieval_outputs\"])\n",
    "        eta_s = (num_examples - found_examples) * (time.process_time()-start_time) / max(1, found_examples)\n",
    "        eta_m = eta_s // 60\n",
    "        eta_h = eta_m // 60\n",
    "        eta_m = eta_m - (eta_h*60)\n",
    "        eta_s = eta_s - ((eta_m*60) + (eta_h*60*60))\n",
    "        print(f\"Found: {found_examples}/{num_examples}, ETA: {eta_h}H:{eta_m}M:{eta_s}s\")\n",
    "        if found_examples//100 > prev_found//100:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(output_dataset, f, indent=2)\n",
    "        counter += 1\n",
    "    file_counter += 1\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_dataset, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
