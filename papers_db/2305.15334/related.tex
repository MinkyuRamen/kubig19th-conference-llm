\section{Related Work}
\label{sec:related}

\paragraph{Large Language Models}
Recent strides in the field of LLMs have renovated many downstream domains~\cite{chowdhery2022palm, touvron2023llama, zhang2022opt, zeng2022glm}, not only in traditional natural language processing tasks but also in program synthesis. Many of these advances are achieved by augmenting pre-trained LLMs by prompting~\cite{wei2022chain, gao2022pal} and instruction fine-tuning~\cite{chung2022scaling, sanh2021multitask, wang2022super, iyer2022opt}. Recent open-sourced models like
LLaMa~\cite{touvron2023llama}, Alpaca~\cite{alpaca}, and Vicuna~\cite{vicuna} have furthered the understanding of LLMs and facilitated their experimentation. While our approach, \oursmethod{}, incorporates techniques akin to those mentioned, its primary emphasis is on enhancing the LLMs' ability to utilize millions of tools, as opposed to refining their conversational skills. Additionally, we pioneer the study of fine-tuning a base model by supplementing it with information retrieval - a first, to the best of our knowledge.

\paragraph{Tool Usage}
The discussion of tool usage within LLMs has seen an upsurge, with models like Toolformer taking the lead~\cite{schick2023toolformer, komeili2021internet, lazaridou2022internet, nakano2021webgpt}. Tools often incorporated include web-browsing~\cite{schick2020exploiting}, calculators~\cite{cobbe2021training, thoppilan2022lamda}, translation systems~\cite{thoppilan2022lamda}, and Python interpreters~\cite{gao2022pal}. 
While these efforts can be seen as preliminary explorations of marrying LLMs with tool usage, they generally focus on specific tools. Our paper, in contrast, aims to explore a vast array of tools (i.e., API calls) in an open-ended fashion, potentially covering a wide range of applications. 


With the recent launch of Toolformer~\cite{schick2023toolformer} and GPT-4~\cite{openai2023gpt4}, the importance of API calls has been highlighted, encouraging many works in employing API calls as tooling
~\cite{shen2023hugginggpt, liang2023taskmatrix}. Moreover, the application of API calls in robotics has been explored to some extent~\cite{vemprala2023chatgpt, ahn2022can}. However, these works primarily aim at showcasing the potential of ``prompting'' LLMs rather than establishing a systematic method for evaluation and training (including fine-tuning). Our work, on the other hand, concentrates on systematic evaluation and building a pipeline for future use.

\paragraph{LLMs for Program Synthesis}
Harnessing LLMs for program synthesis has historically been a challenging task~\cite{li2022competition, chen2021evaluating, xu2022systematic, jain2022jigsaw, devlin2017robustfill, lachaux2020unsupervised}. Researchers have proposed an array of strategies to prompt LLMs to perform better in coding tasks, including in-context learning~\cite{wei2022chain, kojima2022large, chen2021evaluating}, task decomposition~\cite{kim2023language, yao2022react}, and self-debugging~\cite{chen2023teaching, shinn2023reflexion}. Besides prompting, there have also been efforts to pretrain language models specifically for code generation~\cite{nijkamp2022codegen, li2023starcoder, nijkamp2023codegen2}.

However, these strategies focus on prompting large language models or pre-training them for general program synthesis. In our research, in contrast, we focus on a much restricted domain: the synthesis of linear programs using API calls. General program synthesis, not only is complex, but is also hard to verify and evaluate. API calls, on the other hand, function more like tool usage. This allows the LLM to significantly expand its capabilities without grappling with low-level implementation details.