{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/minkyuramen/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/minkyuramen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import os\n",
    "from pprint import pprint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "import tool_pool as tp\n",
    "\n",
    "dotenv_path = '/Users/minkyuramen/Desktop/project/env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# load tool\n",
    "tools = [tp.loadpaper, tp.recommendpaper, tp.loadfigure]\n",
    "# load Agent prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM that will drive the agent\n",
    "# Only certain models support this\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': \"'ToolFormer : Language Models Can Teach Themselves to Use Tools'\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n",
      "title\n",
      "Toolformer: Language Models Can Teach Themselves to Use Tools\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Approach\n",
      "        Sampling API Calls\n",
      "        Executing API Calls\n",
      "        Filtering API Calls\n",
      "        Model Finetuning\n",
      "        Inference\n",
      "  3 Tools\n",
      "        Question Answering\n",
      "        Calculator\n",
      "        Wikipedia Search\n",
      "        Machine Translation System\n",
      "        Calendar\n",
      "  4 Experiments\n",
      "    4.1 Experimental Setup\n",
      "        Dataset Generation\n",
      "        Model Finetuning\n",
      "        Baseline Models\n",
      "    4.2 Downstream Tasks\n",
      "      4.2.1 LAMA\n",
      "      4.2.2 Math Datasets\n",
      "      4.2.3 Question Answering\n",
      "      4.2.4 Multilingual Question Answering\n",
      "      4.2.5 Temporal Datasets\n",
      "    4.3 Language Modeling\n",
      "    4.4 Scaling Laws\n",
      "  5 Analysis\n",
      "        Decoding Strategy\n",
      "        Data Quality\n",
      "  6 Related Work\n",
      "        Language Model Pretraining\n",
      "        Tool Use\n",
      "        Bootstrapping\n",
      "  7 Limitations\n",
      "  8 Conclusion\n",
      "  References\n",
      "  Appendix A API Details\n",
      "    A.1 Implementation\n",
      "        Question Answering\n",
      "        Calculator\n",
      "        Calendar\n",
      "        Machine Translation\n",
      "    A.2 Prompts\n",
      "        Question Answering\n",
      "        Calculator\n",
      "        Wikipedia Search\n",
      "        Machine Translation\n",
      "        Calendar\n",
      "  Appendix B Toolformer Training\n",
      "  Appendix C Zero-Shot Prompts\n",
      "    C.1 LAMA and TempLAMA\n",
      "    C.2 Math Benchmarks\n",
      "    C.3 Question Answering\n",
      "    C.4 Multilingual Question Answering\n",
      "  Appendix D Dateset\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': \"'ToolFormer : Language Models Can Teach Themselves to Use Tools'\", 'sections': ['Filtering API Calls']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mFiltering API Calls\n",
      "Let iğ‘–i be the position of the API call cisubscriptğ‘ğ‘–c_{i} in the sequence ğ±=x1,â€¦,xnğ±subscriptğ‘¥1â€¦subscriptğ‘¥ğ‘›\\mathbf{x}=x_{1},\\ldots,x_{n}, and let risubscriptğ‘Ÿğ‘–r_{i} be the response from the API. Further, given a sequence (wiâˆ£iâˆˆâ„•)conditionalsubscriptğ‘¤ğ‘–ğ‘–â„•(w_{i}\\mid i\\in\\mathbb{N}) of weights, let\n",
      "\n",
      "Liâ€‹(ğ³)=âˆ’âˆ‘j=inwjâˆ’iâ‹…logâ¡pMâ€‹(xjâˆ£ğ³,x1:jâˆ’1)subscriptğ¿ğ‘–ğ³superscriptsubscriptğ‘—ğ‘–ğ‘›â‹…subscriptğ‘¤ğ‘—ğ‘–subscriptğ‘ğ‘€conditionalsubscriptğ‘¥ğ‘—ğ³subscriptğ‘¥:1ğ‘—1L_{i}(\\mathbf{z})=-\\sum_{j=i}^{n}w_{j-i}\\cdot\\log{p_{M}(x_{j}\\mid\\mathbf{z},x_{1:j-1})}\n",
      "\n",
      "be the weighted cross entropy loss for Mğ‘€M over the tokens xi,â€¦,xnsubscriptğ‘¥ğ‘–â€¦subscriptğ‘¥ğ‘›x_{i},\\ldots,x_{n} if the model is prefixed with ğ³ğ³\\mathbf{z}.\n",
      "We compare two different instantiations of this loss:\n",
      "\n",
      "Li+superscriptsubscriptğ¿ğ‘–\\displaystyle L_{i}^{+}\n",
      "=Liâ€‹(eâ€‹(ci,ri))absentsubscriptğ¿ğ‘–esubscriptğ‘ğ‘–subscriptğ‘Ÿğ‘–\\displaystyle=L_{i}(\\text{e}(c_{i},r_{i}))\n",
      "\n",
      "Liâˆ’superscriptsubscriptğ¿ğ‘–\\displaystyle L_{i}^{-}\n",
      "=minâ¡(Liâ€‹(Îµ),Liâ€‹(eâ€‹(ci,Îµ)))absentsubscriptğ¿ğ‘–ğœ€subscriptğ¿ğ‘–esubscriptğ‘ğ‘–ğœ€\\displaystyle=\\min\\left(L_{i}(\\varepsilon),L_{i}(\\text{e}(c_{i},\\varepsilon))\\right)\n",
      "\n",
      "where Îµğœ€\\varepsilon denotes an empty sequence. The former is the weighted loss over all tokens xi,â€¦,xnsubscriptğ‘¥ğ‘–â€¦subscriptğ‘¥ğ‘›x_{i},\\ldots,x_{n} if the API call and its result are given to Mğ‘€M as a prefix;333We provide eâ€‹(ci,ri)esubscriptğ‘ğ‘–subscriptğ‘Ÿğ‘–\\text{e}(c_{i},r_{i}) as a prefix instead of inserting it at position iğ‘–i because Mğ‘€M is not yet finetuned on any examples containing API calls, so inserting it in the middle of ğ±ğ±\\mathbf{x} would interrupt the flow and not align with patterns in the pretraining corpus, thus hurting perplexity. the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to Mğ‘€M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a filtering threshold Ï„fsubscriptğœğ‘“\\tau_{f}, we thus only keep API calls for which\n",
      "\n",
      "Liâˆ’âˆ’Li+â‰¥Ï„fsuperscriptsubscriptğ¿ğ‘–superscriptsubscriptğ¿ğ‘–subscriptğœğ‘“L_{i}^{-}-L_{i}^{+}\\geq\\tau_{f}\n",
      "\n",
      "holds, i.e., adding the API call and its result reduces the loss by at least Ï„fsubscriptğœğ‘“\\tau_{f}, compared to not doing any API call or obtaining no result from it.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m'Filtering API Calls' íŒŒíŠ¸ì—ì„œëŠ” API í˜¸ì¶œì„ í•„í„°ë§í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìˆ˜ì‹ê³¼ í•¨ê»˜ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "ë¨¼ì €, API í˜¸ì¶œ ciì— ëŒ€í•œ ìœ„ì¹˜ë¥¼ ğ‘–ië¡œ ì •ì˜í•˜ê³ , í•´ë‹¹ APIì˜ ì‘ë‹µì„ rië¡œ ì •ì˜í•©ë‹ˆë‹¤. ë˜í•œ, ê°€ì¤‘ì¹˜ ì‹œí€€ìŠ¤ (wiâˆ£iâˆˆâ„•)ë¥¼ ê³ ë ¤í•  ë•Œ, ë‹¤ìŒê³¼ ê°™ì´ ê°€ì¤‘ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ì •ì˜í•©ë‹ˆë‹¤:\n",
      "\n",
      "Liâ€‹(ğ³)=âˆ’âˆ‘j=inwjâˆ’iâ‹…logâ¡pMâ€‹(xjâˆ£ğ³,x1:jâˆ’1)\n",
      "\n",
      "ì—¬ê¸°ì„œ Li(ğ³)ëŠ” ëª¨ë¸ Mì— ëŒ€í•œ ê°€ì¤‘ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë˜í•œ, Li+ì™€ Li-ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:\n",
      "\n",
      "Li+ = Liâ€‹(eâ€‹(ci,ri))\n",
      "Li- = minâ¡(Liâ€‹(Îµ),Liâ€‹(eâ€‹(ci,Îµ))\n",
      "\n",
      "ì—¬ê¸°ì„œ ÎµëŠ” ë¹ˆ ì‹œí€€ìŠ¤ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Li+ëŠ” API í˜¸ì¶œê³¼ í•´ë‹¹ ê²°ê³¼ë¥¼ ëª¨ë¸ Mì— ì ‘ë‘ì–´ë¡œ ì œê³µí–ˆì„ ë•Œì˜ ì†ì‹¤ì„ ë‚˜íƒ€ë‚´ë©°, Li-ëŠ” API í˜¸ì¶œì„ í•˜ì§€ ì•Šê±°ë‚˜ ê²°ê³¼ë¥¼ ì œê³µí•˜ì§€ ì•Šì•˜ì„ ë•Œì˜ ì†ì‹¤ ì¤‘ ìµœì†Ÿê°’ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
      "\n",
      "ë§ˆì§€ë§‰ìœ¼ë¡œ, í•„í„°ë§ ì„ê³„ê°’ Ï„fì— ëŒ€í•´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” API í˜¸ì¶œë§Œ ìœ ì§€ë©ë‹ˆë‹¤:\n",
      "\n",
      "Li--Li+ â‰¥ Ï„f\n",
      "\n",
      "ì¦‰, API í˜¸ì¶œê³¼ í•´ë‹¹ ê²°ê³¼ë¥¼ ì œê³µí–ˆì„ ë•Œì˜ ì†ì‹¤ì´ API í˜¸ì¶œì„ í•˜ì§€ ì•Šì•˜ì„ ë•Œì˜ ì†ì‹¤ë³´ë‹¤ ì ì–´ë„ Ï„fë§Œí¼ ê°ì†Œí•´ì•¼ í•´ë‹¹ API í˜¸ì¶œì„ ìœ ì§€í•©ë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# output = agent_executor.invoke({\"input\": \"explain about the math expression about the Filtering api calls in the paper 'ToolFormer : Language Models Can Teach Themselves to Use Tools' í•œê¸€ë¡œ ë§í•´ì¤˜\"})\n",
    "output = agent_executor.invoke({\"input\": \"'ToolFormer : Language Models Can Teach Themselves to Use Tools'ì—ì„œ api callsë¥¼ filtering í•˜ëŠ” íŒŒíŠ¸ë¥¼ ìˆ˜ì‹ê³¼ í•¨ê»˜ ì„¤ëª…í•´ì¤˜. í•œê¸€ë¡œ ëŒ€ë‹µí•´ì¤˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'Filtering API Calls' íŒŒíŠ¸ì—ì„œëŠ” API í˜¸ì¶œì„ í•„í„°ë§í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìˆ˜ì‹ê³¼ í•¨ê»˜ ì„¤ëª…í•˜ë©´ \"\n",
      " 'ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n'\n",
      " '\\n'\n",
      " 'ë¨¼ì €, API í˜¸ì¶œ ciì— ëŒ€í•œ ìœ„ì¹˜ë¥¼ ğ‘–ië¡œ ì •ì˜í•˜ê³ , í•´ë‹¹ APIì˜ ì‘ë‹µì„ rië¡œ ì •ì˜í•©ë‹ˆë‹¤. ë˜í•œ, ê°€ì¤‘ì¹˜ ì‹œí€€ìŠ¤ (wiâˆ£iâˆˆâ„•)ë¥¼ '\n",
      " 'ê³ ë ¤í•  ë•Œ, ë‹¤ìŒê³¼ ê°™ì´ ê°€ì¤‘ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ì •ì˜í•©ë‹ˆë‹¤:\\n'\n",
      " '\\n'\n",
      " 'Li\\u200b(ğ³)=âˆ’âˆ‘j=inwjâˆ’iâ‹…log\\u2061pM\\u200b(xjâˆ£ğ³,x1:jâˆ’1)\\n'\n",
      " '\\n'\n",
      " 'ì—¬ê¸°ì„œ Li(ğ³)ëŠ” ëª¨ë¸ Mì— ëŒ€í•œ ê°€ì¤‘ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë˜í•œ, Li+ì™€ Li-ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:\\n'\n",
      " '\\n'\n",
      " 'Li+ = Li\\u200b(e\\u200b(ci,ri))\\n'\n",
      " 'Li- = min\\u2061(Li\\u200b(Îµ),Li\\u200b(e\\u200b(ci,Îµ))\\n'\n",
      " '\\n'\n",
      " 'ì—¬ê¸°ì„œ ÎµëŠ” ë¹ˆ ì‹œí€€ìŠ¤ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Li+ëŠ” API í˜¸ì¶œê³¼ í•´ë‹¹ ê²°ê³¼ë¥¼ ëª¨ë¸ Mì— ì ‘ë‘ì–´ë¡œ ì œê³µí–ˆì„ ë•Œì˜ ì†ì‹¤ì„ ë‚˜íƒ€ë‚´ë©°, Li-ëŠ” '\n",
      " 'API í˜¸ì¶œì„ í•˜ì§€ ì•Šê±°ë‚˜ ê²°ê³¼ë¥¼ ì œê³µí•˜ì§€ ì•Šì•˜ì„ ë•Œì˜ ì†ì‹¤ ì¤‘ ìµœì†Ÿê°’ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " 'ë§ˆì§€ë§‰ìœ¼ë¡œ, í•„í„°ë§ ì„ê³„ê°’ Ï„fì— ëŒ€í•´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” API í˜¸ì¶œë§Œ ìœ ì§€ë©ë‹ˆë‹¤:\\n'\n",
      " '\\n'\n",
      " 'Li--Li+ â‰¥ Ï„f\\n'\n",
      " '\\n'\n",
      " 'ì¦‰, API í˜¸ì¶œê³¼ í•´ë‹¹ ê²°ê³¼ë¥¼ ì œê³µí–ˆì„ ë•Œì˜ ì†ì‹¤ì´ API í˜¸ì¶œì„ í•˜ì§€ ì•Šì•˜ì„ ë•Œì˜ ì†ì‹¤ë³´ë‹¤ ì ì–´ë„ Ï„fë§Œí¼ ê°ì†Œí•´ì•¼ í•´ë‹¹ API '\n",
      " 'í˜¸ì¶œì„ ìœ ì§€í•©ë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': \"'Large Language Model Connected with Massive APIs'\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n",
      "title\n",
      "Gorilla: Large Language Model Connected with Massive APIs\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Related Work\n",
      "        Large Language Models\n",
      "        Tool Usage\n",
      "        LLMs for Program Synthesis\n",
      "  3 Methodology\n",
      "    3.1 Dataset Collection\n",
      "        API Documentation\n",
      "        Instruction Generation\n",
      "    3.2 Gorilla\n",
      "        API Call with Constraints\n",
      "        Retriever-Aware training\n",
      "        Gorilla Inference\n",
      "    3.3 Verifying APIs\n",
      "        AST Sub-Tree Matching\n",
      "  4 Evaluation\n",
      "        Baselines\n",
      "        Retrievers\n",
      "    4.1 AST Accuracy on API call\n",
      "        Finetuning without Retrieval\n",
      "        Finetuning with Retrieval\n",
      "        Hallucination with LLM\n",
      "    4.2 Test-Time Documentation Change\n",
      "    4.3 API Call with Constraints\n",
      "  5 Conclusion\n",
      "  6 Limitations & Social Impacts\n",
      "  7 Acknowledgement\n",
      "  References\n",
      "  8 Appendix\n",
      "    8.1 Dataset Details\n",
      "        Domain Classification\n",
      "        API Call Task\n",
      "        API Provider Component\n",
      "        Explanation Element\n",
      "        Code\n",
      "    8.2 Gorilla Details\n",
      "        Data\n",
      "        Training\n",
      "    8.3 Performance Comparison\n",
      "      8.3.1 Evaluation\n",
      "      8.3.2 Hallucination\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': \"'Large Language Model Connected with Massive APIs'\", 'sections': ['Methodology']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3 Methodology\n",
      "Figure 3: Gorilla: A system for enabling LLMs to interact with APIs. The upper half represents the training procedure as described in SecÂ 3. This is the most exhaustive API data-set for ML to the best of our knowledge. During inference (lower half), Gorilla supports two modes - with retrieval, and zero-shot. In this example, it is able to suggest the right API call for generating the image from the userâ€™s natural language query.\n",
      "\n",
      "In this section, we describe APIBench, a comprehensive benchmark constructed from TorchHub, TensorHub, and HuggingFace API Model Cards. We begin by outlining the process of collecting the API dataset and how we generated instruction-answer pairs. We then introduce Gorilla, a novel training paradigm with a informationâ€“retriever incorporated into the training and inference pipelines. Finally, we present our AST tree matching evaluation metric.\n",
      "\n",
      "3.1 Dataset Collection\n",
      "\n",
      "To collect the dataset, we meticulously recorded all online model cards for HuggingFaceâ€™s â€œThe Model Hubâ€, PyTorch Hub, and TensorFlow Hub Models. Throughout the rest of the paper, we call these HuggingFace, Torch Hub, and TensorFlow Hub respectively for brevity.\n",
      "\n",
      "API Documentation\n",
      "\n",
      "The HuggingFace platform hosts and servers about 203,681 models. However, many of them have poor documentation, lack dependencies, have no information in their model card, etc. To filter these out, we pick the top 20 models from each domain.\n",
      "We consider 7 domains in multimodal data, 8 in CV, 12 in NLP, 5 in Audio, 2 in tabular data, and 2 in reinforcement learning. Post filtering, we got a total of 925 models from HuggingFace. TensorFlow Hub is versioned into v1 and v2. The latest version (v2) has 801 models in total, and we process all of them. Post filtering out models, whose mode cards had little to no information, we are left with 626 models. Similar to TensorFlow Hub, we get 95 models from Torch Hub.\n",
      "We then converted the model cards for each of these 1,645 API calls into a json object with the following fields: {domain, framework, functionality, api_name, api_call, api_arguments, environment_requirements, example_code, performance, and description.}. We provide more information in the Appendix. These fields were chose to generalize beyond the API calls within ML domain, to other domains, includin RESTful API calls.\n",
      "\n",
      "Instruction Generation\n",
      "\n",
      "Guided by the self-instruct paradigmÂ [42], we employed GPT-4 to generate synthetic instruction data.\n",
      "We provided three in-context examples, along with a reference API documentation, and tasked the model with generating real-world use cases that call upon the API. We specifically instructed the model to refrain from using any API names or hints when creating instructions. We constructed six examples (Instruction-API pairs) for each of the three model hubs. These 18 points, were the only hand-generated or modified data. For each of our 1,645 API datapoints, we sample 3 of 6 corresponding instruction examples to generate a total of 10 instruction-api pairs as demonstrated in FigureÂ 3. We would like to highlight that we only need to employ GPT-4 to generate the instructions and this can be swapped with open-source alternatives such as LLaMA, Alpaca, etc.\n",
      "\n",
      "3.2 Gorilla\n",
      "\n",
      "Our model Gorilla, is retrieve-aware finetuned LLaMA-7B model, specifically for API calls. As shown in FigÂ 3, we employ self-instruct to generate {instruction, API} pairs. To fine-tune LLaMA, we convert this to a user-agent chat-style conversation, where each data-point is a conversation with one round each for the user and the agent. We then perform standard instruction finetuning on the base LLaMA-7B model. For our experiments, we train Gorilla with and without the retriever.\n",
      "\n",
      "API Call with Constraints\n",
      "\n",
      "API calls often come with inherent constraints. These constraints necessitate that the LLM not only comprehend the functionality of the API call but also categorize the calls according to different constraint parameters.\n",
      "This requirement introduces an additional layer of complexity to the process, demanding a more nuanced understanding from the LLM.\n",
      "Specifically, for machine learning API calls, two common sets of constraints are: parameter size and a lower bound on accuracy. Consider, for instance, the following prompt: â€˜â€˜Invoke an image classification model that uses less than 10M parameters, but maintains an ImageNet accuracy of at least 70%â€™â€™. Such a prompt presents a substantial challenge for the LLM to accurately interpret and respond to. Not only must the LLM understand the userâ€™s functional description, but it also needs to reason about the various constraints embedded within the request. This challenge underlines the intricate demands placed on LLMs in real-world API calls. It is not sufficient for the model to merely comprehend the basic functionality of an API call; it must also be capable of navigating the complex landscape of constraints that accompany such calls. These observations necessitate the need to fine-tune an LLM for APIs.\n",
      "\n",
      "Retriever-Aware training\n",
      "\n",
      "For training with retriever, the instruction-tuned dataset, also has an additional \"Use this API documentation for reference: <retrieved_API_doc_JSON>\" appended to the user prompt. Through this, we aim to teach the LLM to parse the second half of the question to answer the first half. We demonstrate that this a) makes the LLM adapt to test-time changes in API documentation, and b) improves performance from in-context learning, and finally c) show that it reduces hallucination error.\n",
      "\n",
      "Surprisingly, we find that augmenting a LLM with retrieval, does not always lead to improved performance, and can at-times hurt performance. We share more insights along with details in SecÂ 4.\n",
      "\n",
      "Gorilla Inference\n",
      "\n",
      "During Inference, the user provides the prompt in natural language (Fig:Â 3). This can be for a simple task (e.g, â€œI would like to identify the objects in an imageâ€), or they can specify a vague goal, (.e.g, â€œI am going to the zoo, and would like to track animalsâ€). Gorilla, similar to training, can be used for inference in two modes: zero-shot and with retrieval. In zero-shot, this prompt (with NO further prompt tuning) is fed to the Gorilla LLM model when then returns the API call that will help in accomplishing the task and/or goal. In retrieval mode, the retriever (either of BM25 or GPT-Index) first retrieves the most up-to-date API documentation stored in the API Database. This is then concatenated to the user prompt along with the message Use this API documentation for reference: before feeding it to Gorilla. The output of Gorilla is an API to be invoked. Besides the concatenation as described, we do NO further prompt tuning in our system. While we do have a system to execute these APIs, that is not a focus of this paper.\n",
      "\n",
      "3.3 Verifying APIs\n",
      "\n",
      "Inductive program synthesis, where a program is synthesized to satisfy test cases, has found success in several avenuesÂ [4, 25].\n",
      "However, test cases fall short when evaluating API calls, as it is often hard to verify the semantic correctness of the code. For example, consider the task of classifying an image.\n",
      "There are over 40 different models that can be used for the task. Even if we were to narrow down to a single family of Densenet, there are four different configurations possible. Hence, there exist multiple correct answers and it is hard to tell if the API being used is functionally equivalent to the reference API by unit tests. Thus, to evaluate the performance of our model, we compare their functional equivalence using the dataset we collected. To trace which API in the dataset is the LLM calling, we adopt the AST tree-matching strategy. Since we only consider one API call in this paper, checking if the AST of the candidate API call is a sub-tree of the reference API call reveals which API is being used in the dataset.\n",
      "\n",
      "Identifying and even defining hallucinations can be challenging.\n",
      "We use the AST matching process to directly identify the hallucinations.\n",
      "We define a hallucination as an API call that is not a sub-tree of any API in the database â€“ invoking an entirely imagined tool.\n",
      "This form of hallucination is distinct from invoking an API incorrectly which we instead define as an error.\n",
      "\n",
      "AST Sub-Tree Matching\n",
      "\n",
      "We perform AST sub-tree matching to identify which API in our dataset is the LLM calling. Since each API call can have many arguments, we need to match on each of these arguments. Further, since, Python allows for default arguments, for each API, we define which arguments to match in our database. For example, we check repo_or_dir and model arguments in our function call. In this way, we can easily check if the argument matches the reference API or not. Please refer to Fig.Â 4 for more details. In this example, Gorilla returns a torch API call. We first build the tree, and verify that it matches a subtree in our dataset along nodes torch.hub.load, pytorch/vision, and densenet121. But, we donâ€™t check for match along leaf node pretrained = True since that is an optional python argument.\n",
      "\n",
      "Figure 4: AST Sub-Tree Matching to evaluate API calls. On the left is an API call returned by Gorilla. We first build the associated API tree. We then compare this to our dataset, to see if the API dataset has a subtree match. In the above example, the matching subtree is highlighted in brown, signifying that the API call is indeed correct. Pretrained=True is an optional argument.\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m'Large Language Model Connected with Massive APIs' ë…¼ë¬¸ì˜ Methodology ì„¹ì…˜ì€ ë‹¤ìŒê³¼ ê°™ì´ ìš”ì•½ë©ë‹ˆë‹¤:\n",
      "\n",
      "- **Dataset Collection**: APIBenchëŠ” TorchHub, TensorHub ë° HuggingFace API Model Cardsì—ì„œ êµ¬ì„±ëœ í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. API ë°ì´í„°ì…‹ ìˆ˜ì§‘ ë° ëª…ë ¹-ì‘ë‹µ ìŒ ìƒì„± ê³¼ì •ì„ ê°œìš”í•˜ë©°, API ë¬¸ì„œ ë° ëª…ë ¹ ìƒì„±ì— ëŒ€í•œ ì„¸ë¶€ ë‚´ìš©ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "- **API Documentation**: HuggingFace í”Œë«í¼ì—ëŠ” ì•½ 203,681ê°œì˜ ëª¨ë¸ì´ í˜¸ìŠ¤íŒ…ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ ì¼ë¶€ëŠ” ë¬¸ì„œí™”ê°€ ë¯¸í¡í•˜ê±°ë‚˜ ì˜ì¡´ì„±ì´ ì—†ê±°ë‚˜ ëª¨ë¸ ì¹´ë“œì— ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê±¸ëŸ¬ë‚´ê¸° ìœ„í•´ ê° ë„ë©”ì¸ì˜ ìƒìœ„ 20ê°œ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´í›„, Torch Hub, TensorFlow Hubì—ì„œë„ ìœ ì‚¬í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì—¬ ì´ 1,645ê°œì˜ API í˜¸ì¶œì„ ì–»ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- **Instruction Generation**: GPT-4ë¥¼ í™œìš©í•˜ì—¬ í•©ì„± ì§€ì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ëª¨ë¸ ì¹´ë“œë¥¼ JSON ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ API í˜¸ì¶œì— ëŒ€í•œ ì¼ë°˜í™”ëœ í•„ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ API í˜¸ì¶œì— ëŒ€í•œ ì§€ì‹œë¥¼ ìƒì„±í•˜ê³  API í˜¸ì¶œì— ëŒ€í•œ í•™ìŠµ ë°ì´í„°ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.\n",
      "\n",
      "- **Gorilla**: GorillaëŠ” API í˜¸ì¶œì„ ìœ„í•´ íŠ¹ë³„íˆ ì¡°ì •ëœ LLaMA-7B ëª¨ë¸ë¡œ, ì§€ì‹œ-ì‘ë‹µ ìŒì„ ìƒì„±í•˜ê¸° ìœ„í•´ self-instructë¥¼ í™œìš©í•©ë‹ˆë‹¤. API í˜¸ì¶œì— ëŒ€í•œ ì œì•½ ì¡°ê±´ê³¼ í•¨ê»˜ API í˜¸ì¶œì„ ì²˜ë¦¬í•˜ëŠ” ë° í•„ìš”í•œ ì¶”ê°€ì ì¸ ë³µì¡ì„±ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
      "\n",
      "- **API Call with Constraints**: API í˜¸ì¶œì€ ì¢…ì¢… ë‚´ì¬ì ì¸ ì œì•½ ì¡°ê±´ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì œì•½ ì¡°ê±´ì€ LLMì´ API í˜¸ì¶œì˜ ê¸°ëŠ¥ì„ ì´í•´í•˜ëŠ” ê²ƒë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ì–‘í•œ ì œì•½ ì¡°ê±´ ë§¤ê°œë³€ìˆ˜ì— ë”°ë¼ í˜¸ì¶œì„ ë¶„ë¥˜í•´ì•¼ í•¨ì„ ìš”êµ¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "- **Retriever-Aware Training**: ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•œ í›ˆë ¨ì„ ìœ„í•´ ì§€ì‹œ ì¡°ì •ëœ ë°ì´í„°ì…‹ì— \"ì´ API ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤: <ê²€ìƒ‰ëœ_API_ë¬¸ì„œ_JSON>\"ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLMì´ API ë¬¸ì„œì˜ ë³€ê²½ì— ì ì‘í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, í™˜ìƒ ì˜¤ë¥˜ë¥¼ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
      "\n",
      "- **Gorilla Inference**: ì¶”ë¡  ì¤‘ì— ì‚¬ìš©ìëŠ” ìì—°ì–´ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. GorillaëŠ” í›ˆë ¨ê³¼ ìœ ì‚¬í•˜ê²Œ ì¶”ë¡ ì„ ìœ„í•´ ë‘ ê°€ì§€ ëª¨ë“œë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤: zero-shot ë° ê²€ìƒ‰ê³¼ í•¨ê»˜. ì´ë¥¼ í†µí•´ GorillaëŠ” ì‘ì—… ë°/ë˜ëŠ” ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” API í˜¸ì¶œì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "- **Verifying APIs**: API í˜¸ì¶œì„ í‰ê°€í•˜ê¸° ìœ„í•´ AST ì„œë¸Œ íŠ¸ë¦¬ ì¼ì¹˜ ë©”íŠ¸ë¦­ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ API ë°ì´í„°ì…‹ì—ì„œ LLMì´ í˜¸ì¶œí•˜ëŠ” APIë¥¼ ì¶”ì í•˜ê³ , í™˜ìƒì„ ì‹ë³„í•˜ê¸° ìœ„í•´ AST ì¼ì¹˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ë¥¼ í†µí•´ 'Large Language Model Connected with Massive APIs' ë…¼ë¬¸ì˜ Methodology ì„¹ì…˜ì— ëŒ€í•œ ìš”ì•½ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# output = agent_executor.invoke({\"input\": \"summary the method of the paper 'Large Language Model Connected with Massive APIs' í•œê¸€ë¡œ ë§í•´ì¤˜\"})\n",
    "output = agent_executor.invoke({\"input\": \"'Large Language Model Connected with Massive APIs'ë…¼ë¬¸ì˜ methodë¥¼ ìš”ì•½í•´ ì„¤ëª…í•´ì¤˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'Large Language Model Connected with Massive APIs' ë…¼ë¬¸ì˜ Methodology ì„¹ì…˜ì€ ë‹¤ìŒê³¼ \"\n",
      " 'ê°™ì´ ìš”ì•½ë©ë‹ˆë‹¤:\\n'\n",
      " '\\n'\n",
      " '- **Dataset Collection**: APIBenchëŠ” TorchHub, TensorHub ë° HuggingFace API '\n",
      " 'Model Cardsì—ì„œ êµ¬ì„±ëœ í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. API ë°ì´í„°ì…‹ ìˆ˜ì§‘ ë° ëª…ë ¹-ì‘ë‹µ ìŒ ìƒì„± ê³¼ì •ì„ ê°œìš”í•˜ë©°, API '\n",
      " 'ë¬¸ì„œ ë° ëª…ë ¹ ìƒì„±ì— ëŒ€í•œ ì„¸ë¶€ ë‚´ìš©ì„ ì œê³µí•©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '- **API Documentation**: HuggingFace í”Œë«í¼ì—ëŠ” ì•½ 203,681ê°œì˜ ëª¨ë¸ì´ í˜¸ìŠ¤íŒ…ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ '\n",
      " 'ì¼ë¶€ëŠ” ë¬¸ì„œí™”ê°€ ë¯¸í¡í•˜ê±°ë‚˜ ì˜ì¡´ì„±ì´ ì—†ê±°ë‚˜ ëª¨ë¸ ì¹´ë“œì— ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê±¸ëŸ¬ë‚´ê¸° ìœ„í•´ ê° ë„ë©”ì¸ì˜ ìƒìœ„ 20ê°œ ëª¨ë¸ì„ '\n",
      " 'ì„ íƒí•©ë‹ˆë‹¤. ì´í›„, Torch Hub, TensorFlow Hubì—ì„œë„ ìœ ì‚¬í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì—¬ ì´ 1,645ê°œì˜ API í˜¸ì¶œì„ '\n",
      " 'ì–»ìŠµë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '- **Instruction Generation**: GPT-4ë¥¼ í™œìš©í•˜ì—¬ í•©ì„± ì§€ì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ëª¨ë¸ ì¹´ë“œë¥¼ JSON ê°ì²´ë¡œ '\n",
      " 'ë³€í™˜í•˜ì—¬ API í˜¸ì¶œì— ëŒ€í•œ ì¼ë°˜í™”ëœ í•„ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ API í˜¸ì¶œì— ëŒ€í•œ ì§€ì‹œë¥¼ ìƒì„±í•˜ê³  API í˜¸ì¶œì— ëŒ€í•œ í•™ìŠµ ë°ì´í„°ë¥¼ '\n",
      " 'í™•ë³´í•©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '- **Gorilla**: GorillaëŠ” API í˜¸ì¶œì„ ìœ„í•´ íŠ¹ë³„íˆ ì¡°ì •ëœ LLaMA-7B ëª¨ë¸ë¡œ, ì§€ì‹œ-ì‘ë‹µ ìŒì„ ìƒì„±í•˜ê¸° ìœ„í•´ '\n",
      " 'self-instructë¥¼ í™œìš©í•©ë‹ˆë‹¤. API í˜¸ì¶œì— ëŒ€í•œ ì œì•½ ì¡°ê±´ê³¼ í•¨ê»˜ API í˜¸ì¶œì„ ì²˜ë¦¬í•˜ëŠ” ë° í•„ìš”í•œ ì¶”ê°€ì ì¸ ë³µì¡ì„±ì„ '\n",
      " 'ì†Œê°œí•©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '- **API Call with Constraints**: API í˜¸ì¶œì€ ì¢…ì¢… ë‚´ì¬ì ì¸ ì œì•½ ì¡°ê±´ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì œì•½ ì¡°ê±´ì€ '\n",
      " 'LLMì´ API í˜¸ì¶œì˜ ê¸°ëŠ¥ì„ ì´í•´í•˜ëŠ” ê²ƒë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ì–‘í•œ ì œì•½ ì¡°ê±´ ë§¤ê°œë³€ìˆ˜ì— ë”°ë¼ í˜¸ì¶œì„ ë¶„ë¥˜í•´ì•¼ í•¨ì„ ìš”êµ¬í•©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '- **Retriever-Aware Training**: ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•œ í›ˆë ¨ì„ ìœ„í•´ ì§€ì‹œ ì¡°ì •ëœ ë°ì´í„°ì…‹ì— \"ì´ API ë¬¸ì„œë¥¼ '\n",
      " 'ì°¸ì¡°í•˜ì‹­ì‹œì˜¤: <ê²€ìƒ‰ëœ_API_ë¬¸ì„œ_JSON>\"ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLMì´ API ë¬¸ì„œì˜ ë³€ê²½ì— ì ì‘í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, í™˜ìƒ '\n",
      " 'ì˜¤ë¥˜ë¥¼ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '- **Gorilla Inference**: ì¶”ë¡  ì¤‘ì— ì‚¬ìš©ìëŠ” ìì—°ì–´ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. GorillaëŠ” í›ˆë ¨ê³¼ ìœ ì‚¬í•˜ê²Œ ì¶”ë¡ ì„ '\n",
      " 'ìœ„í•´ ë‘ ê°€ì§€ ëª¨ë“œë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤: zero-shot ë° ê²€ìƒ‰ê³¼ í•¨ê»˜. ì´ë¥¼ í†µí•´ GorillaëŠ” ì‘ì—… ë°/ë˜ëŠ” ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” '\n",
      " 'ë° ë„ì›€ì´ ë˜ëŠ” API í˜¸ì¶œì„ ë°˜í™˜í•©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '- **Verifying APIs**: API í˜¸ì¶œì„ í‰ê°€í•˜ê¸° ìœ„í•´ AST ì„œë¸Œ íŠ¸ë¦¬ ì¼ì¹˜ ë©”íŠ¸ë¦­ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ API '\n",
      " 'ë°ì´í„°ì…‹ì—ì„œ LLMì´ í˜¸ì¶œí•˜ëŠ” APIë¥¼ ì¶”ì í•˜ê³ , í™˜ìƒì„ ì‹ë³„í•˜ê¸° ìœ„í•´ AST ì¼ì¹˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " \"ì´ë¥¼ í†µí•´ 'Large Language Model Connected with Massive APIs' ë…¼ë¬¸ì˜ Methodology ì„¹ì…˜ì— \"\n",
      " 'ëŒ€í•œ ìš”ì•½ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recompendpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'Facilitating Large Language Models to Master 16000+ Real-world APIs', 'type': 'reference'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m([{'paperId': '0bfc804e31eecfd77f45e4ee7f4d629fffdcd628', 'title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'abstract': 'Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.'}, {'paperId': 'b4798b374f5064476f838545f75569c22e682a03', 'title': 'Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia', 'abstract': 'Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act\"reasonably\", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data.'}], [{'paperId': '99832586d55f540f603637e458a292406a0ed75d', 'title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'abstract': 'While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io', 'influentialCitationCount': 184, 'publicationDate': '2022-10-06', 'intent': 'background', 'context': 'ReACT (Yao et al., 2022) proposes to better integrate reasoning with acting by allowing LLMs to give a proper reason for an action and incorporating environmental feedback for reasoning.'}, {'paperId': 'e65b346d442e9962a4276dc1c1af2956d9d5f1eb', 'title': 'Self-Instruct: Aligning Language Models with Self-Generated Instructions', 'abstract': 'Large â€œinstruction-tunedâ€ language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.', 'influentialCitationCount': 133, 'publicationDate': '2022-12-20', 'intent': 'None', 'context': 'None'}, {'paperId': '53d128ea815bcc0526856eb5a9c42cc977cb36a7', 'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools', 'abstract': 'Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.', 'influentialCitationCount': 69, 'publicationDate': '2023-02-09', 'intent': 'background', 'context': 'Tool Learning Recent studies have shed light on the burgeoning capabilities of LLMs in mastering tools and making decisions within complex environments (Qin et al., 2023b; Vemprala et al., 2023; Nakano et al., 2021; Qin et al., 2023a; Shen et al., 2023; Wu et al., 2023; Schick et al., 2023; Hao et al., 2023; Qian et al., 2023; Song et al., 2023).'}, {'paperId': '131f499e4d3503da93022d07fcf804a18483bea9', 'title': 'WizardLM: Empowering Large Language Models to Follow Complex Instructions', 'abstract': \"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM\", 'influentialCitationCount': 92, 'publicationDate': '2023-04-24', 'intent': 'None', 'context': 'None'}, {'paperId': 'd1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43', 'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face', 'abstract': 'Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.', 'influentialCitationCount': 48, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'None', 'context': 'None'}])\u001b[0m\u001b[32;1m\u001b[1;3mì—¬ê¸° ëª‡ ê°€ì§€ ì½ê¸° ì¢‹ì€ ë…¼ë¬¸ ì¶”ì²œì´ ìˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs**\n",
      "   - **ìš”ì•½**: ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ì‹¤ì œ APIë¥¼ ìŠµë“í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ëŠ” ToolLLMì„ ì†Œê°œí•©ë‹ˆë‹¤. ToolLLMì€ ë°ì´í„° êµ¬ì¶•, ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ë¥¼ í¬í•¨í•˜ëŠ” ì¼ë°˜ì ì¸ ë„êµ¬ ì‚¬ìš© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "   - **ë°œí–‰ì¼**: ë¯¸ìƒ\n",
      "\n",
      "2. **Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia**\n",
      "   - **ìš”ì•½**: ë³¸ ì—°êµ¬ëŠ” Concordiaë¥¼ ì†Œê°œí•˜ì—¬ ì‹ ì²´ì , ì‚¬íšŒì  ë˜ëŠ” ë””ì§€í„¸ ê³µê°„ì— ê·¼ê±°í•œ í–‰ë™ì„ í•˜ëŠ” ìƒì„±ì  ì—ì´ì „íŠ¸ ê¸°ë°˜ ëª¨ë¸ë§ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
      "   - **ë°œí–‰ì¼**: ë¯¸ìƒ\n",
      "\n",
      "3. **ReAct: Synergizing Reasoning and Acting in Language Models**\n",
      "   - **ìš”ì•½**: ì´ ë…¼ë¬¸ì€ LLMsê°€ ì¶”ë¡ ê³¼ í–‰ë™ì„ í†µí•©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•˜ë©°, ReActë¼ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
      "   - **ë°œí–‰ì¼**: 2022ë…„ 10ì›” 6ì¼\n",
      "\n",
      "4. **Self-Instruct: Aligning Language Models with Self-Generated Instructions**\n",
      "   - **ìš”ì•½**: Self-InstructëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì‹œ ë”°ë¥´ê¸° ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¡œ, ìì²´ ìƒì„±ëœ ì§€ì‹œì™€ ì¼ì¹˜ì‹œí‚´ìœ¼ë¡œì¨ ëª¨ë¸ì„ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.\n",
      "   - **ë°œí–‰ì¼**: 2022ë…„ 12ì›” 20ì¼\n",
      "\n",
      "5. **Toolformer: Language Models Can Teach Themselves to Use Tools**\n",
      "   - **ìš”ì•½**: ToolformerëŠ” LLMsê°€ ì™¸ë¶€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ìì²´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ë¡œ, ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ì—…ì—ì„œ íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n",
      "   - **ë°œí–‰ì¼**: 2023ë…„ 2ì›” 9ì¼\n",
      "\n",
      "ì´ ë…¼ë¬¸ë“¤ì„ í†µí•´ ë‹¤ì–‘í•œ ì£¼ì œì™€ ê¸°ìˆ ì  ì¸¡ë©´ì—ì„œ í¥ë¯¸ë¡œìš´ ë‚´ìš©ì„ ë°œê²¬í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_executor.invoke({\"input\": \"'Facilitating Large Language Models to Master 16000+ Real-world APIs' ë…¼ë¬¸ ì½ê¸° ì „ì— ì½ê¸° ì¢‹ì€ ë…¼ë¬¸ ì¶”ì²œí•´ì¤˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ì—¬ê¸° ëª‡ ê°€ì§€ ì½ê¸° ì¢‹ì€ ë…¼ë¬¸ ì¶”ì²œì´ ìˆìŠµë‹ˆë‹¤:\\n'\n",
      " '\\n'\n",
      " '1. **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world '\n",
      " 'APIs**\\n'\n",
      " '   - **ìš”ì•½**: ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ì‹¤ì œ APIë¥¼ ìŠµë“í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ëŠ” ToolLLMì„ ì†Œê°œí•©ë‹ˆë‹¤. ToolLLMì€ '\n",
      " 'ë°ì´í„° êµ¬ì¶•, ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ë¥¼ í¬í•¨í•˜ëŠ” ì¼ë°˜ì ì¸ ë„êµ¬ ì‚¬ìš© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\\n'\n",
      " '   - **ë°œí–‰ì¼**: ë¯¸ìƒ\\n'\n",
      " '\\n'\n",
      " '2. **Generative agent-based modeling with actions grounded in physical, '\n",
      " 'social, or digital space using Concordia**\\n'\n",
      " '   - **ìš”ì•½**: ë³¸ ì—°êµ¬ëŠ” Concordiaë¥¼ ì†Œê°œí•˜ì—¬ ì‹ ì²´ì , ì‚¬íšŒì  ë˜ëŠ” ë””ì§€í„¸ ê³µê°„ì— ê·¼ê±°í•œ í–‰ë™ì„ í•˜ëŠ” ìƒì„±ì  ì—ì´ì „íŠ¸ '\n",
      " 'ê¸°ë°˜ ëª¨ë¸ë§ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.\\n'\n",
      " '   - **ë°œí–‰ì¼**: ë¯¸ìƒ\\n'\n",
      " '\\n'\n",
      " '3. **ReAct: Synergizing Reasoning and Acting in Language Models**\\n'\n",
      " '   - **ìš”ì•½**: ì´ ë…¼ë¬¸ì€ LLMsê°€ ì¶”ë¡ ê³¼ í–‰ë™ì„ í†µí•©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•˜ë©°, ReActë¼ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.\\n'\n",
      " '   - **ë°œí–‰ì¼**: 2022ë…„ 10ì›” 6ì¼\\n'\n",
      " '\\n'\n",
      " '4. **Self-Instruct: Aligning Language Models with Self-Generated '\n",
      " 'Instructions**\\n'\n",
      " '   - **ìš”ì•½**: Self-InstructëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì‹œ ë”°ë¥´ê¸° ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¡œ, ìì²´ ìƒì„±ëœ '\n",
      " 'ì§€ì‹œì™€ ì¼ì¹˜ì‹œí‚´ìœ¼ë¡œì¨ ëª¨ë¸ì„ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.\\n'\n",
      " '   - **ë°œí–‰ì¼**: 2022ë…„ 12ì›” 20ì¼\\n'\n",
      " '\\n'\n",
      " '5. **Toolformer: Language Models Can Teach Themselves to Use Tools**\\n'\n",
      " '   - **ìš”ì•½**: ToolformerëŠ” LLMsê°€ ì™¸ë¶€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ìì²´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ë¡œ, ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ì—…ì—ì„œ '\n",
      " 'íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\\n'\n",
      " '   - **ë°œí–‰ì¼**: 2023ë…„ 2ì›” 9ì¼\\n'\n",
      " '\\n'\n",
      " 'ì´ ë…¼ë¬¸ë“¤ì„ í†µí•´ ë‹¤ì–‘í•œ ì£¼ì œì™€ ê¸°ìˆ ì  ì¸¡ë©´ì—ì„œ í¥ë¯¸ë¡œìš´ ë‚´ìš©ì„ ë°œê²¬í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'Facilitating Large Language Models to Master 16000+ Real-world APIs', 'type': 'citation'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'paperId': '19c222d1f18317d58cc85491f37479bc0dc49f41', 'title': 'API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs', 'abstract': \"Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.\", 'influentialCitationCount': 8, 'publicationDate': '2023-04-14'}, {'paperId': '455866ca838f356b53a7e3e5b344834f9e93dbbc', 'title': 'ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases', 'abstract': 'Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models.', 'influentialCitationCount': 15, 'publicationDate': '2023-06-08'}, {'paperId': '12b233752c7097ea6525622bed238ae2d2193c5a', 'title': 'MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback', 'abstract': \"To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation. Our analysis of 20 open- and closed-source LLMs offers intriguing findings. (a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural language feedback. (b) Better single-turn performance does not guarantee better multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.\", 'influentialCitationCount': 7, 'publicationDate': '2023-09-19'}, {'paperId': '67daf8c4fe1958d20ebdf95c2a36dd490c73836f', 'title': 'FireAct: Toward Language Agent Fine-tuning', 'abstract': 'Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.', 'influentialCitationCount': 7, 'publicationDate': '2023-10-09'}, {'paperId': '0725b276e351bba6b2a52ecb64f3c964b9acc2f9', 'title': 'ADaPT: As-Needed Decomposition and Planning with Language Models', 'abstract': 'Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.', 'influentialCitationCount': 6, 'publicationDate': '2023-11-08'}]\u001b[0m\u001b[32;1m\u001b[1;3më‹¤ìŒì— ì½ì„ ë…¼ë¬¸ìœ¼ë¡œ ì¶”ì²œí•´ë“œë¦´ ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs**\n",
      "   - ë°œí–‰ì¼: 2023ë…„ 4ì›” 14ì¼\n",
      "   - ì¸ìš© íšŸìˆ˜: 8íšŒ\n",
      "   - ì´ˆë¡: ìµœê·¼ ì—°êµ¬ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Large Language Models, LLMs)ì´ ì™¸ë¶€ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ API-Bankë¥¼ ì†Œê°œí•˜ë©°, ì´ëŠ” LLMsì—ê²Œ íŠ¹íˆ ì„¤ê³„ëœ ë„êµ¬-ë³´ì¡° LLMsë¥¼ ìœ„í•œ í˜ì‹ ì ì¸ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "2. **ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases**\n",
      "   - ë°œí–‰ì¼: 2023ë…„ 6ì›” 8ì¼\n",
      "   - ì¸ìš© íšŸìˆ˜: 15íšŒ\n",
      "   - ì´ˆë¡: ì´ ë…¼ë¬¸ì€ ToolAlpacaë¥¼ ì†Œê°œí•˜ë©°, ì´ëŠ” ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ì¼ë°˜í™”ëœ ë„êµ¬ ì‚¬ìš© ëŠ¥ë ¥ì„ í•™ìŠµí•˜ëŠ” í˜ì‹ ì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "3. **MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback**\n",
      "   - ë°œí–‰ì¼: 2023ë…„ 9ì›” 19ì¼\n",
      "   - ì¸ìš© íšŸìˆ˜: 7íšŒ\n",
      "   - ì´ˆë¡: ë³¸ ë…¼ë¬¸ì€ MINTë¥¼ ì†Œê°œí•˜ë©°, ì´ëŠ” LLMsì˜ ë‹¤ì¤‘ í„´ ìƒí˜¸ì‘ìš© ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "4. **FireAct: Toward Language Agent Fine-tuning**\n",
      "   - ë°œí–‰ì¼: 2023ë…„ 10ì›” 9ì¼\n",
      "   - ì¸ìš© íšŸìˆ˜: 7íšŒ\n",
      "   - ì´ˆë¡: ì´ ë…¼ë¬¸ì€ LMsë¥¼ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•˜ì—¬ ì–¸ì–´ ì—ì´ì „íŠ¸ë¥¼ ì–»ëŠ” ë°©í–¥ì— ëŒ€í•´ ì¡°ì‚¬í•˜ê³  ì£¼ì¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **ADaPT: As-Needed Decomposition and Planning with Language Models**\n",
      "   - ë°œí–‰ì¼: 2023ë…„ 11ì›” 8ì¼\n",
      "   - ì¸ìš© íšŸìˆ˜: 6íšŒ\n",
      "   - ì´ˆë¡: ë³¸ ë…¼ë¬¸ì€ ë³µì¡í•œ ì‘ì—…ì„ ìœ„í•œ As-Needed Decomposition ë° Planningì„ ì†Œê°œí•˜ë©°, ì´ëŠ” LLMì´ ì‹¤í–‰í•  ìˆ˜ ì—†ëŠ” í•˜ìœ„ ì‘ì—…ì„ ëª…ì‹œì ìœ¼ë¡œ ê³„íší•˜ê³  ë¶„í•´í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì–´ë–¤ ë…¼ë¬¸ì„ ì½ì–´ë³´ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_executor.invoke({\"input\": \"'Facilitating Large Language Models to Master 16000+ Real-world APIs' ë…¼ë¬¸ ì½ì–´ë´¤ëŠ”ë° ì¢‹ì€ê±° ê°™ì•„. ë‹¤ìŒì— ì½ì„ ë¹„ìŠ·í•œ ë…¼ë¬¸ ì¶”ì²œí•´ì¤˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ë‹¤ìŒì— ì½ì„ ë…¼ë¬¸ìœ¼ë¡œ ì¶”ì²œí•´ë“œë¦´ ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤:\\n'\n",
      " '\\n'\n",
      " '1. **API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs**\\n'\n",
      " '   - ë°œí–‰ì¼: 2023ë…„ 4ì›” 14ì¼\\n'\n",
      " '   - ì¸ìš© íšŸìˆ˜: 8íšŒ\\n'\n",
      " '   - ì´ˆë¡: ìµœê·¼ ì—°êµ¬ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Large Language Models, LLMs)ì´ ì™¸ë¶€ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ '\n",
      " 'ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ API-Bankë¥¼ ì†Œê°œí•˜ë©°, ì´ëŠ” LLMsì—ê²Œ íŠ¹íˆ ì„¤ê³„ëœ ë„êµ¬-ë³´ì¡° LLMsë¥¼ ìœ„í•œ í˜ì‹ ì ì¸ '\n",
      " 'ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '2. **ToolAlpaca: Generalized Tool Learning for Language Models with 3000 '\n",
      " 'Simulated Cases**\\n'\n",
      " '   - ë°œí–‰ì¼: 2023ë…„ 6ì›” 8ì¼\\n'\n",
      " '   - ì¸ìš© íšŸìˆ˜: 15íšŒ\\n'\n",
      " '   - ì´ˆë¡: ì´ ë…¼ë¬¸ì€ ToolAlpacaë¥¼ ì†Œê°œí•˜ë©°, ì´ëŠ” ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ì¼ë°˜í™”ëœ ë„êµ¬ ì‚¬ìš© ëŠ¥ë ¥ì„ í•™ìŠµí•˜ëŠ” í˜ì‹ ì ì¸ '\n",
      " 'í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '3. **MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language '\n",
      " 'Feedback**\\n'\n",
      " '   - ë°œí–‰ì¼: 2023ë…„ 9ì›” 19ì¼\\n'\n",
      " '   - ì¸ìš© íšŸìˆ˜: 7íšŒ\\n'\n",
      " '   - ì´ˆë¡: ë³¸ ë…¼ë¬¸ì€ MINTë¥¼ ì†Œê°œí•˜ë©°, ì´ëŠ” LLMsì˜ ë‹¤ì¤‘ í„´ ìƒí˜¸ì‘ìš© ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '4. **FireAct: Toward Language Agent Fine-tuning**\\n'\n",
      " '   - ë°œí–‰ì¼: 2023ë…„ 10ì›” 9ì¼\\n'\n",
      " '   - ì¸ìš© íšŸìˆ˜: 7íšŒ\\n'\n",
      " '   - ì´ˆë¡: ì´ ë…¼ë¬¸ì€ LMsë¥¼ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•˜ì—¬ ì–¸ì–´ ì—ì´ì „íŠ¸ë¥¼ ì–»ëŠ” ë°©í–¥ì— ëŒ€í•´ ì¡°ì‚¬í•˜ê³  ì£¼ì¥í•©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " '5. **ADaPT: As-Needed Decomposition and Planning with Language Models**\\n'\n",
      " '   - ë°œí–‰ì¼: 2023ë…„ 11ì›” 8ì¼\\n'\n",
      " '   - ì¸ìš© íšŸìˆ˜: 6íšŒ\\n'\n",
      " '   - ì´ˆë¡: ë³¸ ë…¼ë¬¸ì€ ë³µì¡í•œ ì‘ì—…ì„ ìœ„í•œ As-Needed Decomposition ë° Planningì„ ì†Œê°œí•˜ë©°, ì´ëŠ” LLMì´ '\n",
      " 'ì‹¤í–‰í•  ìˆ˜ ì—†ëŠ” í•˜ìœ„ ì‘ì—…ì„ ëª…ì‹œì ìœ¼ë¡œ ê³„íší•˜ê³  ë¶„í•´í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " 'ì–´ë–¤ ë…¼ë¬¸ì„ ì½ì–´ë³´ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?')\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadfigure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadfigure` with `{'query': 'StableToolBench', 'instruction': 'Pass Rate evaluation', 'start_page': 1}`\n",
      "\n",
      "\n",
      "\u001b[0m### Searched Paper ###\n",
      "Title: StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models\n",
      "Authors: ['Zhicheng Guo', 'Sijie Cheng', 'Hao Wang', 'Shihao Liang', 'Yujia Qin', 'Peng Li', 'Zhiyuan Liu', 'Maosong Sun', 'Yang Liu']\n",
      "Published: 2024-03-12\n",
      "{'paperId': '66eae99f971b8dc3b3fd83e335848d9c95594bb5', 'externalIds': {'DBLP': 'journals/corr/abs-2403-07714', 'ArXiv': '2403.07714', 'DOI': '10.48550/arXiv.2403.07714', 'CorpusId': 268363691}}\n",
      "ArXiv_id: 2403.07714\n",
      "File ./papers_db/2403.07714.pdf already exists. Skipping download.\n",
      "Page limit reached at 6\n",
      "Source files downloaded to: ./papers_db/2403.07714/2403.07714.tar.gz\n",
      "./papers_db/2403.07714/figs/pass_rate.pdf\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;200m\u001b[1;3mStableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models Zhicheng Guo1,2, Sijie Cheng1,2,3, Hao Wang4, Shihao Liang5, Yujia Qin1, Peng Li2,Zhiyuan Liu1,Maosong Sun1,Yang Liu1,2,6 1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China 2Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China 301.AI4Google5The University of Hong Kong 6Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China {guo-zc21 ,csj23 }@mails.tsinghua.edu.cn Abstract Large Language Models (LLMs) have wit- nessed remarkable advancements in recent years, prompting the exploration of tool learn- ing, which integrates LLMs with external tools to address diverse real-world challenges. As- sessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand- crafted online tools with limited scale, or large- scale real online APIs suffering from instability of API status. To address this problem, we in- troduce StableToolBench, a benchmark evolv- ing from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to allevi- ate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system. 1 Introduction With the rapid developments of Large Language Models (LLMs; Brown et al., 2020; Gemini Team, 2023; OpenAI, 2023; Touvron et al., 2023), tool learning which leverage LLMs to schedule a va- riety of external tools has attracted enormous at- tention (Nakano et al., 2022; Yao et al., 2022b; Lu et al., 2023). Previous studies (Hao et al., 2023; Hsieh et al., 2023; Schick et al., 2023; Tang et al., 2023) aim to augment LLMs with tools to enhance performance on conventional natural lan- guage processing (NLP) downstream tasks, while recent work (Qin et al., 2023a; Yao et al., 2022a; Cai et al., 2024) primarily focus on solving real- world scenarios that require the use of tools. In Project:zhichengg.github.io/stb.github.io/ GitHub:THUNLP-MT/StableToolBenchgeneral, tool learning complements the capabilities of vanilla LLMs and bridges the gap to real-world applications. To assess the capability of LLMs to use tools, a series of tool learning benchmarks have been in- troduced. Several pioneering studies have heavily relied on human-crafted offline tools (Yang et al., 2023b; Xu et al., 2023) or hand-selected online tools (Li et al., 2023b,a; Chen et al., 2023b). While these tools are high-quality, their scale remains rel- atively small, thereby limiting their ability to accu- rately reflect real-world scenarios. To address this limitation, subsequent studies (Tang et al., 2023; Ye et al., 2024; Qin et al., 2023b) have advocated for leveraging extensive collections of online tools that span across various domains. Owing to the increased scale, the automatic evaluation of tool learning has moved closer to real-world scenarios. However, concerns have been raised regarding the stability of these online tools, which has implica- tions for the reproducibility and comparability of benchmark performance over time1. For instance, the well-recognised ToolBench2(Qin et al., 2023c) has shown performance discrepancies that cannot be reproduced months after its release, as analysed in Section 2.1. This is even more important when faced with a complex environment, where APIs and tools keep changing while the evaluation should maintain its consistency across time. Existing large-scale benchmarks may struggle to provide stable evaluations for various reasons. We propose several hypotheses for this issue. Firstly, the complexity of tasks involving tool usage makes it challenging for the common automatic evaluator, gpt-3.5 , to function effectively as a discriminator. As discussed in Section 2.2, the evaluator cannot reliably determine whether a task is solvable or 1According to its definition, benchmarks should remain stable, and the model performance assessed on them must be comparable over time. 2We use ToolEval2 in ToolBench as the benchmark. 1arXiv:2403.07714v4 [cs.CL] 19 Jun 2024unsolvable, leading to variability in model perfor- mance due to this capability limitation. Secondly, the stability of API status for a significant portion of online tools (55.6% in ToolBench) is inconsis- tent. Users may be required to authorise the use of these tools or APIs, and tools provided by de- velopers may be accessible during the initial con- struction of the benchmark but become unavailable later. This fluctuation further undermines the reli- ability and reproducibility of model performance assessments over time. This situation results in a problem where the constructed queries in the benchmarks may no longer be completed with their originally referenced tools. Consequently, it is cru- cial to strike a balance between enhancing the sta- bility of these benchmarks and maintaining their diversity and scope. To address these issues, we propose a new bench- mark named StableToolBench, which incorporates a virtual API system and a stable evaluation system. We first build a virtual API system to replace the real one. As a start, we build a caching system to store the outputs of API calls. This approach ensures the stability and reproducibility of API be- haviours. Given the limited number of benchmark questions, our caching system can cover a signifi- cant number of API call scenarios. However, rely- ing solely on a cache is insufficient because many APIs remain unavailable. To resolve this problem, we use large language models (LLMs) to simulate the behaviours of these APIs. Specifically, we feed the documentation and few-shot real API calls if available in the cache to LLMs and ask LLMs to mock the behaviour of the APIs given a request. As a result, users can always get responses from APIs in an indistinguishable way as long as the LLMs are accessible. On the whole, our system first tries to find a hit in the cache. Unless there is a cache miss and a real API call is not received, the simulated server will be used. We then improve the evaluation system to make it more stable. We design two metrics (i.e., SoPR and SoWR) after judging solvable tasks and re- place all the automatic evaluators with GPT-4 to mitigate the randomness and indistinguishability during evaluation. Experiments demonstrate that our virtual API system, when combined with the improved evaluation system, can provide stable evaluation against API modifications. Furthermore, our system exhibits significant reliability in terms of realism, diversity, and documentation following accuracy. 0 10 20 30 40 50 Pass Rate(%)ChatGPT w/ CoTChatGPT w/ DFSToolLLaMA v2w/ CoTToolLLaMA v2w/ DFSMethod Reported ReproducedFigure 1: Comparison of performance (Pass Rate) re- ported in the paper and reproduced by us on the I1- Instruction group of ToolBench. The main contributions of our work are sum- marised as follows: â€¢A tool-learning benchmark featured a large number of cached stable simulated APIs, well- balancing stability and reality of the APIs and much more stable evaluation metrics. â€¢Extensive experiments show that our bench- mark provides much more stable model perfor- mance, robust to various types of API failures. â€¢Besides enhanced stability, our virtual API system exhibits reality, diversity and reliabil- ity comparable to that of the real API system. 2 Stability Analysis on ToolBench In this section, we initiate a comprehensive analysis to reveal the stability of established tool bench- marks, using Toolbench (Qin et al., 2023b) as a case study. We examine the stability of Tool- Bench by investigating three key dimensions: per- formance, evaluation, and API status. 2.1 Stability of Performance Benchmarks are designed to consistently evaluate the performance of various models over time. To test this consistency, we reproduce the model per- formances and record any variations. Our study employs Chain-of-Thought (CoT; Wei et al., 2023) and Depth First Search (DFS) strategies, leveraging ChatGPT and ToolLLaMA for comparative anal- ysis. We adhere strictly to the configurations de- tailed in ToolBench, utilising the ChatGPT version gpt-3.5-turbo-0613 andToolLLaMA-v2 . As de- picted in Figure 1, we compare the original Pass Rates for the I1-Instruction group reported by Tool- Bench with the Pass Rates we reproduced for four conventional methods. Our findings indicate a no- 2Task w/ Tools w/ Answer Unsolvable PassAnswer Solved?Task Solvable? Solved Unsolved/UnsureRandom GuessFailUnsure Answer Solved?Solvable Solved Unsolved UnsureFigure 2: Pass Rate evaluation in ToolBench paper. MethodTask AnswerPass WinS US UE S US UE CoT168 23 9 19 170 11 33.0 50.0 165 29 6 16 174 10 31.5 46.5 151 40 9 20 167 13 37.5 53.0 DFS116 68 16 17 167 16 50.5 54.0 122 59 19 20 162 18 46.5 48.0 132 54 14 22 157 21 55.0 56.0 Table 1: Experiments use GPT-3.5-Turbo-0613 with CoT and DFS. S, US, and UE indicate solvable (solved), unsolvable (unsolved), and unsure. Pass and Win de- note pass rate and win rate, respectively. Win rates are evaluated against the first run of CoT. This experiment is run on 4 Feb 2024. table decline in the performance of all methods over time, which raises concerns about the stability of ToolBench as a benchmark. 2.2 Stability of Evaluation In ToolBench, there are two types of metrics, in- cluding Pass Rate (PR) and Win Rate (WR). PR is calculated based on using gpt-3.5-turbo-16k to determine if a task is solvable and whether the generated answer can solve the corresponding task. Figure 2 details the computation process of PR. Specifically, a solvable task results in a pass if the answer is solved, a failure if unsolved, and is ran- domly determined if unsure. For tasks deemed unsure, a pass is assigned only if the answer is solved; otherwise, a random outcome is chosen. If a task is unsolvable, the result defaults to a pass regardless of the answer status. Moreover, WR is derived from the comparative PR of paired candi- dates. A candidateâ€™s WR increases by one each time it passes while the other fails. In all other situations, WR relies on gpt-3.5-turbo-16k to automatically evaluate the paired candidates. To assess the stability of evaluation, we perform both CoT and DFS using gpt-3.5-turbo-0613 Not Connectable: 14.8% Not Found: 3.5% Parameter Change: 3.6% Parsing Error: 25.9% Other: 1.4% Not Authorised: 6.4%Success: 44.4%Not Available Not Authorised SuccessFigure 3: Statistics of API changes. Parsing errors are caused by post-processing errors of local API documen- tation, which have been solved in our benchmark. on the I1-Instruction group dataset. These analyses are conducted using the provided tools and repeat over three iterations each. The resulting PR and WR are presented in Table 1, with detailed task and answer items. Despite PRs of DFS being gener- ally higher than CoT, the contribution of the task is larger than the answer. However, it is worth noting that the tasks are the same in both CoT and DFS, where their results are expected to be consistent. On the contrary, the discrimination of answers be- tween CoT and DFS is weak, where a considerable proportion are unsolved. Moreover, WR does not reflect the same trend as PR, where the second run of WR in DFS (48.0) is even lower than the first run of CoT. Therefore, all the phenomena reflect thatgpt-3.5-turbo-16k can not assume the role of the automatic evaluator in tool learning, which will be discussed in Section 4.6. 2.3 Stability of API Status We investigate the change of API status in Tool- Bench. In detail, we scan the original APIs down- loaded from ToolBench, and use gpt-4-turbo to automatically write calls via the prompts as shown in Appendix H. According to the keyword in API feedback, we classify these APIs into three cate- gories: success, not availability, and not authori- sation3. The API status and the detailed errors of not availability are presented in Figure 3. As can be seen, only 44.4% of API calls are successful, while other API calls are mostly not available with various errors and some are not authorised. Furthermore, to validate the impact of API call failures on the stability of model performance, we manually make some success tools4down by re- turning a special failure call. Specifically, we ran- 3Note that we use the toolbench-key provided in Tool- Bench to simulate the real running process in the benchmark. 4In ToolBench, a tool is composed of several APIs. For example, a database tool can have two APIs: a writing API and a reading API. 30% 10% 20% 50% Percentage of Failing Tools20253035404550Solvable Pass Rate 3.5 Turbo 0613 + CoT 3.5 Turbo 0613 + DFS 4 0613 + CoT 4 0613 + DFS Figure 4: Solvable Pass Rate (SoPR) change when man- ually making APIs down on the I1 Instruction group. domly sample a proportion of success tools con- taining success APIs found in Figure 3. At test- ing time, when sampled tools are called, a spe- cial response will be thrown: {â€œerrorâ€: â€œâ€, â€œresponseâ€: â€œThis API did not return any useful information...â€} to simulate the API call failures. We conduct baseline models with dif- ferent proportions (i.e., 0%, 10%, 20%, and 50%) of sampled APIs on the I1 Instruction set. Due to the issues in evaluation, we use our stable evalu- ation system proposed in Section 3.2 as the same as our main experiments. For each experiment, we evaluate three times and report the average scores as shown in Figure 4. It can be seen that the per- formance degrades a lot when the proportion of successful APIs is down, thus the impact of API status on stability is considerable. 3 StableToolBench Considering that stability is a crucial feature of benchmarking, in this paper, we specifically design a virtual API server and stable evaluation system to improve the stability based on ToolBench, and pro- pose a new benchmark, named StableToolBench. 3.1 Virtual API Server With real APIs, many of the failures encountered when reproducing its experiments are caused by ex- pired APIs, network issues, or failed authentication. To address this problem, we specifically propose a virtual API server with two components as illus- trated in Figure 5, including a caching system and API simulator. Moreover, we design API calling rules to combine these two components to ensure the virtual API server is stable.Caching System. We first implement a caching system that stores responses of all API callings to ensure consistency. The caching system uses keys composed of their category, tool, API name, and arguments. As a start, we populate the initial cache using the API call history from the training data and the reproduced test data released in Tool- Bench5. To ensure the quality of cached APIs, only valid records following the rule in Appendix D will be saved. It is worth noting that we will also reserve some APIs with exceptions to keep the reality. In this way, most API responses will be readily available, allowing the benchmark focus on probing the tool usage ability of designed methods with minimal impact on tool availability. Further- more, the API call in new experiments will also be continuously updated in the cache to ensure scala- bility. The statistics of cache are shown in Table 2. Additionally, as an extra benefit, this approach re- Source Train Set Test Set New Exp Total Before 58,105 5,921 255,828 352,630 After 25,995 2,393 136,592 164,980 Table 2: Cache components and their sizes before and after filtration. The cache of new experiments is updated until 12 Feb 2024. duces the latency introduced by interacting with real APIs, and also saves the costs for the API sim- ulator discussed below. API Simulator. Due to the limited coverage of the caching system, we propose to use LLMs to simulate API responses that are not in the cache and unavailable. Specifically, we ask gpt-4-turbo to simulate the API behaviour based on the original documentation in ToolBench. The API documenta- tion includes the descriptions of the functions and their corresponding parameters. To mitigate the dif- ference between simulated and real APIs, we use real API calls in the caching system as few-shot examples (Brown et al., 2020) for the LLM to bet- ter mock the behaviours. We keep the maximum number of examples at five. When less than five ex- amples exist in the cache, all of them will be used. Detailed prompts can be found in Appendix F. API Calling Rules. Based on the caching sys- tem and the API simulator, we create API calling rules to ensure the stability of the virtual API server. 5https://drive.google.com/drive/folders/ 1yBUQ732mPu-KclJnuQELEhtKakdXFc3J 4API Call: GetFlight(from: Beijing, to: Shanghai)Miss! Response: DA000 API SimulatorCaching System Error!Hit!Success!UpdateCaching System Imagine you are an API Server operating within a specialized tool, which contains a collection of distinct APIs. Your task is to craft a JSON formatted response that aligns with the expected output of the API. â€¦System PromptCall1: GetFlight(from: Beijing, to: London) Response: CA100 Call2: GetFlight(from: London, to: New York) Response: BA200Call3:GetFlight(from: Boston, to: Sydney). Response: DA300Few-shot Examples Description: This API is to get the flight from one city to another, â€¦Required Parameters: 1.from: â€¦ 2. to: â€¦Optional Parameters: â€¦API Documentation API SimulatorAPI CallResponseCall: GetFlight(from: Beijing, to: London), Category: â€¦BA888, CA999, â€¦Call: GetBook(name: XXX), ), Category: â€¦The book says â€¦.Call: BookFlight(Flight No: VA111), ), Category: â€¦Success! â€¦â€¦â€¦â€¦â€¦Figure 5: The process of calling APIs in our proposed virtual API server. +1 Task w/ ToolsGemini Pro GPT 4 Turbo Claude 2Task Solvable?Answer Solved? Majority V ote+0.5 +0Unsure UnsolvedSolvedPhase 1 Solvable Task w/ Tools w/ AnswerPhase 2 Figure 6: The process of our SoPR evaluation. When a call request (e,args) , where eis the API endpoint and args is the arguments for that end- point, is received, the system will first search the caching system for (e,args) pair. If a cache hit ex- ists, the cached response will be directly returned. When there is a cache miss, then the system will try to call the real API for a response to maintain the reality of the whole system. If the real API calling is successful, the response will be returned. How- ever, when the caching system does not contain and the real API is not available, the system will finally call the simulated API. The final response whether from the real API or the simulated API will be saved to update in the caching system. 3.2 Stable Evaluation System In this section, we propose a two-phase evalua- tion process, including judging solvable tasks and evaluating with two metrics, as shown in Figure 6. Moreover, we replace all the automatic evaluators withgpt-4-turbo . Judging Solvable Tasks. Considering that both unsolvable and unsure tasks would introduce enor- mous randomness while the results of tasks fluc- tuate wildly, we try to obtain a fixed collection of solvable tasks to eliminate the problems. ToI1-I I1-C I1-T I2-I I2-C I3-I Total Full 200 200 200 200 200 100 1,100 Solvable 163 153 158 106 124 61 765 Table 3: Statistics of original full and solvable tasks before and after judging. C,I,T stands for the Category, Instruction and Tool subgroup of the test set. Experi- ments below follow the denotation. achieve this, we use three state-of-the-art LLMs, i.e.,gpt-4-turbo ,gemini-pro , andclaude-2 , to determine whether a task is solvable or unsolv- able. The prompt is shown in Appendix G. The task will be judged as solvable when more than two models evaluate it as solvable. The statistics of solvable tasks across all test datasets in ToolBench are shown in Table 3. Metrics. We then report Solvable Pass Rate (SoPR) and Solvable Win Rate (SoWR) based on our obtained solvable tasks. Due to the limitation ofgpt-3.5-turbo-16k in tool learning, we uni- formly adopt gpt-4-turbo as the automatic eval- uator. SoPR is in essence PR with all tasks solv- able and only assesses the answers using the same prompt in ToolBench. The evaluator assigns out- comes of answers categorised as Solved, Unsolved, or Unsure, which respectively contribute scores of 1, 0.5, and 0 to the overall SoPR calculation. As for SoWR, when one is solved and the other is unsolved, the solved one wins. Under other cir- cumstances, gpt-4-turbo will be used to make a win-lose decision. 5\u001b[0m\u001b[32;1m\u001b[1;3mThe Pass Rate evaluation in the paper \"StableToolBench\" is explained with the help of a figure. \n",
      "\n",
      "The Pass Rate evaluation process involves judging solvable tasks and evaluating them with two metrics: Solvable Pass Rate (SoPR) and Solvable Win Rate (SoWR). The figure illustrates the process of evaluating solvable tasks with tools and answers, where tasks are categorized as solvable, unsolvable, or unsure. The evaluation is done using models like Gemini Pro, GPT-4 Turbo, and Claude 2 to determine the solvability of tasks. \n",
      "\n",
      "The SoPR metric assesses the pass rate of solvable tasks, where answers are categorized as Solved, Unsolved, or Unsure, contributing scores of 1, 0.5, and 0 respectively. The SoWR metric determines the win rate based on the outcomes of solved and unsolved tasks, with the solved task winning in case of a tie.\n",
      "\n",
      "This evaluation process ensures stability and reliability in benchmarking tool learning capabilities.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# output = agent_executor.invoke({\"input\": \"explain about the 'Figure 2: Pass Rate evaluation in ToolBench paper.' in the 3 page of the paper 'StableToolBench' í•œê¸€ë¡œ ë§í•´ì¤˜\"})\n",
    "output = agent_executor.invoke({\"input\": \"'StableToolBench'ì˜ Pass Rate evaluation ë¶€ë¶„ì„ ê·¸ë¦¼ê³¼ í•¨ê»˜ ì„¤ëª…í•´ì¤˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Pass Rate evaluation in the paper \"StableToolBench\" is explained with '\n",
      " 'the help of a figure. \\n'\n",
      " '\\n'\n",
      " 'The Pass Rate evaluation process involves judging solvable tasks and '\n",
      " 'evaluating them with two metrics: Solvable Pass Rate (SoPR) and Solvable Win '\n",
      " 'Rate (SoWR). The figure illustrates the process of evaluating solvable tasks '\n",
      " 'with tools and answers, where tasks are categorized as solvable, unsolvable, '\n",
      " 'or unsure. The evaluation is done using models like Gemini Pro, GPT-4 Turbo, '\n",
      " 'and Claude 2 to determine the solvability of tasks. \\n'\n",
      " '\\n'\n",
      " 'The SoPR metric assesses the pass rate of solvable tasks, where answers are '\n",
      " 'categorized as Solved, Unsolved, or Unsure, contributing scores of 1, 0.5, '\n",
      " 'and 0 respectively. The SoWR metric determines the win rate based on the '\n",
      " 'outcomes of solved and unsolved tasks, with the solved task winning in case '\n",
      " 'of a tie.\\n'\n",
      " '\\n'\n",
      " 'This evaluation process ensures stability and reliability in benchmarking '\n",
      " 'tool learning capabilities.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadfigure` with `{'query': 'StableToolBench', 'instruction': 'SoPR evaluation', 'start_page': 1}`\n",
      "\n",
      "\n",
      "\u001b[0m### Searched Paper ###\n",
      "Title: StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models\n",
      "Authors: ['Zhicheng Guo', 'Sijie Cheng', 'Hao Wang', 'Shihao Liang', 'Yujia Qin', 'Peng Li', 'Zhiyuan Liu', 'Maosong Sun', 'Yang Liu']\n",
      "Published: 2024-03-12\n",
      "{'paperId': '66eae99f971b8dc3b3fd83e335848d9c95594bb5', 'externalIds': {'DBLP': 'journals/corr/abs-2403-07714', 'ArXiv': '2403.07714', 'DOI': '10.48550/arXiv.2403.07714', 'CorpusId': 268363691}}\n",
      "ArXiv_id: 2403.07714\n",
      "File ./papers_db/2403.07714.pdf already exists. Skipping download.\n",
      "Page limit reached at 6\n",
      "Source files downloaded to: ./papers_db/2403.07714/2403.07714.tar.gz\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sopr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStableToolBench\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m ë…¼ë¬¸ì˜ SoPR evaluationê³¼ \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mToolBench\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m ë…¼ë¬¸ì—ì„œ ì†Œê°œë˜ëŠ” Pass Rate evaluationì„ ê·¸ë¦¼ì´ ìˆë‹¤ë©´ ë³´ì—¬ì£¼ë©´ì„œ ë¹„êµí•´ì¤˜\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/agents/agent.py:1433\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1433\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1442\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1443\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/agents/agent.py:1139\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1132\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1139\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1149\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/agents/agent.py:1224\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/agents/agent.py:1246\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[0;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/tools.py:452\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(observation, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/tools.py:409\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[1;32m    407\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    408\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 409\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, \u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    414\u001b[0m     )\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/tools.py:750\u001b[0m, in \u001b[0;36mStructuredTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[1;32m    742\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[1;32m    745\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    746\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    747\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    748\u001b[0m         )\n\u001b[1;32m    749\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[0;32m--> 750\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m     )\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/project/kubig19th-conference-llm/papers_llm/api_pool.py:495\u001b[0m, in \u001b[0;36mquery2figure_and_content\u001b[0;34m(query, instruction, start_page)\u001b[0m\n\u001b[1;32m    493\u001b[0m name \u001b[38;5;241m=\u001b[39m query_name_matching(instruction, pdf_files)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# figure display\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m \u001b[43mdisplay_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "File \u001b[0;32m~/Desktop/project/kubig19th-conference-llm/papers_llm/api_pool.py:459\u001b[0m, in \u001b[0;36mdisplay_figure\u001b[0;34m(pdf_files, name)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay_figure\u001b[39m(pdf_files, name):\n\u001b[1;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    <figure display>\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    nameê³¼ pdf_filesë¥¼ ë°›ì•„ figureë¥¼ display\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     pdf_path \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pdf_path)\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# PDF íŒŒì¼ ì—´ê¸°\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sopr'"
     ]
    }
   ],
   "source": [
    "output = agent_executor.invoke({\"input\": \"'StableToolBench' ë…¼ë¬¸ì˜ SoPR evaluationê³¼ 'ToolBench' ë…¼ë¬¸ì—ì„œ ì†Œê°œë˜ëŠ” Pass Rate evaluationì„ ê·¸ë¦¼ì´ ìˆë‹¤ë©´ ë³´ì—¬ì£¼ë©´ì„œ ë¹„êµí•´ì¤˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
