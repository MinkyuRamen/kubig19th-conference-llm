
\begin{table}[]
\centering
\caption{The achitecture and training data of all the models in our evaluation. The models are grouped by their architecture and training data.}
\label{tab:all_models}
\begin{adjustbox}{max width=\textwidth}

\setlength{\tabcolsep}{2pt}
\begin{tabular}{ccccccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{3}{c}{\textbf{Architecture}}                                                                             & \multicolumn{3}{c}{\textbf{Data}}                                                                                                                                                                                       \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\multicolumn{1}{c}{}                                & \multicolumn{1}{c}{\textbf{Family}} & \multicolumn{1}{c}{\textbf{Size}} & \multicolumn{1}{c}{\textbf{Max SS}} & \multicolumn{1}{c}{\textbf{\# Tokens}} & \multicolumn{1}{c}{\textbf{Pretraining}}                                                                                  & \multicolumn{1}{c}{\textbf{Finetuning}} \\
\midrule
\multicolumn{1}{c}{\textit{\textbf{Closed-source}}}          &                                     &                                   &                                     &                                        &                                                                                                                           &                                                    \\
text-davinci-003                                    & gpt3                                & 175b                              & 4096                                & -                                      & -                                                                                                                         & -                                                  \\
gpt-3.5-turbo                                       & gpt3                                & -                                 & 4096                                & -                                      & -                                                                                                                         & -                                                  \\
text-curie-001                                      & gpt3                                & 6.7b                              & 2048                                & -                                      & -                                                                                                                         & -                                                  \\
gpt4                                                & gpt4                                & -                                 & 8192                                & -                                      & -                                                                                                                         & -                                                  \\
\midrule
\multicolumn{1}{c}{\textit{\textbf{Open-source}}}           &                                     &                                   &                                     &                                        &                                                                                                                           &                                                    \\
bloomz                                              & bloom                               & 176b                              & 2048                                & 366B                                   & bloom corpus                                                                                            & xP3                                                \\
\cmidrule(lr){1-7}
llama-65b                                           & llama                               & 65b                               & 2048                                & 1.4T                                   & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}CCNet, C4, \\GitHub, Wikipedia, \\ Books, ArXiv, \\ Stack Exchange\end{tabular}} & -                                                  \\
llama-30b                                           & llama                               & 30b                               & 2048                                & 1.4T                                   &                                                                                                                           & -                                                  \\
llama-13b                                           & llama                               & 13b                               & 2048                                & 1.4T                                   &                                                                                                                           & -                                                  \\
llama-13b-alpaca                              & llama                               & 13b                               & 2048                                & 1.4T                                   &                                                                                                                           & GPT-4 responses, Alpaca                                                  \\
\cmidrule(lr){1-7}
starcoderbase                                        & bigcode                                & 15.5b                               & 8192                                &             1T                       &      \multirow{2}{*}{The Stack}                                                                                                                     &    -  \\
starcoder                                        & bigcode                                & 15.5b                               & 8192                                &             1T                       &                                                                                                                          &    The Stack (Python)  \\
\cmidrule(lr){1-7}
opt-30b                                         & opt                                 & 30b                               & 2048                                & 300B                                   & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}The Pile, BookCorpus, \\ CC-Stories, Reddit, \\ CCNewsV2\end{tabular}}         &             -         \\
opt-1.3b                                        & opt                                 & 1.3b                              & 2048                                & 300B                                   &                                                                                                                           &                                   -                 \\
opt-iml-30b                                             & opt                                 & 30b                               & 2048                                & 300B                                   &                                                                                                                           &                                  \multirow{2}{*}{OPT-IML Bench}                  \\
opt-iml-1.3b                                            & opt                                 & 1.3b                              & 2048                                & 300B                                   &                                                                                                                           &                                                    \\
\cmidrule(lr){1-7}
gpt-neox-20b                                            & neox                                & 20b                               & 2048                                & 450B                                   & \multirow{8}{*}{The Pile}                                                                                                 & -                                                  \\
GPT-NeoXT-Chat-Base-20B                             & neox                                & 20b                               & 2048                                &   460B                                     &                                                                                                                           & OpenChatKit IT          \\
codegen-16B-nl                                      & neox                                & 16b                               & 2048                                & 700B                                   &                                                                                                                           & -                                                  \\
codegen-16B-multi                                   & neox                                & 16b                               & 2048                                & 1T                                     &                                                                                                                           & BigQuery                                           \\
codegen-16B-mono                                    & neox                                & 16b                               & 2048                                & 1T                                     &                                                                                                                           & BigQuery, BigPython                               \\
% Cerebras-GPT-13B                                    & gpt2                                & 13b                               & 2048                                & 300B                                   &                                                                                                                           & -                                                  \\
pythia-12b                                          & neox                                & 12b                               & 2048                                & 300B                                   &                                                                                                                           & -                                                  \\
dolly-v2-12b                                        & neox                                & 12b                               & 2048                                & 300B                                   &                                                                                                                           & Dolly IT     \\
pythia-6.9b/2.8b1.4b                                          & neox                                & multi                               & 2048                                & 300B                                   &                                                                                                                           & -                                                  \\
\cmidrule(lr){1-7}
stablelm-base-alpha-7b                                        & neox                                & 7b                               & 4096                                &             800B                       &      \multirow{4}{*}{The Pile (1.5T)}                                                                                                                     &    -  \\
stablelm-base-alpha-3b                                        & neox                                & 3b                               & 4096                                &             800B                       &                                                                                                                          &    -  \\
stablelm-tuned-alpha-7b                                        & neox                                & 7b                               & 4096                                &             800B                       &                                                                                                                           & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Alpaca, GPT4All,\\Anthropic, Dolly, ShareGPT\end{tabular}}     \\
stablelm-tuned-alpha-3b                                        & neox                                & 3b                               & 4096                                &             800B                       &                                                                                                                           &      \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}