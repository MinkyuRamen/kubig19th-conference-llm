% \input{tables/all_tasks}
To evaluate open-source LLMs in the domain of tool manipulation, we curate a benchmark suite from both existing datasets and newly collected ones. This benchmark stands out as the first open-source test bench with predefined test cases for quantitative evaluation, distinguishing it from recent tool manipulation research using closed LLMs \cite{li2023api, qin2023tool}. In this section, we introduce the software tools and the evaluation infrastructure. We also demonstrate the level of challenges posed by each tool, in terms of the ability to generalize to unseen API combinations and the requirement for advanced reasoning.
% In~\Cref{sec:experiments} we validate the impact of our inspired techniques for tool manipulation on this benchmark.
% \comment{QT: lets give a name of the benchmark}
% \comment{QT, add some brief descriptions to tasks if possible}

\subsection{Software tools and evaluation infrastructure} 
As shown in~\Cref{tab:all_tasks}, our benchmark consists of five tasks we collected and three tasks derived from existing datasets, including VirtualHome\cite{puig2018virtualhome, huang2022language}, Webshop\cite{yao2023webshop} and Tabletop\cite{liang2022code}.
% These tasks ranges from querying the weather through web services~\cite{openweather} to shopping on e-commerce websites~\cite{yao2023webshop}. 
They cover both single-step and multiple-step action generation, which requires selecting and combining from $2$ to $108$ API functions to accomplish the goals. 
Each task consists of approximately approximately $100$ test cases, including goal descriptions and the ground truth API calls. We also provide a limited number of demonstration examples to aid model predictions\footnote{For WebShop, we find that more than $\mathcal{O}(n)$ demonstration examples can improve the success rate. Nonetheless, these examples can be acquired from programmatic software operations without heavy human curation.}. We include a comprehensive introduction and analysis of each task within the benchmark in \Cref{sec:benchmark_details}. 
% Note that, we prepare two versions of examples for webshop with significantly different lengths. The longer version of the examples are from their original datasets directly and we include them here to encourage LLMs to fully utilize all the examples. 
% However, none of the models we use in the experiments affords to put those examples in prompt given the limited sequence length. Thus we prepare another version of examples with much shorter content. 
% The shorter version is created by first removing 80\% of the non-targeted items from any webpage description, and selecting only the 200 shortest trajectories from the complete set.

We use \emph{success rate} as the primary evaluation metric for most tasks, except for the WebShop where we report rewards, as well as for VirtualHome where we use executability and Longest Common Subsequence (LCS), following the original metrics proposed by the respective authors. To facilitate evaluation, we build an infrastructure that executes the API calls generated by the action generators and assess the final outcome. This process enables reliable evaluation of tool manipulation capabilities without restricting the action generators to perfectly match the ground truth API calls. 



\subsection{Level of challenges}
\input{tables/all_tasks}

\begin{wraptable}[13]{r}{0.36\textwidth}
\centering
\vspace{-12pt}
\captionof{table}{A typical task of Google Sheets manipulation. It requires both selecting the correct API function and reasoning on the arguments.}
\label{tab:adv_reasoning}
% \vspace{-4pt}
\small
% \vspace{-0.45em}
\begin{adjustbox}{max width=\linewidth}
% \setlength{\tabcolsep}{3pt}
\begin{tabular}{ccc}
\toprule
Product                        & Cost                           & Price                          \\ 
\cmidrule(lr){1-3}
beef                           & 1                              & 3                              \\
pork                           & 5                              & 4                              \\
chicken                        & 10                             & 11                             \\
\midrule
\multicolumn{3}{l}{Task: Update beef's price to 10.}                                             \\
\multicolumn{3}{l}{\begin{tabular}[c]{@{}l@{}}Action:\\ \textbf{worksheet.update("C2", 10)}\end{tabular}} \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{wraptable}
% \vspace{-10pt}
To assess the level of challenge, we examine  \snact\  tasks based on their API complexity and the requirement for advanced reasoning.
% associated with each task in \snact, 
Intuitively, API complexity indicates the challenges in generalizing to unseen API combinations and non-default argument values. Challenges beyond API complexity then involve advanced reasoning.
% Beyond this, there is also another dimension of challenges that is harder to quantify. 
% We name it advanced reasoning, indicating the values of the arguments to fill in the API functions cannot be easily parsed from the natural language goals.

% To facilitate understanding on the challenge levels of our benchmark, we coarsely qualify each task along skills required beyond direct API selection and argument filling. Concretely we categorize along the requirement on generalization to unseen API combinations and advanced reasoning.
% the requirements on generalization to unseen API combinations and on advanced reasoning.

% \paragraph{Generalization to unseen API combinations} In our benchmark, we provide demonstration examples to in-context learning for tool manipulation. As we want to simulate a practical setting where a developer only curate minimal examples as human supervision, we only provide $\mathcal{O}(n)$ demonstrations\footnote{\comment{QT, please give a footnote why webshop have thousands of demonstrations but do still require minimal supervision}} for a software tool with $n$ API functions. In this setting, one key challenge for action generators is to generalize from demonstration examples to goals requiring unseen combination of APIs. 
% To quantify this generalization challenge, we first define the overlap between a demonstration example $e_{d} \in \mathcal{E}_d$ and a test example $e_{t}$ as $o(e_d, e_t) = \frac{|\mathcal{S}(e_t) \cap \mathcal{S}(e_d)|}{|\mathcal{S}(e_d) \cup \mathcal{S}(e_t)|}$ where $\mathcal{S}$ returns the set of APIs required by an example. Let $\tilde{o}(e_t, \mathcal{E}_d) = \max_{e\in\mathcal{E}_d} o(e, e_t)$ be the smallest overlap between $e_t$ and a demonstration example from $\mathcal{E}_d$. We consider $e_t$ requires an unseen API combination if $\tilde{o}(e_t, \mathcal{E}_d)$ is larger than $Z$. We then quantify the proportion of unseen test cases as the unseen rate and use it as a proxy of the challenge level.  
% As shown in~\Cref{tab:all_tasks}, we can observe that tasks in our benchmark demonstrate a wide range of unseen rate. We can observe $X$ to $Y$ unseen rate across five tools. For the rest of tools, Open Weather and Cat API tools have the unseen combination rate is $0$ because they both require single API call to accomplish the goal in test cases while Webshop always requires repeatedly triggering two APIs in a multi-step fashion.

\paragraph{API Complexity} 
% In \snact, we provide a set of API functions as well as a set of demonstration examples for LLM to do in-context learning on manipulating a given tool. One key challenge for action generators is to generalize from demonstration examples to goals requiring unseen combination of APIs as well as their arguments filling patterns. 
To quantify the challenge in generalizing to unseen API combinations, we develop a task-agnostic complexity score $S\in \mathbb{R}_0^+$, where 
\begin{equation}
    S(\mathcal{T}, \mathcal{X}, \mathcal{D}) = \mathbb{E}_{t \in \mathcal{T}}\min\nolimits_{e \in \mathcal{X}} d(t, e).
\end{equation}
It averages over all the test samples in the test set $\mathcal{T}$ on the minimum distance between $t$ and any demonstration example $e$ from the example pool $\mathcal{X}$.
In particular, the distance $d(t, e)$ between each test sample $t$ and a demonstration example $e$ is negatively proportional to the probability of transforming the API combination of $e$ to match that of $t$, by randomly dropping the API functions irrelevant to $t$ and inserting the uncovered API functions required by $t$ from the API pool $\mathcal{D}$.
% In particular, the distance between each test sample and a demonstration example $d(t, e)$ is negatively proportional to the probability of correctly generating the API combination of $t$ by randomly dropping the unnecessary API functions from a demonstration $e$ and inserting the uncovered API functions from the pool $\mathcal{D}$.
We refer to the details of the complexity score to \Cref{sec:app_complexity_metrics} and list their values in \Cref{tab:all_tasks}.
The score is non-negative and the higher the score is, the more complex a task is. 
Despite the fact that this complexity score reflects the challenge level of API selection, it does not capture all the difficulties of a task. A task with low complexity score can still be very challenging as it might require advanced reasoning. 
For instance, even though Webshop is challenging, the API selection complexity of it is zero. This is because there are only two API functions requiring only one argument each in Webshop, and they are both covered by the examples, so there is no API selection complexity.
% ; while the scores are higher for the tasks that require selecting multiple API calls from a large pool of API functions, like Trip booking and VirtualHome. 




\paragraph{Advanced reasoning} 
% We define advanced reasoning as skills beyond directly selecting API functions based on the goal and extracting some text in goals as API argument value. 
Within our benchmark, advanced reasoning encompasses challenges beyond generalizing to unseen API combinations. These challenges include non API-based coding for tasks such as Google Sheets and Tabletop, as well as decision-making based on observations returned from the WebShop environment. 
For instance, in the Google Sheets example shown in \Cref{tab:adv_reasoning}, the coordinate of the beef price's cell ("C2") cannot be easily derived from either the goal or the table itself. The action generator needs to understand the content or write additional python code to derive this coordinate before calling the API function.  
In the similar scenario, WebShop task requires the action generator to extract the exact button ID to click on the webpage given the description. 
These challenges, categorized as advanced reasoning, complement the API complexity category.
% These challenges are hard to quantify in a unified fashion, so we exclude them from the score $S$ above. But they are complementary to the API selection complexity.



% All the tasks we curated are listed in Table \ref{tab:all_tasks}. They cover the experiments 
% - We don't include the questions that cannot lead to any evecutable answers in the test set, because those questions can be easily filtered out with some classifiers in practice. We want to focus on evaluating the real action generation capability of the models, rather than this classification capability. 
% - PRINCIPLE: Let a new NLP researcher understand what's happening here!
% When introducing each task, we need to describe the following
% 1. What this task is for
% 2. How the test set is created
% 3. Difficulty distribution
% 4. What metrics to eval

% 1. Tool and task composition: 2 x 2, how many API
% 2. Task description
% 3. Evaluation methodology: will only talk about success rate for simplicity, others goes into appendix
% 4. Demonstration and alignment data: 
%    a. Statistics to demonstrate that there is enough unseen combinations of APIs in the test case vs the demonstraiton examples and alignment data. 
%       - For this, I will add a row in table 1.
%    b. Numbers to tell the total developer time on curating demonstration examples and algiment data is low. 
%      - This doesn't belong to this section I think, we can put them into the experiments. 



