\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alain et~al.(2016)Alain, Bengio, Yao, Yosinski, Thibodeau-Laufer,
  Zhang, and Vincent]{alain2016gsns}
Guillaume Alain, Yoshua Bengio, Li~Yao, Jason Yosinski, Eric Thibodeau-Laufer,
  Saizheng Zhang, and Pascal Vincent.
\newblock {GSNs}: generative stochastic networks.
\newblock \emph{Information and Inference: A Journal of the IMA}, 5\penalty0
  (2):\penalty0 210--249, 2016.

\bibitem[Bordes et~al.(2017)Bordes, Honari, and Vincent]{bordes2016learning}
Florian Bordes, Sina Honari, and Pascal Vincent.
\newblock Learning to generate samples from noise through infusion training.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Che et~al.(2020)Che, Zhang, Sohl-Dickstein, Larochelle, Paull, Cao,
  and Bengio]{che2020your}
Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull,
  Yuan Cao, and Yoshua Bengio.
\newblock Your {GAN} is secretly an energy-based model and you should use
  discriminator driven latent sampling.
\newblock \emph{arXiv preprint arXiv:2003.06060}, 2020.

\bibitem[Chen et~al.(2018{\natexlab{a}})Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6571--6583, 2018{\natexlab{a}}.

\bibitem[Chen et~al.(2018{\natexlab{b}})Chen, Mishra, Rohaninejad, and
  Abbeel]{chen2018pixelsnail}
Xi~Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel.
\newblock Pixel{SNAIL}: An improved autoregressive generative model.
\newblock In \emph{International Conference on Machine Learning}, pages
  863--871, 2018{\natexlab{b}}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Deng et~al.(2020)Deng, Bakhtin, Ott, Szlam, and
  Ranzato]{deng2020residual}
Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato.
\newblock Residual energy-based models for text generation.
\newblock \emph{arXiv preprint arXiv:2004.11714}, 2020.

\bibitem[Dinh et~al.(2014)Dinh, Krueger, and Bengio]{dinh2014nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock {NICE}: Non-linear independent components estimation.
\newblock \emph{arXiv preprint arXiv:1410.8516}, 2014.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using {R}eal {NVP}.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Du and Mordatch(2019)]{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and modeling with energy based models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3603--3613, 2019.

\bibitem[Gao et~al.(2018)Gao, Lu, Zhou, Zhu, and Nian~Wu]{gao2018learning}
Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian~Wu.
\newblock Learning generative {ConvNets} via multi-grid modeling and sampling.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9155--9164, 2018.

\bibitem[Gao et~al.(2020)Gao, Nijkamp, Kingma, Xu, Dai, and Wu]{gao2020flow}
Ruiqi Gao, Erik Nijkamp, Diederik~P Kingma, Zhen Xu, Andrew~M Dai, and
  Ying~Nian Wu.
\newblock Flow contrastive estimation of energy-based models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7518--7528, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2672--2680, 2014.

\bibitem[Goyal et~al.(2017)Goyal, Ke, Ganguli, and
  Bengio]{goyal2017variational}
Anirudh Goyal, Nan~Rosemary Ke, Surya Ganguli, and Yoshua Bengio.
\newblock Variational walkback: Learning a transition operator as a stochastic
  recurrent net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4392--4402, 2017.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Chen, Bettencourt, and
  Duvenaud]{grathwohl2019ffjord}
Will Grathwohl, Ricky T.~Q. Chen, Jesse Bettencourt, and David Duvenaud.
\newblock {FFJORD}: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Grathwohl et~al.(2020)Grathwohl, Wang, Jacobsen, Duvenaud, Norouzi,
  and Swersky]{grathwohl2020your}
Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud,
  Mohammad Norouzi, and Kevin Swersky.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Gregor et~al.(2016)Gregor, Besse, Rezende, Danihelka, and
  Wierstra]{gregor2016towards}
Karol Gregor, Frederic Besse, Danilo~Jimenez Rezende, Ivo Danihelka, and Daan
  Wierstra.
\newblock Towards conceptual compression.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  3549--3557, 2016.

\bibitem[Harsha et~al.(2007)Harsha, Jain, McAllester, and
  Radhakrishnan]{harsha2007communication}
Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan.
\newblock The communication complexity of correlation.
\newblock In \emph{Twenty-Second Annual IEEE Conference on Computational
  Complexity (CCC'07)}, pages 10--23. IEEE, 2007.

\bibitem[Havasi et~al.(2019)Havasi, Peharz, and
  Hernández-Lobato]{havasi2018minimal}
Marton Havasi, Robert Peharz, and José~Miguel Hernández-Lobato.
\newblock Minimal random code learning: Getting bits back from compressed model
  parameters.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock {GANs} trained by a two time-scale update rule converge to a local
  {Nash} equilibrium.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6626--6637, 2017.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2017beta}
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot,
  Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.
\newblock beta-{VAE}: Learning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Ho et~al.(2019)Ho, Chen, Srinivas, Duan, and Abbeel]{ho2019flow++}
Jonathan Ho, Xi~Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Huang et~al.(2020)Huang, Makhzani, Cao, and
  Grosse]{huang2020evaluating}
Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse.
\newblock Evaluating lossy compression rates of deep generative models.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Kalchbrenner et~al.(2017)Kalchbrenner, van~den Oord, Simonyan,
  Danihelka, Vinyals, Graves, and Kavukcuoglu]{kalchbrenner2017video}
Nal Kalchbrenner, Aaron van~den Oord, Karen Simonyan, Ivo Danihelka, Oriol
  Vinyals, Alex Graves, and Koray Kavukcuoglu.
\newblock Video pixel networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1771--1779, 2017.

\bibitem[Kalchbrenner et~al.(2018)Kalchbrenner, Elsen, Simonyan, Noury,
  Casagrande, Lockhart, Stimberg, Oord, Dieleman, and
  Kavukcuoglu]{kalchbrenner2018efficient}
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande,
  Edward Lockhart, Florian Stimberg, Aaron van~den Oord, Sander Dieleman, and
  Koray Kavukcuoglu.
\newblock Efficient neural audio synthesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  2410--2419, 2018.

\bibitem[Karras et~al.(2018)Karras, Aila, Laine, and
  Lehtinen]{karras2018progressive}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of {GAN}s for improved quality, stability, and
  variation.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019style}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4401--4410, 2019.

\bibitem[Karras et~al.(2020{\natexlab{a}})Karras, Aittala, Hellsten, Laine,
  Lehtinen, and Aila]{karras2020training}
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and
  Timo Aila.
\newblock Training generative adversarial networks with limited data.
\newblock \emph{arXiv preprint arXiv:2006.06676v1}, 2020{\natexlab{a}}.

\bibitem[Karras et~al.(2020{\natexlab{b}})Karras, Laine, Aittala, Hellsten,
  Lehtinen, and Aila]{karras2019analyzing}
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and
  Timo Aila.
\newblock Analyzing and improving the image quality of {StyleGAN}.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 8110--8119, 2020{\natexlab{b}}.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
Diederik~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10215--10224, 2018.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
Diederik~P Kingma, Tim Salimans, Rafal Jozefowicz, Xi~Chen, Ilya Sutskever, and
  Max Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4743--4751, 2016.

\bibitem[Lawson et~al.(2019)Lawson, Tucker, Dai, and
  Ranganath]{lawson2019energy}
John Lawson, George Tucker, Bo~Dai, and Rajesh Ranganath.
\newblock Energy-inspired models: Learning with sampler-induced distributions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8501--8513, 2019.

\bibitem[Levy et~al.(2018)Levy, Hoffman, and
  Sohl-Dickstein]{levy2018generalizing}
Daniel Levy, Matt~D. Hoffman, and Jascha Sohl-Dickstein.
\newblock Generalizing {H}amiltonian {M}onte {C}arlo with neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Maal{\o}e et~al.(2019)Maal{\o}e, Fraccaro, Li{\'e}vin, and
  Winther]{maaloe2019biva}
Lars Maal{\o}e, Marco Fraccaro, Valentin Li{\'e}vin, and Ole Winther.
\newblock {BIVA}: A very deep hierarchy of latent variables for generative
  modeling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6548--6558, 2019.

\bibitem[Menick and Kalchbrenner(2019)]{menick2018generating}
Jacob Menick and Nal Kalchbrenner.
\newblock Generating high fidelity images with subscale pixel networks and
  multidimensional upscaling.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Nichol(2020)]{nichol2020vq}
Alex Nichol.
\newblock {VQ-DRAW}: A sequential discrete {VAE}.
\newblock \emph{arXiv preprint arXiv:2003.01599}, 2020.

\bibitem[Nijkamp et~al.(2019{\natexlab{a}})Nijkamp, Hill, Han, Zhu, and
  Wu]{nijkamp2019anatomy}
Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying~Nian Wu.
\newblock On the anatomy of {MCMC}-based maximum likelihood learning of
  energy-based models.
\newblock \emph{arXiv preprint arXiv:1903.12370}, 2019{\natexlab{a}}.

\bibitem[Nijkamp et~al.(2019{\natexlab{b}})Nijkamp, Hill, Zhu, and
  Wu]{nijkamp2019learning}
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning non-convergent non-persistent short-run {MCMC} toward
  energy-based model.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5233--5243, 2019{\natexlab{b}}.

\bibitem[Ostrovski et~al.(2018)Ostrovski, Dabney, and
  Munos]{ostrovski2018autoregressive}
Georg Ostrovski, Will Dabney, and Remi Munos.
\newblock Autoregressive quantile networks for generative modeling.
\newblock In \emph{International Conference on Machine Learning}, pages
  3936--3945, 2018.

\bibitem[Prenger et~al.(2019)Prenger, Valle, and
  Catanzaro]{prenger2019waveglow}
Ryan Prenger, Rafael Valle, and Bryan Catanzaro.
\newblock {WaveGlow}: A flow-based generative network for speech synthesis.
\newblock In \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3617--3621. IEEE, 2019.

\bibitem[Razavi et~al.(2019)Razavi, van~den Oord, and
  Vinyals]{razavi2019generating}
Ali Razavi, Aaron van~den Oord, and Oriol Vinyals.
\newblock Generating diverse high-fidelity images with {VQ-VAE-2}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14837--14847, 2019.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, pages
  1530--1538, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International Conference on Machine Learning}, pages
  1278--1286, 2014.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015unet}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock {U-Net}: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical Image Computing and
  Computer-Assisted Intervention}, pages 234--241. Springer, 2015.

\bibitem[Salimans and Kingma(2016)]{salimans2016weight}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  901--909, 2016.

\bibitem[Salimans et~al.(2015)Salimans, Kingma, and
  Welling]{salimans2015markov}
Tim Salimans, Diederik Kingma, and Max Welling.
\newblock {Markov Chain Monte Carlo} and variational inference: Bridging the
  gap.
\newblock In \emph{International Conference on Machine Learning}, pages
  1218--1226, 2015.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2234--2242, 2016.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Tim Salimans, Andrej Karpathy, Xi~Chen, and Diederik~P Kingma.
\newblock Pixel{CNN}++: Improving the {PixelCNN} with discretized logistic
  mixture likelihood and other modifications.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International Conference on Machine Learning}, pages
  2256--2265, 2015.

\bibitem[Song et~al.(2017)Song, Zhao, and Ermon]{song2017nice}
Jiaming Song, Shengjia Zhao, and Stefano Ermon.
\newblock A-{NICE}-{MC}: Adversarial training for {MCMC}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5140--5150, 2017.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11895--11907, 2019.

\bibitem[Song and Ermon(2020)]{song2020improved}
Yang Song and Stefano Ermon.
\newblock Improved techniques for training score-based generative models.
\newblock \emph{arXiv preprint arXiv:2006.09011}, 2020.

\bibitem[van~den Oord et~al.(2016{\natexlab{a}})van~den Oord, Dieleman, Zen,
  Simonyan, Vinyals, Graves, Kalchbrenner, Senior, and
  Kavukcuoglu]{oord2016wavenet}
Aaron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals,
  Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock {WaveNet}: A generative model for raw audio.
\newblock \emph{arXiv preprint arXiv:1609.03499}, 2016{\natexlab{a}}.

\bibitem[van~den Oord et~al.(2016{\natexlab{b}})van~den Oord, Kalchbrenner, and
  Kavukcuoglu]{oord2016pixel}
Aaron van~den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock \emph{International Conference on Machine Learning},
  2016{\natexlab{b}}.

\bibitem[van~den Oord et~al.(2016{\natexlab{c}})van~den Oord, Kalchbrenner,
  Vinyals, Espeholt, Graves, and Kavukcuoglu]{oord2016conditional}
Aaron van~den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex
  Graves, and Koray Kavukcuoglu.
\newblock Conditional image generation with {PixelCNN} decoders.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4790--4798, 2016{\natexlab{c}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural Computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Wang et~al.(2020)Wang, Wang, Zhang, Owens, and
  Efros]{wang2019cnngenerated}
Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei~A Efros.
\newblock Cnn-generated images are surprisingly easy to spot...for now.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2020.

\bibitem[Wang et~al.(2018)Wang, Girshick, Gupta, and He]{wang2018non}
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
\newblock Non-local neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7794--7803, 2018.

\bibitem[Wiggers and Hoogeboom(2020)]{wiggers2020predictive}
Auke~J Wiggers and Emiel Hoogeboom.
\newblock Predictive sampling with forecasting autoregressive models.
\newblock \emph{arXiv preprint arXiv:2002.09928}, 2020.

\bibitem[Wu et~al.(2020)Wu, K{\"o}hler, and No{\'e}]{wu2020stochastic}
Hao Wu, Jonas K{\"o}hler, and Frank No{\'e}.
\newblock Stochastic normalizing flows.
\newblock \emph{arXiv preprint arXiv:2002.06707}, 2020.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 3--19, 2018.

\bibitem[Xie et~al.(2016)Xie, Lu, Zhu, and Wu]{xie2016theory}
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu.
\newblock A theory of generative convnet.
\newblock In \emph{International Conference on Machine Learning}, pages
  2635--2644, 2016.

\bibitem[Xie et~al.(2017)Xie, Zhu, and Nian~Wu]{xie2017synthesizing}
Jianwen Xie, Song-Chun Zhu, and Ying Nian~Wu.
\newblock Synthesizing dynamic patterns by spatial-temporal generative convnet.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7093--7101, 2017.

\bibitem[Xie et~al.(2018)Xie, Zheng, Gao, Wang, Zhu, and
  Nian~Wu]{xie2018learning}
Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying
  Nian~Wu.
\newblock Learning descriptor networks for 3d shape synthesis and analysis.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8629--8638, 2018.

\bibitem[Xie et~al.(2019)Xie, Zhu, and Wu]{xie2019learning}
Jianwen Xie, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning energy-based spatial-temporal generative convnets for
  dynamic patterns.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2019.

\bibitem[Yu et~al.(2015)Yu, Zhang, Song, Seff, and Xiao]{yu15lsun}
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.
\newblock {LSUN}: Construction of a large-scale image dataset using deep
  learning with humans in the loop.
\newblock \emph{arXiv preprint arXiv:1506.03365}, 2015.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\end{thebibliography}
