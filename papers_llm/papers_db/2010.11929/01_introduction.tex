\section{Introduction}


Self-attention-based architectures, in particular Transformers~\citep{vaswani2017}, have become the model of choice in natural language processing (NLP).
The dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset~\citep{devlin19-bert}.
Thanks to Transformers' computational efficiency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters~\citep{brown2020-gpt3,lepikhin2020gshard}.
With the models and datasets growing, there is still no sign of saturating performance.


In computer vision, however, convolutional architectures remain dominant~\citep{LeCun1989BackpropagationAT,KrizhevskyNIPS12,he2016deep}.
Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention~\citep{wang2018-nonlocalnn,carion20-detr}, some replacing the convolutions entirely~\citep{ramachandran19-sasa,wang2020-axialdeeplab}.
The latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns.
Therefore, in large-scale image recognition, classic ResNet-like architectures are still state of the art~\citep{mahajan2018,xie2020-noisystudent,kolesnikov2020-bit}.

Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications.
To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer.
Image patches are treated the same way as tokens (words) in an NLP application.
We train the model on image classification in supervised fashion.

When trained on mid-sized datasets such as \imagenet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size.
This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.

However, the picture changes if the models are trained on larger datasets (14M-300M images). 
We find that large scale training trumps inductive bias.
Our \oursfull{} (\oursabbrv{}) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints.
When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, \oursabbrv{} approaches or beats state of the art on multiple image recognition benchmarks.
In particular, the best model reaches the accuracy of $88.55\%$ on \imagenet, $90.72\%$ on \imagenet-ReaL, $94.55\%$ on CIFAR-100, and $77.63\%$ on the VTAB suite of 19 tasks.















