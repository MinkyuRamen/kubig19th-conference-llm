\section{Related Work}

Transformers were proposed by \citet{vaswani2017} for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT~\citep{devlin19-bert} uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task~\citep{radford2018-gpt,radford2019-gpt2,brown2020-gpt3}.

Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes.
Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past.
\citet{parmar18-imagetransformer} applied the self-attention only in local neighborhoods for each query pixel instead of globally.
Such local multi-head dot-product self attention blocks can completely replace convolutions~\citep{hu2019local,ramachandran19-sasa,zhao2020-san}.
In a different line of work, Sparse Transformers~\citep{child2019-sparsetransformers} employ scalable approximations to global self-attention in order to be applicable to images.
An alternative way to scale attention is to apply it in blocks of varying sizes \citep{weissenborn2019-savm}, in the extreme case only along individual axes~\citep{ho2019-axialattention,wang2020-axialdeeplab}.
Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators.

Most related to ours is the model of~\citet{cordonnier2020-sacnn}, which extracts patches of size $2 \times 2$ from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, \citet{cordonnier2020-sacnn} use a small patch size of $2 \times 2$ pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.

There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification~\citep{bello2019-attentionaugmentedcnn} or by further processing the output of a CNN using self-attention, e.g. for object detection~\citep{hu2018-relationnetworks, carion20-detr}, video processing~\citep{wang2018-nonlocalnn,sun2019-videobert}, image classification~\citep{wu2020-visualtransformer}, unsupervised object discovery~\citep{locatello2020-slotattention}, or unified text-vision tasks~\citep{chenuniter,vilbert,visualbert}.

Another recent related model is image GPT (iGPT)~\citep{chen20-igpt}, which applies Transformers to image pixels after reducing image resolution and color space. The model is trained in an unsupervised fashion as a generative model, and the resulting representation can then be fine-tuned or probed linearly for classification performance, achieving a maximal accuracy of 72\% on \imagenet.


Our work adds to the increasing collection of papers that explore image recognition at larger scales than the standard \imagenet dataset.
The use of additional data sources allows to achieve state-of-the-art results on standard benchmarks~\citep{mahajan2018, touvron2019, xie2020-noisystudent}.
Moreover, \citet{sun2017-jft} study how CNN performance scales with dataset size, and \citet{kolesnikov2020-bit, djolonga2020-robustness} perform an empirical exploration of CNN transfer learning from large scale datasets such as ImageNet-21k and JFT-300M.
We focus on these two latter datasets as well, but train Transformers instead of ResNet-based models used in prior works. 


