\section{Conclusion}

We have explored the direct application of Transformers to image recognition.
Unlike prior works using self-attention in computer vision, we do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step.
Instead, we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP.
This simple, yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.
Thus, \oursfull matches or exceeds the state of the art on many image classification datasets, whilst being relatively cheap to pre-train.

While these initial results are encouraging, many challenges remain.
One is to apply \oursabbrv to other computer vision tasks, such as detection and segmentation.
Our results, coupled with those in \citet{carion20-detr}, indicate the promise of this approach.
Another challenge is to continue exploring self-supervised pre-training methods.
Our initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pre-training.
Finally, further scaling of \oursabbrv{} would likely lead to improved performance.