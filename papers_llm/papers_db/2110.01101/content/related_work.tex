\section{Related Work}
\subsection{Parallel and Distributed Reinforcement Learning}
Existing works that aim to improve the execution time of Reinforcement Learning (RL) algorithms focus on increasing the parallelism by increasing the number of actors and learners. GORILA \cite{gorila} proposes the first parallel architecture of DQN \cite{dqn} to play Atari games \cite{openai_gym}. 
They employ independent actors and learners in parallel with a global parameter server. 
Our method follows the general architecture of GORILA \cite{gorila} at a high level and proposes detailed data structures and thread-level synchronization mechanism to maximize the scalable performance. A3C \cite{a3c} uses asynchronous actors to collect the data and update the agent using actor critic algorithms without using a Replay Buffer. Due to synchronization overhead, A3C doesn't scale very well. IMPALA \cite{impala} relaxes the synchronization overhead of A3C by using importance sampling. RLlib \cite{ray_rllib} proposes abstractions for distributed reinforcement learning for software developers built on top of the Ray library \cite{ray_rllib} written in Python \cite{python}. PAAC \cite{paac} proposes parallel advantaged actor critic. They synchronize the actors after every environmental step. This significantly slows down the entire system. In contrast, our actors act independently in parallel.
\cite{map_reduce_parallel_rl} proposes parallel reinforcement learning using popular MapReduce \cite{map_reduce} framework with linear function approximation.
\cite{parallel_rl} proposes to use parallel actors to learn in tabular MDP while our method can tackle general continuous space MDP with neural network policies. 

A key bottleneck in these works is the management of Replay Buffer. Thread-level synchronization overheads and irregular memory accesses while accessing the Replay Buffer lead to poor scalability when parallelism is increased by adding more hardware resources. Ape-X \cite{apex} proposes distributed Prioritized Replay Buffer with parallel actors and a single learner to accelerate reinforcement learning algorithms on large scale clusters. However, to the best of our knowledge, our approach is the first to explicitly focus on improving the efficiency of Replay Buffer management on multi-core platforms by developing a novel data structure and low overhead thread-level synchronization mechanisms to enable high throughput parallel Replay Buffer management.

In addition to these works, specialized hardware designs to accelerate reinforcement learning have also emerged recently. \cite{trpo_fpga} proposes customized Pearlmutter Propagation on FPGAs to accelerate conjugate gradient method used in TRPO \cite{trpo}. \cite{ppo_fpga} proposes a systolic-array based architecture on FPGAs to accelerate PPO \cite{ppo}. However, these works do not require the use of Replay Buffer.


\subsection{Parallel Stochastic Gradient Descent}
We also review techniques for performing parallel stochastic gradient descent as it is used in our learner implementation. \cite{parameter_server} proposes parameter server to facilitate parallel stochastic gradient descent. Each worker samples a batch of data, computes the gradients and send them to the central parameter server. The parameter server aggregates the gradients and performs the update. The workers then pull the updated weights from the parameter server. \cite{AsyncPSGD} proposes asynchronous stochastic gradient descent to reduce the negative impact of asynchrony with general convergence time bounds. For simplicity, we adopt the parameter server \cite{parameter_server} framework and leave advanced asynchronous methods for future work.