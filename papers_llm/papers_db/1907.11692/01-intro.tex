\section{Introduction}
\label{intro}

Self-training methods such as ELMo~\cite{peters2018deep}, GPT~\cite{radford2018gpt}, BERT \cite{devlin2018bert}, XLM~\cite{lample2019cross}, and XLNet \cite{yang2019xlnet} have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. %
Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances. 


We present a replication study of BERT pretraining~\cite{devlin2018bert}, which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. %
We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call \ourmodel{}, that can match or exceed the performance of all of the post-BERT methods.
Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (\textsc{CC-News}) of comparable size to other privately used datasets, to better control for training set size effects. 

When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD.
When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by \newcite{yang2019xlnet}.
Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS-B.
We also match state-of-the-art results on SQuAD and RACE.
Overall, we re-establish that BERT's masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling~\cite{yang2019xlnet}.\footnote{It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.}

In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; (2) We use a novel dataset, \textsc{CC-News}, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch~\cite{paszke2017automatic}.