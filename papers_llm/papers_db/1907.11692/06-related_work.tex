\section{Related Work} \label{sec:relwork}

Pretraining methods have been designed with different training objectives, including language modeling~\cite{dai2015semi,peters2018deep,howard2018universal},  machine translation~\cite{mccann2017learned}, and masked language modeling~\cite{devlin2018bert,lample2019cross}. Many recent papers have used a basic recipe of  finetuning models for each end task~\cite{howard2018universal,radford2018gpt}, and pretraining with some variant of a masked language model objective.  However, newer methods have improved performance by multi-task fine tuning~\cite{dong2019unified}, incorporating entity embeddings~\cite{sun2019ernie}, span prediction~\cite{joshi2019spanbert}, and multiple variants of autoregressive pretraining~\cite{song2019mass,chan2019kermit,yang2019xlnet}.  Performance is also typically improved by training bigger models on more data~\cite{devlin2018bert,baevski2019cloze,yang2019xlnet,radford2019language}. Our goal was to replicate, simplify, and better tune the training of BERT, as a reference point for better understanding the relative performance of all of these methods. 

