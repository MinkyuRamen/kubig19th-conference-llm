\section{Conclusion} \label{sec:conclusion}
In this work, we proposed a comprehensive two-staged framework, namely \modelname{}, to perform monocular depth estimation and depth super-resolution using conditional denoising diffusion probabilistic models. Our depth diffusion model effectively generates depth maps conditioned on low-resolution RGB images in the first stage, while the super-resolution model produces high-resolution depth maps based on a low resolution RGB-D input in the second stage. We introduced a novel depth noise augmentation technique to enhance the robustness of the super-resolution model. Through a series of experiments and ablations we justified design decisions and the effectiveness of our framework and complement those with a variety of generated high resolution RGB-D images.

% Limitations
\subsection{Limitations} \label{subsec:limitations}
While our approach generates high-resolution depth maps, the two-stage approach using two consecutive DDPMs leads to a high demand of resources for sampling and training, especially with increasing image resolutions. We observe that for some "in the wild predictions" the domain gap between input data and training data is to large to generate plausible depth maps indicating a larger dataset might be beneficial. Further, our model outputs a perspective depth which requires a known projection matrix to obtain the depth in any desired coordinate system.

% Future Line of Work
\subsection{Future Work} \label{subsec:future_work}
 Future research might focus on applying faster sampling algorithms and different model architectures like latent diffusion models which might be promising especially for generating depth maps in higher resolutions. Furthermore, one could extent our approach to directly predict the depth in camera or world coordinates or incorporate camera parameters either into the model architecture or in the pre-processing/augmentation stage of the data pipeline. Further, it would be interesting to investigate different representations for the depth data like point clouds or meshes. Finally, one could also imagine, extending our approach to generate occluded regions of the subject, like it's back, combining it to a complete 3D model.