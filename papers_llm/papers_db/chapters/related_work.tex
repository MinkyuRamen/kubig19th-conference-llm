\section{Related Work} \label{sec:related_work}
%In this section we put our work into a broader context and provide a summary of other related work, which by no means we claim to be extensive, but rather an overview.
In this section, we situate our work within a broader context and offer a concise summary of other relevant studies.

% DDPMs 
\subsection{Denoising Diffusion Probabilistic Models} Denoising diffusion models are a type of generative deep learning models first formulated by \cite{sohl-dickstein_deep_2015} and further extended to Denoising Diffusion Probabilistic models (DDPMs) by \cite{ho_denoising_2020,nichol_improved_2021}. These models use a Markov Chain to gradually convert one simple and well-known distribution (e.g. a Gaussian distribution) into a usually more complex data distribution the model can sample from. 

% diffusion applications
Diffusion models have been applied to many applications and modalities including image generation \citep{dhariwal_diffusion_2021}, image-to-image translation \citep{saharia_palette_2022}, super-resolution \citep{ho_cascaded_2021, saharia_image_2021}, video \citep{ho_video_2022,harvey_flexible_2022,yang_diffusion_2022}, audio \citep{kong_diffwave_2021,lee_priorgrad_2022}, text-to-image \citep{ramesh_hierarchical_2022, nichol_towards_2022,saharia_photorealistic_2022}, and simultaneous multi-modal generation \citep{ruan_mm-diffusion_2023}.

% generative trilemma
In the generative learning trilemma formulated by \cite{xiao_tackling_2022}, which states that generative models cannot satisfy all three key requirements fast sampling, high quality samples and mode coverage, diffusion models have shown good results in high quality image generation \citep{rampas_novel_2023,ho_cascaded_2021} and mode coverage. While Variational Auto-Encoders (VAEs) \citep{kingma_auto-encoding_2022,razavi_generating_2019} and flow based models \citep{dinh_density_2017,durkan_neural_2019} are strong in covering multi-modal data distributions and can be sampled from very fast, the quality of their samples is usually not as high as of Generative Adversarial Models (GANs) or diffusion models. GANs \citep{goodfellow_generative_2014,brock_large_2019,kirch_vologan_2022} on the other-hand produce high quality images and are sampled quickly but are prone to mode collapse and often do not cover the entire data distribution.

% speed up sampling
DDPMs require many reverse diffusion steps (often several hundreds or even thousands of steps) to sample from, making it difficult train and deploy them even on modern GPUs. Hence it is no surprise that a lot of research focuses on increasing the sample speed of diffusion models by reducing the number of required steps \citep{song_denoising_2022, xiao_tackling_2022, nichol_improved_2021, salimans_progressive_2022,chung_come-closer-diffuse-faster_2022}, perform diffusion in the latent space rather than the full-resolution data distribution \citep{preechakul_diffusion_2022,rombach_high-resolution_2022,sinha_d2c_2021, gu_vector_2022, tang_improved_2023} or formulate the diffusion problem as time-continuous problem to take advantage of accelerated stochastic differential equations (SDE) solvers \citep{song_score-based_2021,song_maximum_2021, song_improved_2020,karras_elucidating_2022}.

% conditional generation
To control the output of the model it must be provided with an additional condition input. The model might be conditioned on another input of the same modality like in colorization \citep{saharia_palette_2022}, inpaintings \citep{batzolis_conditional_2021}, generation from semantic mask \citep{meng_sdedit_2022} or image super-resolution \citep{saharia_image_2021,ho_cascaded_2021}, on an input of different modality like in depth estimation \citep{duan_diffusiondepth_2023} or segmentation \citep{baranchuk_label-efficient_2022}, on class labels \citep{dhariwal_diffusion_2021} or on text prompts \citep{ramesh_hierarchical_2022, nichol_towards_2022,saharia_photorealistic_2022}; to name a few.

Depending on the representation of the condition input, it might be concatenated with the noise input \citep{batzolis_conditional_2021}, fed to multiple layers within the network like adaptive instance normalization \citep{karras_style-based_2019} or via an independent network branch \citep{zhang_adding_2023}. Beside the architectural choice one must also decide how strong the condition should be. One could only apply it for certain time steps in the reverse diffusion process, only apply it during inference on an unconditionally trained model \citep{choi_ilvr_2021} or using guidance, which applies a weighted sum of a conditional and unconditional generation and hence trades-off sample diversity with sample quality. Among others, guidance can be performed with an additionally trained classifier \citep{nichol_improved_2021}, by training the diffusion model conditionally and unconditionally at the same time and sampling using a weighted sum of both \citep{ho_classifier-free_2022} or by using pre-trained CLIP embeddings \citep{nichol_towards_2022,ramesh_hierarchical_2022}.

% Monocular depth estimation
\subsection{Monocular Depth Estimation} Knowledge on the depth of a scene is crucial in a vast number of applications including virtual reality (VR), augmented reality (AR), environment perception, autonomous driving, robotics, state estimation, navigation, mapping and many more. Various surveys \citep{bhoi_monocular_2019,zhao_monocular_2020,ming_deep_2021, masoumian_monocular_2022} summarize the rich and extensive literature on monocular depth estimation; the estimation of the depth based on a single image; an inherently ill-posed and ambiguous problem.

In contrast, conventional geometric-based approaches such as structure from motion and stereo vision matching rely on geometric constraints and multiple viewpoints to reconstruct 3D structures. On the other hand, sensor-based methods leverage dedicated hardware like LiDAR sensors or RGB-D cameras to directly capture depth information. While these methods find practical application, they suffer from significant limitations including high hardware expenses, imprecise and sparse predictions, and limited accessibility for consumer use.

Many different representations can be deployed to obtain depth information: 2D depth maps (dense or sparse), 3D point cloud, 3D pseudo point cloud predicted from other modality (e.g. stereo camera), camera independent disparity maps, light fields, neural radiance fields (NERFs, \cite{mildenhall2021nerf}), 3D meshes, voxels and height maps to name a few.

Numerous deep learning based approaches for monocular depth estimation have been researched in recent years. 
\cite{lu_self-supervised_2022}, \cite{chen_self-supervised_2018} and \cite{zhang_unsupervised_2020} train their models using stereo images while inferring only single view images. The model either learns the correspondence between the two views and can reconstruct the other view or the model incorporates the knowledge of a stereo camera into its weights and hence strengthen its capability to extract meaningful features from a single image. \cite{yue_self-supervised_2022} and \cite{ zhao_joint_2022} apply a multi-task training objective by not only predicting depth but also other related tasks (e.g. normal vector prediction) that help the model to learn a more profound representation and understanding of the scene which also benefits the downstream task of depth estimation.
\cite{watson_temporal_2021} and \cite{bian_unsupervised_2021} use mono camera videos to estimate the depth.

Other deep learning-based approaches focus on multiple sensor modalities to estimate the depth of the scene. \cite{zhang_lidar-guided_2022} use LiDAR point clouds in combination with stereo images, \cite{eldesokey_confidence_2020} use monocular RGB images combined with sparse LiDAR point clouds, \cite{liu_pseudo-lidar_2020} input monocular RGB combined with a depth map and \cite{piao_dynamic_2021} combine a single RGB image with a focal stack.

In this work, we focus on monocular depth estimation using single RGB images as input and generating dense depth maps as output.
We observed that most model architectures are based on GANs, VAEs or similar approaches. We hence further review the usage of diffusion models in the field of depth estimation.

% Depth Diffusion
\subsection{Depth Diffusion}
We observe that very little work has been published on monocular depth estimation using diffusion models. We hypothesize that one of the major reasons is the necessity for fast sampling constraint by real-time applications like autonomous driving. We are certain that the community will find a way to further increase the sampling speed in a future and hence we see great potential in diffusion models for monocular depth estimation.

\cite{saxena_monocular_2023} used a diffusion model to perform monocular depth estimation on indoor and outdoor scenes introducing a preprocessing step to complete incomplete depth data and were even able to resolve depth ambiguity introduced from transparent surfaces.
\cite{duan_diffusiondepth_2023} use a latent diffusion model in combination with a Swin Transformer backbone \citep{liu_swin_2021} to first encode the depth into latent space and then perform the reverse diffusion in the latent space by iteratively applying their light weighted monocular conditioned denoising block. Finally, they apply a fully convolutional decoder on the diffused latent space to obtain the final depth prediction.
\cite{ranftl_towards_2020} propose methods to mix multiple depth datasets for robust training to mitigate the challenges of acquiring dense ground truth data from a variety of scenes. \cite{bhat_zoedepth_2023} proposes a method to combine relative depth and metric depth in a zero-shot manner, by pre-training on a large corpus of relative depth datasets and finetuning on metric depth.

Not as closely related but still applying diffusion models on 3D related data representations, we found \textbf{3D point cloud generation} conditioned on monocular images \citep{zhou_3d_2021}, conditioned on an encoded shape latent \citep{luo_diffusion_2021} and conditioned in CLIP-tokens \citep{nichol_point-e_2022}, \textbf{novel view synthesis} from a single view \citep{rockwell_pixelsynth_2021,watson_novel_2022}, for perpetual view generations for long camera trajectories where depth is predicted as an intermediate representation \citep{liu_infinite_2021} or combining text prompts for 2D generation with Neural Radiance Fields (NeRFs) \citep{poole_dreamfusion_2022}, \textbf{depth estimation} from multiple camera images at different viewpoints \citep{khan_differentiable_2021} and \textbf{depth-aware guidance} methods that guide the image generation process by its intermediate depth representation \citep{kim_dag_2023}.