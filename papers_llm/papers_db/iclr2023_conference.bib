@article{qin2023tool,
  title={Tool learning with foundation models},
  author={Qin, Yujia and Hu, Shengding and Lin, Yankai and Chen, Weize and Ding, Ning and Cui, Ganqu and Zeng, Zheni and Huang, Yufei and Xiao, Chaojun and Han, Chi and others},
  journal={arXiv preprint arXiv:2304.08354},
  year={2023}
}

@article{ye2023large,
  title={Large Language Model as Autonomous Decision Maker},
  author={Ye, Yining and Cong, Xin and Qin, Yujia and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2308.12519},
  year={2023}
}

@article{liu2023llm+,
  title={Llm+ p: Empowering large language models with optimal planning proficiency},
  author={Liu, Bo and Jiang, Yuqian and Zhang, Xiaohan and Liu, Qiang and Zhang, Shiqi and Biswas, Joydeep and Stone, Peter},
  journal={arXiv preprint arXiv:2304.11477},
  year={2023}
}

@article{yang2023mm,
  title={Mm-react: Prompting chatgpt for multimodal reasoning and action},
  author={Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
  journal={arXiv preprint arXiv:2303.11381},
  year={2023}
}

@article{song2022llm,
  title={Llm-planner: Few-shot grounded planning for embodied agents with large language models},
  author={Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M and Chao, Wei-Lun and Su, Yu},
  journal={arXiv preprint arXiv:2212.04088},
  year={2022}
}

@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}

@article{wang2023describe,
  title={Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents},
  author={Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  journal={arXiv preprint arXiv:2302.01560},
  year={2023}
}

@article{gur2023real,
  title={A real-world webagent with planning, long context understanding, and program synthesis},
  author={Gur, Izzeddin and Furuta, Hiroki and Huang, Austin and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  journal={arXiv preprint arXiv:2307.12856},
  year={2023}
}

@article{qiao2023making,
  title={Making Language Models Better Tool Learners with Execution Feedback},
  author={Qiao, Shuofei and Gui, Honghao and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13068},
  year={2023}
}

@article{gao2023assistgpt,
  title={AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn},
  author={Gao, Difei and Ji, Lei and Zhou, Luowei and Lin, Kevin Qinghong and Chen, Joya and Fan, Zihan and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2306.08640},
  year={2023}
}

@article{lu2023chameleon,
  title={Chameleon: Plug-and-play compositional reasoning with large language models},
  author={Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.09842},
  year={2023}
}

@article{yang2023gpt4tools,
  title={Gpt4tools: Teaching large language model to use tools via self-instruction},
  author={Yang, Rui and Song, Lin and Li, Yanwei and Zhao, Sijie and Ge, Yixiao and Li, Xiu and Shan, Ying},
  journal={arXiv preprint arXiv:2305.18752},
  year={2023}
}

@article{zhuang2023toolqa,
  title={ToolQA: A Dataset for LLM Question Answering with External Tools},
  author={Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
  journal={arXiv preprint arXiv:2306.13304},
  year={2023}
}

@article{deng2023mind2web,
  title={Mind2Web: Towards a Generalist Agent for the Web},
  author={Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Samuel and Wang, Boshi and Sun, Huan and Su, Yu},
  journal={arXiv preprint arXiv:2306.06070},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zhou2023webarena,
  title={Webarena: A realistic web environment for building autonomous agents},
  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and others},
  journal={arXiv preprint arXiv:2307.13854},
  year={2023}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{openaichatgptblog,   
    title = {Open{AI}: Introducing {ChatGPT}},   
    url = {https://openai.com/blog/chatgpt},   
    author = {OpenAI},   
    year = {2022}
}

@misc{openai2023gpt4,
 archiveprefix = {arXiv},
 author = {OpenAI},
 eprint = {2303.08774},
 primaryclass = {cs.CL},
 title = {GPT-4 Technical Report},
 year = {2023}
}

@article{qin2023webcpm,
  title={WebCPM: Interactive Web Search for Chinese Long-form Question Answering},
  author={Qin, Yujia and Cai, Zihan and Jin, Dian and Yan, Lan and Liang, Shihao and Zhu, Kunlun and Lin, Yankai and Han, Xu and Ding, Ning and Wang, Huadong and others},
  journal={arXiv preprint arXiv:2305.06849},
  year={2023}
}

@article{cobbe2021training,
 author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
 journal = {ArXiv preprint},
 title = {Training verifiers to solve math word problems},
 volume = {abs/2110.14168},
 year = {2021}
}

@article{nakano2021webgpt,
 author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
 journal = {ArXiv preprint},
 title = {WebGPT: Browser-assisted question-answering with human feedback},
 volume = {abs/2112.09332},
 year = {2021}
}

@article{schick2023toolformer,
 author = {Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
 journal = {ArXiv preprint},
 title = {Toolformer: Language models can teach themselves to use tools},
 volume = {abs/2302.04761},
 year = {2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{ding2022delta,
  title={Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={arXiv preprint arXiv:2203.06904},
  year={2022}
}

@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@article{wang2022self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{patil2023gorilla,
  title={Gorilla: Large Language Model Connected with Massive APIs},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@article{li2023api,
  title={Api-bank: A benchmark for tool-augmented llms},
  author={Li, Minghao and Song, Feifan and Yu, Bowen and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin},
  journal={arXiv preprint arXiv:2304.08244},
  year={2023}
}

@article{song2023restgpt,
  title={RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs},
  author={Song, Yifan and Xiong, Weimin and Zhu, Dawei and Li, Cheng and Wang, Ke and Tian, Ye and Li, Sujian},
  journal={arXiv preprint arXiv:2306.06624},
  year={2023}
}

@article{tang2023toolalpaca,
  title={ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases},
  author={Tang, Qiaoyu and Deng, Ziliang and Lin, Hongyu and Han, Xianpei and Liang, Qiao and Sun, Le},
  journal={arXiv preprint arXiv:2306.05301},
  year={2023}
}

@article{xu2023tool,
  title={On the Tool Manipulation Capability of Open-source Large Language Models},
  author={Xu, Qiantong and Hong, Fenglu and Li, Bo and Hu, Changran and Chen, Zhengyu and Zhang, Jian},
  journal={arXiv preprint arXiv:2305.16504},
  year={2023}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@misc{wei2023chainofthought,
 archiveprefix = {arXiv},
 author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
 eprint = {2201.11903},
 primaryclass = {cs.CL},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 year = {2023}
}

@article{yao2022react,
 author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
 journal = {ArXiv preprint},
 title = {React: Synergizing reasoning and acting in language models},
 volume = {abs/2210.03629},
 year = {2022}
}

@inproceedings{gupta2023visual,
  title={Visual programming: Compositional visual reasoning without training},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14953--14962},
  year={2023}
}

@article{jin2023genegpt,
  title={Genegpt: Augmenting large language models with domain tools for improved access to biomedical information},
  author={Jin, Qiao and Yang, Yifan and Chen, Qingyu and Lu, Zhiyong},
  journal={ArXiv},
  year={2023},
  publisher={ArXiv}
}

@article{yang2023chatgpt,
  title={ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling},
  author={Yang, Linyao and Chen, Hongyang and Li, Zhao and Ding, Xiao and Wu, Xindong},
  journal={arXiv preprint arXiv:2306.11489},
  year={2023}
}

@techreport{vemprala2023chatgpt,
author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
title = {ChatGPT for Robotics: Design Principles and Model Abilities},
institution = {Microsoft},
year = {2023},
month = {February},
number = {MSR-TR-2023-8},
}

@misc{shen2023hugginggpt,
 archiveprefix = {arXiv},
 author = {Yongliang Shen and Kaitao Song and Xu Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},
 eprint = {2303.17580},
 primaryclass = {cs.CL},
 title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},
 year = {2023}
}

@article{wu2023visual,
 author = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
 journal = {ArXiv preprint},
 title = {Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models},
 volume = {abs/2303.04671},
 year = {2023}
}

@article{hao2023toolkengpt,
  title={ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings},
  author={Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.11554},
  year={2023}
}

@article{qian2023creator,
  title={CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation},
  author={Qian, Cheng and Han, Chi and Fung, Yi R and Qin, Yujia and Liu, Zhiyuan and Ji, Heng},
  journal={arXiv preprint arXiv:2305.14318},
  year={2023}
}

@article{xu2023baize,
  title={Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}

@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{penedo2023refinedweb,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@inproceedings{mishra2022cross,
  title={Cross-Task Generalization via Natural Language Crowdsourcing Instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3470--3487},
  year={2022}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{devlin2018bert,
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 url = {https://aclanthology.org/N19-1423},
 year = {2019}
}

@article{chen2023extending,
  title={Extending Context Window of Large Language Models via Positional Interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@inproceedings{bach2022promptsource,
  title={PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
  author={Bach, Stephen and Sanh, Victor and Yong, Zheng Xin and Webson, Albert and Raffel, Colin and Nayak, Nihal V and Sharma, Abheesht and Kim, Taewoon and Bari, M Saiful and F{\'e}vry, Thibault and others},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={93--104},
  year={2022}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@misc{shinn2023reflexion,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Beck Labash and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{huang2022inner,
 author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
 journal = {ArXiv preprint},
 title = {Inner monologue: Embodied reasoning through planning with language models},
 volume = {abs/2207.05608},
 year = {2022}
}

@inproceedings{huang2022language,
 author = {Wenlong Huang and
Pieter Abbeel and
Deepak Pathak and
Igor Mordatch},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/HuangAPM22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {9118--9147},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
 title = {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge
for Embodied Agents},
 volume = {162},
 year = {2022}
}

@article{ahn2022can,
 author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
 journal = {ArXiv preprint},
 title = {Do as i can, not as i say: Grounding language in robotic affordances},
 volume = {abs/2204.01691},
 year = {2022}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@article{jarvelin2002cumulated,
  title={Cumulated gain-based evaluation of IR techniques},
  author={J{\"a}rvelin, Kalervo and Kek{\"a}l{\"a}inen, Jaana},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={20},
  number={4},
  pages={422--446},
  year={2002},
  publisher={ACM New York, NY, USA}
}