\section{Related Work}
\textbf{Tool Learning} \quad
Recent studies have shed light on the burgeoning capabilities of LLMs in mastering tools and making decisions within complex environments~\citep{vemprala2023chatgpt,nakano2021webgpt,qin2023webcpm,shen2023hugginggpt,wu2023visual,schick2023toolformer,hao2023toolkengpt,qian2023creator,song2023restgpt,zhuang2023toolqa,gao2023assistgpt}.
Gaining access to external tools endows LLMs with real-time factual knowledge~\citep{yang2023chatgpt}, multimodal functionalities~\citep{gupta2023visual}, and specialized skills in vertical domains~\citep{jin2023genegpt}. However, open-source LLMs still lag far behind SOTA LLMs in tool use, and how tool-use ability is acquired by SOTA LLMs remains unclear. In this paper, we aim to bridge this gap and fathom the underlying mechanism.

\textbf{Instruction Tuning} \quad
% Prior efforts have demonstrated that supervised fine-tuning on high-quality instruction tuning data facilitates generalization on diverse unseen instructions
Instruction tuning enhances LLMs in understanding human instructions and generating proper responses~\citep{wei2021finetuned,bach2022promptsource,mishra2022cross}. Since manually annotating instruction tuning data is time-consuming, self-instruct~\citep{wang2022self} proposes to generate high-quality data from SOTA LLMs, which facilitates a recent trend of data curation for multi-turn dialogue~\citep{alpaca,vicuna2023,xu2023wizardlm,penedo2023refinedweb,ding2023enhancing}. However, compared with the dialogue, tool learning is inherently more challenging given the vast diversity of APIs and the complexity of multi-tool instructions. As a result, even GPT-4 often fails to find a valid solution path. However, existing tool-learning dataset~\citep{li2023api,patil2023gorilla,tang2023toolalpaca,xu2023tool} and their construction methods cannot effectively address real human needs as mentioned in \cref{sec:introduction}. Instead, our \ourdata is designed for practical scenarios and improves the previous pipeline for tool-learning data construction. % Besides, we target making LLMs generalize to diverse tool-use scenarios instead of focusing on a particular type of tool~\citep{nakano2021webgpt}, which is less practical.

\textbf{Prompting LLMs for Decision Making} \quad
Prompting facilitates LLMs to decompose high-level tasks into sub-tasks and generate grounded plans~\citep{ahn2022can,huang2022language,huang2022inner,ye2023large}. ReACT~\citep{yao2022react} integrates reasoning with acting by allowing LLMs to give a proper reason for an action and incorporating environmental feedback for reasoning. However, these studies do not incorporate a mechanism for decision retraction, which becomes problematic as an initial error can lead to a cascade of subsequent errors. Recently, Reflexion~\citep{shinn2023reflexion} mitigates this issue by asking LLMs to reflect on previous failures. Our \dfs extends Reflexion to a more general method by allowing LLMs to assess different reasoning paths and select the most promising one. It should be noted \dfs shares a similar idea to a concurrent work: tree-of-thought (ToT) reasoning~\citep{yao2023tree}. However, our \dfs targets general decision-making problems where the decision space is \textit{infinite}, compared to ToT's relatively simple tasks that can be addressed by brute-force search, such as Game of 24 and Crosswords. The distinct target between \dfs and ToT determines the significant difference in the implementation details.


% We compare our \dfs with ToT in \cref{sec:compare_dfs_tot}.