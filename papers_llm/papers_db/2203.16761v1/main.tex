% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[table]{xcolor}
\usepackage{comment}
\usepackage{tabularx}

% \usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\def\mathbi#1{\textbf{\em #1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\algblockdefx{FORALLP}{ENDFAP}[1]%
  {\textbf{for all }#1 \textbf{do in parallel}}%
  {\textbf{end for}}
\algblockdefx{FUNCDO}{ENDFUNCDO}[1]%
  {#1 \textbf{do}}%
  {\textbf{end}}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{***} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{MeMOT: Multi-Object Tracking with Memory}

\author{
    Jiarui Cai$^1$\thanks{The work was done during an Amazon internship.} \hspace{0.35cm} Mingze Xu$^2$\thanks{Corresponding Author.} \hspace{0.25cm} Wei Li$^2$ \hspace{0.25cm} Yuanjun Xiong$^2$ \hspace{0.25cm} Wei Xia$^2$ \hspace{0.25cm} Zhuowen Tu$^2$ \hspace{0.25cm} Stefano Soatto$^2$ \\ [.5ex] $^1$University of Washington \hspace{0.9cm} $^2$AWS AI Labs \\ [.5ex]
    {\tt\small jrcai@uw.edu, \{xumingze,wayl,yuanjx,wxia,ztu,soattos\}@amazon.com}
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
    We propose an online tracking algorithm that performs the object detection and data association under a common framework, capable of linking objects after a long time span. This is realized by preserving a large spatio-temporal memory to store the identity embeddings of the tracked objects, and by adaptively referencing and aggregating useful information from the memory as needed. Our model, called MeMOT, consists of three main modules that are all Transformer-based: 1) Hypothesis Generation that produce object proposals in the current video frame; 2) Memory Encoding that extracts the core information from the memory for each tracked object; and 3) Memory Decoding that solves the object detection and data association tasks simultaneously for multi-object tracking. When evaluated on widely adopted MOT benchmark datasets, MeMOT observes very competitive performance.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{introduction}
\input{related_work}
\input{our_approach}
\input{experiments}
\input{conclusion}
\input{appendix}


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
