{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import os\n",
    "from pprint import pprint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "import tool_pool as tp\n",
    "\n",
    "dotenv_path = '/Users/minkyuramen/Desktop/project/env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# load tool\n",
    "tools = [tp.loadpaper, tp.recommendpaper, tp.loadfigure]\n",
    "# load Agent prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM that will drive the agent\n",
    "# Only certain models support this\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"explain about the math expression about the Filtering api calls in the 3 page of the paper 'Language Models Can Teach Themselves to Use Tools' ÌïúÍ∏ÄÎ°ú ÎßêÌï¥Ï§ò\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"summary the method of the paper 'Large Language Model Connected with Massive APIs' ÌïúÍ∏ÄÎ°ú ÎßêÌï¥Ï§ò\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recompendpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"recommend the future works of the paper 'Facilitating Large Language Models to Master 16000+ Real-world APIs' ÌïúÍ∏ÄÎ°ú ÎßêÌï¥Ï§ò\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"Could you recommend previous works to read before this papaer : 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs' ÌïúÍ∏ÄÎ°ú ÎßêÌï¥Ï§ò\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadfigure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"explain about the 'Figure 2: Pass Rate evaluation in ToolBench paper.' in the 3 page of the paper 'StableToolBench' ÌïúÍ∏ÄÎ°ú ÎßêÌï¥Ï§ò\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory in Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "# prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "memory = ChatMessageHistory(session_id=\"test-session\")\n",
    "# message_history = RedisChatMessageHistory(\n",
    "#     url=\"redis://127.0.0.1:6379/0\", ttl=600, session_id=\"my-session\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    lambda session_id: memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Summary the abstract of the paper 'EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS'\"},\n",
    "    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Ï†Ä ÎÖºÎ¨∏ÏóêÏÑú \\n ÌÜ†ÌÅ∞ÏùÑ 4Î≤à ÎÑ£ÏúºÎ©¥ÏÑú ÌôïÏù∏ÌïòÍ≥† Ïã∂ÏóàÎçò Í≤å Î≠êÏïº?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Ïù¥ ÎÖºÎ¨∏Ïùò ÌõÑÏÜç Ïó∞Íµ¨Î•º ÏïåÎ†§Ï§ò ÏµúÏã†ÏàúÏúºÎ°ú\"},\n",
    "    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Paper_assistant(message, session_id = 'temp'):\n",
    "    output = agent_with_chat_history.invoke(\n",
    "        {\"input\": message},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    display(Markdown(output['output']))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paper_assistant('PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION ÌõÑÏÜç ÎÖºÎ¨∏ Ï∂îÏ≤úÌï¥Ï§ò')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n",
      "title\n",
      "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Problem Definition\n",
      "  3 Methods\n",
      "    3.1 Belief-aware Decision-Making Process\n",
      "    3.2 Policy-Level Reflection\n",
      "    3.3 DFS-based Policy Evolution\n",
      "  4 Game: Blackjack\n",
      "    4.1 Results\n",
      "    4.2 Analyisis\n",
      "  5 Game: Limit Texas Hold‚Äôem\n",
      "    5.1 Results\n",
      "    5.2 Analysis on Learning Process\n",
      "    5.3 Analysis on Policy Evolution\n",
      "  6 Conclusion\n",
      "  Limitations\n",
      "  References\n",
      "  Appendix A Experiment Details\n",
      "    A.1 LLMs\n",
      "    A.2 Baselines\n",
      "    A.3 Setups For Blackjack\n",
      "    A.4 Detailed Setup For Texas Hold‚Äôem\n",
      "  Appendix B Introduction of Two Games\n",
      "    B.1 Blackjack\n",
      "    B.2 Limit Texas Hold‚Äôem\n",
      "    B.3 The Challenging of two Games\n",
      "  Appendix C Complementary Experiments\n",
      "    C.1 Whether Few-shot Learning Can Handle Such Complex Interaction\n",
      "    C.2 Policy-Level Reflection without Belief\n",
      "    C.3 Detailed Analysis Experiments\n",
      "  Appendix D Related Works\n",
      "    D.1 LLM-based Application\n",
      "    D.2 LLMs For Interactive Scenarios\n",
      "    D.3 Improving the Quality of LLM Responses\n",
      "    D.4 Automatic Prompt Optimization\n",
      "  Appendix E Detailed Prompts\n",
      "    E.1 Baseline‚Äôs Prompts For Blackjack\n",
      "    E.2 Agent-Pro‚Äôs Prompt For Blackjack\n",
      "    E.3 Baseline‚Äôs Prompts For Limited Texas Hold‚Äôem\n",
      "    E.4 Agent-Pro‚Äôs Prompt For Limited Texas Hold‚Äôem\n",
      "  Appendix F Case Study\n",
      "    F.1 Cases For Blackjack\n",
      "    F.2 Cases For Texas Hold‚Äôem\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': ['Abstract']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAbstractLarge Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, ‚Äùfine-tuning‚Äù its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold‚Äôem, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications 111Code is in https://github.com/zwq2018/Agent-Pro.\u001b[0m\u001b[32;1m\u001b[1;3m**Abstract Summary:**\n",
      "\n",
      "Large Language Models (LLMs) are powerful problem solvers but are often designed for specific tasks using complex prompt engineering. These task-specific designs limit their ability to handle dynamic scenarios like large interactive games. To address this, the paper introduces Agent-Pro, an LLM-based agent that evolves its policy through interactions. Agent-Pro uses a dynamic belief generation and reflection process for policy evolution, focusing on policy-level rather than action-level reflection. It employs depth-first search for policy optimization, leading to continuous improvement. Evaluated in Blackjack and Texas Hold‚Äôem, Agent-Pro outperforms standard LLMs and specialized models, demonstrating its ability to learn and adapt in complex environments.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Abstract Summary:**\n",
       "\n",
       "Large Language Models (LLMs) are powerful problem solvers but are often designed for specific tasks using complex prompt engineering. These task-specific designs limit their ability to handle dynamic scenarios like large interactive games. To address this, the paper introduces Agent-Pro, an LLM-based agent that evolves its policy through interactions. Agent-Pro uses a dynamic belief generation and reflection process for policy evolution, focusing on policy-level rather than action-level reflection. It employs depth-first search for policy optimization, leading to continuous improvement. Evaluated in Blackjack and Texas Hold‚Äôem, Agent-Pro outperforms standard LLMs and specialized models, demonstrating its ability to learn and adapt in complex environments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\"Learning to Evolve via Policy-Level Reflection and Optimization\" ÎÖºÎ¨∏ Ï¥àÎ°ù ÏöîÏïΩÌï¥',\n",
       " 'chat_history': [],\n",
       " 'output': '**Abstract Summary:**\\n\\nLarge Language Models (LLMs) are powerful problem solvers but are often designed for specific tasks using complex prompt engineering. These task-specific designs limit their ability to handle dynamic scenarios like large interactive games. To address this, the paper introduces Agent-Pro, an LLM-based agent that evolves its policy through interactions. Agent-Pro uses a dynamic belief generation and reflection process for policy evolution, focusing on policy-level rather than action-level reflection. It employs depth-first search for policy optimization, leading to continuous improvement. Evaluated in Blackjack and Texas Hold‚Äôem, Agent-Pro outperforms standard LLMs and specialized models, demonstrating its ability to learn and adapt in complex environments.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paper_assistant('\"Learning to Evolve via Policy-Level Reflection and Optimization\" ÎÖºÎ¨∏ Ï¥àÎ°ù ÏöîÏïΩÌï¥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': []}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSection list was incorrect.\n",
      "Here is the title and section of the paper\n",
      "title\n",
      "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Problem Definition\n",
      "  3 Methods\n",
      "    3.1 Belief-aware Decision-Making Process\n",
      "    3.2 Policy-Level Reflection\n",
      "    3.3 DFS-based Policy Evolution\n",
      "  4 Game: Blackjack\n",
      "    4.1 Results\n",
      "    4.2 Analyisis\n",
      "  5 Game: Limit Texas Hold‚Äôem\n",
      "    5.1 Results\n",
      "    5.2 Analysis on Learning Process\n",
      "    5.3 Analysis on Policy Evolution\n",
      "  6 Conclusion\n",
      "  Limitations\n",
      "  References\n",
      "  Appendix A Experiment Details\n",
      "    A.1 LLMs\n",
      "    A.2 Baselines\n",
      "    A.3 Setups For Blackjack\n",
      "    A.4 Detailed Setup For Texas Hold‚Äôem\n",
      "  Appendix B Introduction of Two Games\n",
      "    B.1 Blackjack\n",
      "    B.2 Limit Texas Hold‚Äôem\n",
      "    B.3 The Challenging of two Games\n",
      "  Appendix C Complementary Experiments\n",
      "    C.1 Whether Few-shot Learning Can Handle Such Complex Interaction\n",
      "    C.2 Policy-Level Reflection without Belief\n",
      "    C.3 Detailed Analysis Experiments\n",
      "  Appendix D Related Works\n",
      "    D.1 LLM-based Application\n",
      "    D.2 LLMs For Interactive Scenarios\n",
      "    D.3 Improving the Quality of LLM Responses\n",
      "    D.4 Automatic Prompt Optimization\n",
      "  Appendix E Detailed Prompts\n",
      "    E.1 Baseline‚Äôs Prompts For Blackjack\n",
      "    E.2 Agent-Pro‚Äôs Prompt For Blackjack\n",
      "    E.3 Baseline‚Äôs Prompts For Limited Texas Hold‚Äôem\n",
      "    E.4 Agent-Pro‚Äôs Prompt For Limited Texas Hold‚Äôem\n",
      "  Appendix F Case Study\n",
      "    F.1 Cases For Blackjack\n",
      "    F.2 Cases For Texas Hold‚Äôem\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': ['3.1 Belief-aware Decision-Making Process']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.1 Belief-aware Decision-Making Process\n",
      "To develop an LLM-based agent better suited for interactive environments, we draw inspiration from the Theory of Mind (ToM)¬†(Premack and Woodruff, 1978; Li et¬†al., 2023b; Guo et¬†al., 2023a). In this framework, human condenses perceptions of themselves (self-belief) and the external environment (social-belief) and then ground their decisions on these beliefs, or adjust incorrect beliefs in response to external feedback. We also design a belief-aware decision-making process for Agent-Pro, simulating human cognitive processes in social contexts.\n",
      "\n",
      "First, we need to define the policy of an LLM-based agent, which refers to a specific behavioral strategy guiding the agent to interact and complete tasks. It often involves complex prompts designed by experts, covering task rules, strategies, and output formats. In a zero-sum game with K‚Äã+‚Äã1ùêæ+1K\\text{+}1 players (assuming playing order is (op1,our,op2,..,opK)(op_{1},our,op_{2},..,op_{K})), we denote the policy of our agent as œÄùúã\\pi with some observable information, containing agent‚Äôs private information stsubscriptùë†ùë°{\\color[rgb]{1,0,0}s_{t}}, public information otsubscriptùëúùë°{\\color[rgb]{0,0,1}o_{t}}, our own action atsubscriptùëéùë°{\\color[rgb]{0,1,1}a_{t}}, and the actions of all opponents ato‚Äãp1,ato‚Äãp2,..,ato‚ÄãpKa^{op_{1}}_{t},a^{op_{2}}_{t},..,a^{op_{K}}_{t}, where tùë°t means tùë°t-th rounds of a game. Therefore a complete game trajectory spanning tùë°t rounds:\n",
      "\n",
      "‚Ñã0:t={\\displaystyle\\mathcal{H}_{0:t}=\\{\n",
      "(s0,o0,a0o‚Äãp1,a0,a0o‚Äãp2,‚Ä¶,a0o‚ÄãpK),subscriptùë†0subscriptùëú0subscriptsuperscriptùëéùëúsubscriptùëù10subscriptùëé0subscriptsuperscriptùëéùëúsubscriptùëù20‚Ä¶subscriptsuperscriptùëéùëúsubscriptùëùùêæ0\\displaystyle({\\color[rgb]{1,0,0}s_{0}},{\\color[rgb]{0,0,1}o_{0}},a^{op_{1}}_{0},{\\color[rgb]{0,1,1}a_{0}},a^{op_{2}}_{0},...,a^{op_{K}}_{0}),\n",
      "\n",
      "(1)\n",
      "\n",
      "‚ãÆ‚ãÆ\\displaystyle\\quad\\quad\\quad\\quad\\quad\\vdots\n",
      "\n",
      "(st,ot,ato‚Äãp1,at,ato‚Äãp2,‚Ä¶,ato‚ÄãpK)}\\displaystyle({\\color[rgb]{1,0,0}s_{t}},{\\color[rgb]{0,0,1}o_{t}},a^{op_{1}}_{t},{\\color[rgb]{0,1,1}a_{t}},a^{op_{2}}_{t},...,a^{op_{K}}_{t})\\}\n",
      "\n",
      "As shown in Figure¬†2, when making a decision, Agent-Pro first generates a dynamic belief Œæùúâ{\\color[rgb]{.75,0,.25}\\xi} about itself (self-belief) and opponents (world-belief) in natural language. Then, it predicts an action based on the latest beliefs. For instance, for Texas Hold‚Äôem, Agent-Pro‚Äôs understanding of its hand cards, plan, and potential risk constitutes its self-belief, while the conjectures about the opponents form its world-belief. These beliefs are updated in each decision-making cycle. Equipped with this, Agent-Pro can generate more coherent and consistent actions:\n",
      "\n",
      "Œæt+1,at+1‚àºœÄ‚Äã(‚Ñã0:t,st+1,ot+1,at+1o‚Äãp1,Œæt)similar-tosubscriptùúâùë°1subscriptùëéùë°1ùúãsubscript‚Ñã:0ùë°subscriptùë†ùë°1subscriptùëúùë°1subscriptsuperscriptùëéùëúsubscriptùëù1ùë°1subscriptùúâùë°\\displaystyle{\\color[rgb]{.75,0,.25}\\xi_{t+1}},{\\color[rgb]{0,1,1}a_{t+1}}\\sim\\pi(\\mathcal{H}_{0:t},{\\color[rgb]{1,0,0}s_{t+1}},{\\color[rgb]{0,0,1}o_{t+1}},a^{op_{1}}_{t+1},{\\color[rgb]{.75,0,.25}\\xi_{t}})\n",
      "\n",
      "(2)\n",
      "\n",
      "When a game is over, we acquire the observable state RùëÖR (e.g., private hand cards after showdown) and the final scores SùëÜS of all players. The objective is to find an optimal œÄ‚àósuperscriptùúã\\pi^{*} to maximize S‚Äã(o‚Äãu‚Äãr)ùëÜùëúùë¢ùëüS(our).\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': ['3.2 Policy-Level Reflection']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.2 Policy-Level Reflection\n",
      "Equipped with an initial policy (a simple prompt) and a dynamic belief, Agent-Pro already possesses basic capabilities for game exploration. To further enhance Agent-Pro‚Äôs capabilities, we design a learning mechanism via a policy-level reflection.\n",
      "\n",
      "Specifically, many text-based tasks have employed reflection strategies and immediate environmental feedback to correct prior actions. However, in many typical interaction scenarios with longer decision-making processes, action-level reflections are not directly applicable due to delayed feedback. Therefore, for such a long-horizon interaction process, Agent-Pro is instructed to focus on the rationality of beliefs and underlying behavioral policies rather than individual actions.\n",
      "\n",
      "Belief Calibration As depicted in¬†Figure¬†2, under the guidance of the current behavior policy, Agent-Pro generates actions based on self-belief and world-belief. If these beliefs are inaccurate, they may lead to irrational actions and eventual failure. Therefore, Agent-Pro examines the rationality of these beliefs based on the final results and reflectss on the reasons for the final failure.\n",
      "\n",
      "‚¨á\n",
      "\n",
      "Correctness: Whether its beliefs about itself, the game, and its opponents align with the final results.\n",
      "\n",
      "Consistency: Whether each belief and action is self-contradictory.\n",
      "\n",
      "Rationality: Whether the beliefs accurately reflect the underlying intentions behind the opponents.\n",
      "\n",
      "Reasons: Reflect on why it lost to its opponents, which beliefs are problematic, and what the underlying reasons are.\n",
      "\n",
      "Lastly, to calibrate the incorrect beliefs, Agent-Pro summarizes these reflections and analyses about itself and the external world into specific instructions: Behavioral Guideline and World Modeling, where the former represents generalized behavioral strategies for this task, and the latter signifies its understanding and conjectures about the game world. For instance, in Texas Hold‚Äôem, Agent-Pro summarizes the following contents:\n",
      "\n",
      "‚¨á\n",
      "\n",
      "Behavioral Guideline\n",
      "\n",
      "1-Please summarize a detailed goal based on your reflection on beliefs. {Goal}\n",
      "\n",
      "2-What strategy helps you build correct belief and win at similar.. {Strategy}\n",
      "\n",
      "3-Can this game be considered a typical example for future... {Demonstration}\n",
      "\n",
      "World Modeling\n",
      "\n",
      "1-Accurately model each player to help build more precise beliefs about them, including action, and style.{Opponent}\n",
      "\n",
      "2-Describe any game rules or details that are easy to overlook...{Rule}\n",
      "\n",
      "Agent-Pro summarizes high-level strategies within the Behavioral Guideline and describes the task and opponents in World Modeling. These instructions can calibrate previous incorrect beliefs and improve policy performance. The entire process can be formalized as follows:\n",
      "\n",
      "Instructionn+1superscriptInstructionùëõ1\\displaystyle\\text{Instruction}^{n+1}\n",
      "‚ÜêLLM(‚Ñã0:Tn,{Œæ1n,Œæ2n..},Rn,Sn)\\displaystyle\\leftarrow\\!\\text{LLM}\\left(\\mathcal{H}^{n}_{0:T},\\!\\{{\\color[rgb]{.75,0,.25}\\xi^{n}_{1},\\xi^{n}_{2}}..\\},\\!R^{n},\\!S^{n}\\right)\n",
      "\n",
      "(3)\n",
      "\n",
      "where ‚Ñã0:Tnsubscriptsuperscript‚Ñãùëõ:0ùëá\\mathcal{H}^{n}_{0:T} denotes a complete trajectory at the nùëõn-th match, {Œæ1n,Œæ2n,‚Ä¶}superscriptsubscriptùúâ1ùëõsuperscriptsubscriptùúâ2ùëõ‚Ä¶\\{{\\color[rgb]{.75,0,.25}\\xi_{1}^{n}},{\\color[rgb]{.75,0,.25}\\xi_{2}^{n}},...\\} denotes the belief sequence, RnsuperscriptùëÖùëõR^{n} and SnsuperscriptùëÜùëõS^{n} means the final results and score. Instructionn+1superscriptInstructionùëõ1\\text{Instruction}^{n+1} denotes new generated Behavioral Guideline and World Modeling.\n",
      "\n",
      "Verification After extracting these instructions, Agent-Pro verifies its efficacy. Agent-Pro incorporates these generated Behavioral Guideline and World Modeling into the prompt and then replays the same game again, i.e., the same opponents and initial conditions. If the final score improves, we retain them in the prompt. Otherwise, we regenerate a new one. If it fails to pass verification after three retries, we discard this trajectory ‚Ñãnsuperscript‚Ñãùëõ\\mathcal{H}^{n}:\n",
      "\n",
      "œÄn+1superscriptùúãùëõ1\\displaystyle\\pi^{n+1}\n",
      "‚ÜêV‚Äãe‚Äãr‚Äãi‚Äãf‚ÄãyœÄn‚à™Instructionn+1ùëâùëíùëüùëñùëìùë¶‚ÜêabsentsuperscriptùúãùëõsuperscriptInstructionùëõ1\\displaystyle\\!\\xleftarrow{Verify}\\!\\pi^{n}\\cup\\text{Instruction}^{n+1}\n",
      "\n",
      "(4)\n",
      "\n",
      "where ‚à™\\cup means incorporates new instructions into the previous prompt for œÄn+1superscriptùúãùëõ1\\pi^{n+1}. This new policy encompasses more effective instructions, empowering Agent-Pro to establish accurate self- and world beliefs and generate more rational actions.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': ['3.3 DFS-based Policy Evolution']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.3 DFS-based Policy Evolution\n",
      "To iteratively update the policy, we devise a policy optimization process based on depth-first search (DFS). It encompasses a policy evaluation process to assess the generalization ability of the new policy in novel game scenarios and a search mechanism to progressively find a better policy.\n",
      "\n",
      "Policy Evaluation Each time the policy is updated, Agent-Pro is required to evaluate the new strategies. This evaluation process is distinct from the previous Verification step, as the Verification repeatedly utilizes the ‚Äùtraining‚Äù data for evaluation and can not ensure the generalizability of the new policy. Hence, Agent-Pro conducts a thorough assessment of the new policy in novel trajectories. Besides, it is imperative to eliminate the influence of random factors when policy evaluation, e.g., a poor initial hand due to bad luck or an unfavorable playing order.\n",
      "\n",
      "Therefore, we first randomly generate a new game for K‚Äã+‚Äã1ùêæ+1K\\text{+}1 players. Then we sequentially swap both the hand cards and the playing order of each player, generating a total of (K‚Äã+‚Äã1)2superscriptùêæ+12(K\\text{+}1)^{2} combinations. To eliminate randomness, we concurrently use these (K‚Äã+‚Äã1)2superscriptùêæ+12(K\\text{+}1)^{2} games to evaluate Agent-Pro‚Äôs new policy. We calculate the average score over the (K‚Äã+‚Äã1)2superscriptùêæ+12(K\\text{+}1)^{2} games for each player. Since the influences of hand-card quality and playing order are mitigated, the average score of all combinations can represent the true capabilities of each player. Lastly, we calculate the evaluating metrics:\n",
      "\n",
      "Œî=1(K‚Äã+‚Äã1)2‚Äã‚àëj(K+1)2[ùíÆj‚Äã(our)‚àímaxi‚Å°ùíÆj‚Äã(opi)]Œî1superscriptùêæ+12superscriptsubscriptùëósuperscriptùêæ12delimited-[]subscriptùíÆùëóoursubscriptùëñsubscriptùíÆùëósubscriptopùëñ\\displaystyle\\Delta\\!=\\!\\frac{1}{(K\\text{+}1)^{2}}\\!\\!\\sum_{j}^{(K+1)^{2}}\\!\\!\\!\\left[\\mathcal{S}_{j}(\\text{our})\\!-\\!\\max_{i}\\mathcal{S}_{j}(\\text{op}_{i})\\right]\n",
      "\n",
      "(5)\n",
      "\n",
      "where i‚àà{1,‚Ä¶,K}ùëñ1‚Ä¶ùêæi\\in\\{1,\\ldots,K\\} denotes the index of an opponent, and jùëój denotes the index of the games within (K‚Äã+‚Äã1)2superscriptùêæ+12(K\\text{+}1)^{2} combinations. The ŒîŒî\\Delta assesses both the absolute gains of the policy and its gains relative to the strongest opponent, providing a comprehensive evaluation in multiplayer gaming scenarios.\n",
      "\n",
      "Policy Search\n",
      "Inevitably, sometimes the new policy does not bring an improvement in ŒîŒî\\Delta in the new scenario. In such cases, we employ DFS to search for a better policy from other branches (i.e., other candidate policies). As shown in¬†Figure¬†2, when updating old policy œÄnsuperscriptùúãùëõ\\pi^{n}, we generate BùêµB candidate policies {œÄ1n+1\\{\\pi^{n+1}_{1}, œÄ2n+1subscriptsuperscriptùúãùëõ12\\pi^{n+1}_{2},‚Ä¶, œÄBn+1}\\pi^{n+1}_{B}\\}, forming BùêµB branches. Then, we first calculate Œî1n+1subscriptsuperscriptŒîùëõ11\\Delta^{n+1}_{1} for new policy œÄ1n+1subscriptsuperscriptùúãùëõ11\\pi^{n+1}_{1} and compare it with ŒînsuperscriptŒîùëõ\\Delta^{n}. If Œî1n+1subscriptsuperscriptŒîùëõ11\\Delta^{n+1}_{1} is greater than ŒînsuperscriptŒîùëõ\\Delta^{n}, we accept this evolutionary. Otherwise, we reject œÄ1n+1subscriptsuperscriptùúãùëõ11\\pi^{n+1}_{1} and consider œÄ2n+1subscriptsuperscriptùúãùëõ12\\pi^{n+1}_{2}. If none of the BùêµB candidate policies œÄn+1superscriptùúãùëõ1\\pi^{n+1} enhance Agent-Pro‚Äôs performance, we backtrack to œÄnsuperscriptùúãùëõ\\pi^{n} and consider its sibling nodes œÄ2nsubscriptsuperscriptùúãùëõ2\\pi^{n}_{2}. Similarly, Agent-Pro explores the environment using œÄ2nsubscriptsuperscriptùúãùëõ2\\pi^{n}_{2}, then also updates BùêµB candidate policies and searches in a depth-first manner. Ultimately, we select the policy with the highest ŒîŒî\\Delta across the entire policy tree.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m### Methods Summary\n",
      "\n",
      "#### 3.1 Belief-aware Decision-Making Process\n",
      "- **Inspiration**: Draws from the Theory of Mind (ToM), where humans form beliefs about themselves (self-belief) and the environment (social-belief) to guide decisions.\n",
      "- **Policy Definition**: The policy of an LLM-based agent involves complex prompts covering task rules, strategies, and output formats.\n",
      "- **Dynamic Belief Generation**: Agent-Pro generates dynamic beliefs about itself and opponents in natural language, updating these beliefs in each decision-making cycle.\n",
      "- **Decision Making**: Actions are predicted based on the latest beliefs, aiming for coherent and consistent actions.\n",
      "- **Objective**: To find an optimal policy that maximizes the agent's score.\n",
      "\n",
      "#### 3.2 Policy-Level Reflection\n",
      "- **Initial Capabilities**: Agent-Pro starts with a basic policy and dynamic belief for game exploration.\n",
      "- **Reflection Mechanism**: Focuses on the rationality of beliefs and underlying behavioral policies rather than individual actions.\n",
      "- **Belief Calibration**: Agent-Pro examines the correctness, consistency, and rationality of its beliefs based on final results and reflects on the reasons for failure.\n",
      "- **Instruction Generation**: Summarizes reflections into Behavioral Guidelines and World Modeling to calibrate incorrect beliefs and improve policy performance.\n",
      "- **Verification**: Incorporates new instructions into the prompt and replays the same game to verify improvements. If the score improves, the instructions are retained; otherwise, they are regenerated or discarded after three retries.\n",
      "\n",
      "#### 3.3 DFS-based Policy Evolution\n",
      "- **Policy Evaluation**: Evaluates new policies in novel game scenarios to ensure generalizability, eliminating the influence of random factors by generating multiple game combinations.\n",
      "- **Evaluation Metrics**: Calculates the average score over all combinations to represent true capabilities and assesses both absolute gains and relative gains to the strongest opponent.\n",
      "- **Policy Search**: Uses depth-first search (DFS) to explore and update policies. Generates multiple candidate policies and selects the one with the highest performance improvement. If no improvement is found, backtracks to consider sibling nodes and continues the search.\n",
      "\n",
      "These methods collectively enable Agent-Pro to evolve its policy through dynamic belief generation, reflection, and optimization, allowing it to adapt and improve in complex interactive environments.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Methods Summary\n",
       "\n",
       "#### 3.1 Belief-aware Decision-Making Process\n",
       "- **Inspiration**: Draws from the Theory of Mind (ToM), where humans form beliefs about themselves (self-belief) and the environment (social-belief) to guide decisions.\n",
       "- **Policy Definition**: The policy of an LLM-based agent involves complex prompts covering task rules, strategies, and output formats.\n",
       "- **Dynamic Belief Generation**: Agent-Pro generates dynamic beliefs about itself and opponents in natural language, updating these beliefs in each decision-making cycle.\n",
       "- **Decision Making**: Actions are predicted based on the latest beliefs, aiming for coherent and consistent actions.\n",
       "- **Objective**: To find an optimal policy that maximizes the agent's score.\n",
       "\n",
       "#### 3.2 Policy-Level Reflection\n",
       "- **Initial Capabilities**: Agent-Pro starts with a basic policy and dynamic belief for game exploration.\n",
       "- **Reflection Mechanism**: Focuses on the rationality of beliefs and underlying behavioral policies rather than individual actions.\n",
       "- **Belief Calibration**: Agent-Pro examines the correctness, consistency, and rationality of its beliefs based on final results and reflects on the reasons for failure.\n",
       "- **Instruction Generation**: Summarizes reflections into Behavioral Guidelines and World Modeling to calibrate incorrect beliefs and improve policy performance.\n",
       "- **Verification**: Incorporates new instructions into the prompt and replays the same game to verify improvements. If the score improves, the instructions are retained; otherwise, they are regenerated or discarded after three retries.\n",
       "\n",
       "#### 3.3 DFS-based Policy Evolution\n",
       "- **Policy Evaluation**: Evaluates new policies in novel game scenarios to ensure generalizability, eliminating the influence of random factors by generating multiple game combinations.\n",
       "- **Evaluation Metrics**: Calculates the average score over all combinations to represent true capabilities and assesses both absolute gains and relative gains to the strongest opponent.\n",
       "- **Policy Search**: Uses depth-first search (DFS) to explore and update policies. Generates multiple candidate policies and selects the one with the highest performance improvement. If no improvement is found, backtracks to consider sibling nodes and continues the search.\n",
       "\n",
       "These methods collectively enable Agent-Pro to evolve its policy through dynamic belief generation, reflection, and optimization, allowing it to adapt and improve in complex interactive environments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'ÎÖºÎ¨∏ MethodÏùò Í∞Å ÏöîÏÜåÎ•º ÏöîÏïΩÌï¥Ï§ò',\n",
       " 'chat_history': [HumanMessage(content='\"Learning to Evolve via Policy-Level Reflection and Optimization\" ÎÖºÎ¨∏ Ï¥àÎ°ù ÏöîÏïΩÌï¥'),\n",
       "  AIMessage(content='**Abstract Summary:**\\n\\nLarge Language Models (LLMs) are powerful problem solvers but are often designed for specific tasks using complex prompt engineering. These task-specific designs limit their ability to handle dynamic scenarios like large interactive games. To address this, the paper introduces Agent-Pro, an LLM-based agent that evolves its policy through interactions. Agent-Pro uses a dynamic belief generation and reflection process for policy evolution, focusing on policy-level rather than action-level reflection. It employs depth-first search for policy optimization, leading to continuous improvement. Evaluated in Blackjack and Texas Hold‚Äôem, Agent-Pro outperforms standard LLMs and specialized models, demonstrating its ability to learn and adapt in complex environments.')],\n",
       " 'output': \"### Methods Summary\\n\\n#### 3.1 Belief-aware Decision-Making Process\\n- **Inspiration**: Draws from the Theory of Mind (ToM), where humans form beliefs about themselves (self-belief) and the environment (social-belief) to guide decisions.\\n- **Policy Definition**: The policy of an LLM-based agent involves complex prompts covering task rules, strategies, and output formats.\\n- **Dynamic Belief Generation**: Agent-Pro generates dynamic beliefs about itself and opponents in natural language, updating these beliefs in each decision-making cycle.\\n- **Decision Making**: Actions are predicted based on the latest beliefs, aiming for coherent and consistent actions.\\n- **Objective**: To find an optimal policy that maximizes the agent's score.\\n\\n#### 3.2 Policy-Level Reflection\\n- **Initial Capabilities**: Agent-Pro starts with a basic policy and dynamic belief for game exploration.\\n- **Reflection Mechanism**: Focuses on the rationality of beliefs and underlying behavioral policies rather than individual actions.\\n- **Belief Calibration**: Agent-Pro examines the correctness, consistency, and rationality of its beliefs based on final results and reflects on the reasons for failure.\\n- **Instruction Generation**: Summarizes reflections into Behavioral Guidelines and World Modeling to calibrate incorrect beliefs and improve policy performance.\\n- **Verification**: Incorporates new instructions into the prompt and replays the same game to verify improvements. If the score improves, the instructions are retained; otherwise, they are regenerated or discarded after three retries.\\n\\n#### 3.3 DFS-based Policy Evolution\\n- **Policy Evaluation**: Evaluates new policies in novel game scenarios to ensure generalizability, eliminating the influence of random factors by generating multiple game combinations.\\n- **Evaluation Metrics**: Calculates the average score over all combinations to represent true capabilities and assesses both absolute gains and relative gains to the strongest opponent.\\n- **Policy Search**: Uses depth-first search (DFS) to explore and update policies. Generates multiple candidate policies and selects the one with the highest performance improvement. If no improvement is found, backtracks to consider sibling nodes and continues the search.\\n\\nThese methods collectively enable Agent-Pro to evolve its policy through dynamic belief generation, reflection, and optimization, allowing it to adapt and improve in complex interactive environments.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paper_assistant('ÎÖºÎ¨∏ MethodÏùò Í∞Å ÏöîÏÜåÎ•º ÏöîÏïΩÌï¥Ï§ò')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
