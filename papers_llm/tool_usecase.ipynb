{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /c1/yumin/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /c1/yumin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import os\n",
    "from pprint import pprint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "import tool_pool as tp\n",
    "\n",
    "dotenv_path = '/Users/minkyuramen/Desktop/project/env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# load tool\n",
    "tools = [tp.loadpaper, tp.recommendpaper, tp.loadfigure]\n",
    "# load Agent prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM that will drive the agent\n",
    "# Only certain models support this\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"explain about the math expression about the Filtering api calls in the 3 page of the paper 'Language Models Can Teach Themselves to Use Tools' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"summary the method of the paper 'Large Language Model Connected with Massive APIs' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recompendpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"recommend the future works of the paper 'Facilitating Large Language Models to Master 16000+ Real-world APIs' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"Could you recommend previous works to read before this papaer : 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadfigure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"explain about the 'Figure 2: Pass Rate evaluation in ToolBench paper.' in the 3 page of the paper 'StableToolBench' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory in Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "# prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "memory = ChatMessageHistory(session_id=\"test-session\")\n",
    "# message_history = RedisChatMessageHistory(\n",
    "#     url=\"redis://127.0.0.1:6379/0\", ttl=600, session_id=\"my-session\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    lambda session_id: memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS', 'sections': []}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSection list was incorrect.\n",
      "Here is the title and section of the paper\n",
      "title\n",
      "Efficient Streaming Language Models \n",
      "with Attention Sinks\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Related Work\n",
      "  3 StreamingLLM\n",
      "    3.1 The Failure of Window Attention and Attention Sinks\n",
      "      Identifying the Point of Perplexity Surge.\n",
      "      Why do LLMs break when removing initial tokens’ KV?\n",
      "      LLMs attend to Initial Tokens as Attention Sinks.\n",
      "    3.2 Rolling KV Cache with Attention Sinks\n",
      "    3.3 Pre-Training LLMs with Attention Sinks\n",
      "  4 Experiments\n",
      "    4.1 Language Modeling on Long Texts Across LLM Families and Scales\n",
      "    4.2 Results of Pre-Training with a Sink Token\n",
      "      Convergence and Normal Model Performance.\n",
      "      Streaming Performance.\n",
      "      Attention Visualization.\n",
      "    4.3 Results on Streaming Question Answering with Instruction-tuned Models\n",
      "    4.4 Ablation Studies\n",
      "      Numbers of Initial Tokens.\n",
      "      Cache Sizes.\n",
      "    4.5 Efficency Results\n",
      "  5 Conclusion\n",
      "  Acknowledgements\n",
      "  References\n",
      "  Appendix A Discussions\n",
      "  Appendix B Additional Related Works\n",
      "  Appendix C Accuracy on StreamEval with Increasing Query-Answer Line Distance\n",
      "  Appendix D Long-Range Benchmark Evaluation\n",
      "  Appendix E Llama-2-7B Attention Visualization on Longer Sequences\n",
      "  Appendix F Quatitative Analysis of Attention Sinks in Long Inputs\n",
      "  Appendix G Llama-2-70B Attention Visualization\n",
      "  Appendix H Attention Sinks in Encoder Transformers\n",
      "  Appendix I Using More Sink Tokens in the Pre-Training Stage\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS', 'sections': ['Abstract']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAbstractDeploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges.\n",
      "Firstly, during the decoding stage, caching previous tokens’ Key and Value states (KV) consumes extensive memory.\n",
      "Secondly, popular LLMs cannot generalize to longer texts than the training sequence length.\n",
      "Window attention, where only the most recent KVs are cached, is a natural approach — but we show that it fails when the text length surpasses the cache size.\n",
      "We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a “sink” even if they are not semantically important.\n",
      "Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning.\n",
      "We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n",
      "In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2×\\times speedup.\n",
      "Code and datasets are provided in the link.\u001b[0m\u001b[32;1m\u001b[1;3mThe abstract of the paper \"Efficient Streaming Language Models with Attention Sinks\" highlights the following key points:\n",
      "\n",
      "1. **Challenges in Streaming Applications**: Deploying Large Language Models (LLMs) in streaming applications, such as multi-round dialogues, faces two main challenges:\n",
      "   - High memory consumption due to caching previous tokens' Key and Value states (KV) during decoding.\n",
      "   - Inability of popular LLMs to generalize to longer texts than their training sequence length.\n",
      "\n",
      "2. **Window Attention Limitation**: Window attention, which caches only the most recent KVs, fails when text length exceeds the cache size.\n",
      "\n",
      "3. **Attention Sink Phenomenon**: The paper identifies an \"attention sink\" phenomenon where keeping the KV of initial tokens significantly recovers the performance of window attention, even if these tokens are not semantically important.\n",
      "\n",
      "4. **StreamingLLM Framework**: The authors introduce StreamingLLM, a framework that allows LLMs trained with a finite-length attention window to generalize to infinite sequence lengths without fine-tuning.\n",
      "\n",
      "5. **Performance and Efficiency**: StreamingLLM enables models like Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens. Adding a placeholder token as an attention sink during pre-training further improves streaming deployment.\n",
      "\n",
      "6. **Speedup**: In streaming settings, StreamingLLM achieves up to 22.2× speedup compared to the sliding window recomputation baseline.\n",
      "\n",
      "7. **Resources**: The paper provides code and datasets for further exploration.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Summary the abstract of the paper 'EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS'\"},\n",
    "    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n",
      "title\n",
      "Efficient Streaming Language Models \n",
      "with Attention Sinks\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Related Work\n",
      "  3 StreamingLLM\n",
      "    3.1 The Failure of Window Attention and Attention Sinks\n",
      "      Identifying the Point of Perplexity Surge.\n",
      "      Why do LLMs break when removing initial tokens’ KV?\n",
      "      LLMs attend to Initial Tokens as Attention Sinks.\n",
      "    3.2 Rolling KV Cache with Attention Sinks\n",
      "    3.3 Pre-Training LLMs with Attention Sinks\n",
      "  4 Experiments\n",
      "    4.1 Language Modeling on Long Texts Across LLM Families and Scales\n",
      "    4.2 Results of Pre-Training with a Sink Token\n",
      "      Convergence and Normal Model Performance.\n",
      "      Streaming Performance.\n",
      "      Attention Visualization.\n",
      "    4.3 Results on Streaming Question Answering with Instruction-tuned Models\n",
      "    4.4 Ablation Studies\n",
      "      Numbers of Initial Tokens.\n",
      "      Cache Sizes.\n",
      "    4.5 Efficency Results\n",
      "  5 Conclusion\n",
      "  Acknowledgements\n",
      "  References\n",
      "  Appendix A Discussions\n",
      "  Appendix B Additional Related Works\n",
      "  Appendix C Accuracy on StreamEval with Increasing Query-Answer Line Distance\n",
      "  Appendix D Long-Range Benchmark Evaluation\n",
      "  Appendix E Llama-2-7B Attention Visualization on Longer Sequences\n",
      "  Appendix F Quatitative Analysis of Attention Sinks in Long Inputs\n",
      "  Appendix G Llama-2-70B Attention Visualization\n",
      "  Appendix H Attention Sinks in Encoder Transformers\n",
      "  Appendix I Using More Sink Tokens in the Pre-Training Stage\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS', 'sections': ['3.1 The Failure of Window Attention and Attention Sinks', '3.2 Rolling KV Cache with Attention Sinks']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.1 The Failure of Window Attention and Attention Sinks\n",
      "While the window attention technique offers efficiency during inference, it results in an exceedingly high language modeling perplexity. Consequently, the model’s performance is unsuitable for deployment in streaming applications. In this section, we use the concept of attention sink to explain the failure of window attention, serving as the inspiration behind StreamingLLM.\n",
      "\n",
      "Identifying the Point of Perplexity Surge.\n",
      "\n",
      "Figure 3 shows the perplexity of language modeling on a 20K token text. It is evident that perplexity spikes when the text length surpasses the cache size, led by the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance from the tokens being predicted, are crucial for maintaining the stability of LLMs.\n",
      "\n",
      "Why do LLMs break when removing initial tokens’ KV?\n",
      "\n",
      "We visualize attention maps from all layers and heads of the Llama-2-7B and models in Figure 2. We find that, beyond the bottom two layers, the model consistently focuses on the initial tokens across all layers and heads. The implication is clear: removing these initial tokens’ KV will remove a considerable portion of the denominator in the SoftMax function (Equation 1) in attention computation. This alteration leads to a significant shift in the distribution of attention scores away from what would be expected in normal inference settings.\n",
      "\n",
      "SoftMax​(x)i=exiex1+∑j=2Nexj,x1≫xj,j∈2,…,Nformulae-sequenceSoftMaxsubscript𝑥𝑖superscript𝑒subscript𝑥𝑖superscript𝑒subscript𝑥1superscriptsubscript𝑗2𝑁superscript𝑒subscript𝑥𝑗formulae-sequencemuch-greater-thansubscript𝑥1subscript𝑥𝑗𝑗2…𝑁\\text{SoftMax}(x)_{i}=\\frac{e^{x_{i}}}{e^{x_{1}}+\\sum_{j=2}^{N}e^{x_{j}}},\\quad x_{1}\\gg x_{j},j\\in 2,\\dots,N\n",
      "\n",
      "(1)\n",
      "\n",
      "There are two possible explanations for the importance of the initial tokens in language modeling: (1) Either their semantics are crucial, or\n",
      "(2) the model learns a bias towards their absolute position. To distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four tokens are substituted with the linebreak token “\\n\". The observations indicate that the model still significantly emphasizes these initial linebreak tokens. Furthermore, reintroducing them restores the language modeling perplexity to levels comparable to having the original initial tokens. This suggests that the absolute position of the starting tokens, rather than their semantic value, holds greater significance.\n",
      "\n",
      "LLMs attend to Initial Tokens as Attention Sinks.\n",
      "\n",
      "To explain why the model disproportionately focuses on initial tokens—regardless of their semantic relevance to language modeling, we introduce the concept of “attention sink\". The nature of the SoftMax function (Equation 1) prevents all attended tokens from having zero values. This requires aggregating some information from other tokens across all heads in all layers, even if the current embedding has sufficient self-contained information for its prediction. Consequently, the model tends to dump unnecessary attention values to specific tokens. A similar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko et al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.\n",
      "\n",
      "Table 1: Window attention has poor performance on long text. The perplexity is restored when we reintroduce the initial four tokens alongside the recent 1020 tokens (4+1020). Substituting the original four initial tokens with linebreak tokens “\\n\" (4\"\\n\"+1020) achieves comparable perplexity restoration.\n",
      "Cache config x+y denotes adding x initial tokens with y recent tokens. Perplexities are measured on the first book (65K tokens) in the PG19 test set.\n",
      "\n",
      "Llama-2-13B\n",
      "PPL (↓↓\\downarrow)\n",
      "\n",
      "0 + 1024 (Window)\n",
      "5158.07\n",
      "\n",
      "4 + 1020\n",
      "5.40\n",
      "\n",
      "4\"\\n\"+1020\n",
      "5.60\n",
      "\n",
      "Table 2: Effects of reintroduced initial token numbers on StreamingLLM. (1) Window attention (0+y) has a drastic increase in perplexity. (2) Introducing one or two initial tokens usually doesn’t suffice to fully restore model perplexity, indicating that the model doesn’t solely use the first token as the attention sink.\n",
      "(3) Introducing four initial tokens generally suffices; further additions have diminishing returns. Cache config x+y denotes adding x initial tokens to y recent tokens. Perplexities are evaluated on 400K tokens in the concatenated PG19 test set.\n",
      "\n",
      "Cache Config\n",
      "0+2048\n",
      "1+2047\n",
      "2+2046\n",
      "4+2044\n",
      "8+2040\n",
      "\n",
      "Falcon-7B\n",
      "17.90\n",
      "12.12\n",
      "12.12\n",
      "12.12\n",
      "12.12\n",
      "\n",
      "MPT-7B\n",
      "460.29\n",
      "14.99\n",
      "15.00\n",
      "14.99\n",
      "14.98\n",
      "\n",
      "Pythia-12B\n",
      "21.62\n",
      "11.95\n",
      "12.09\n",
      "12.09\n",
      "12.02\n",
      "\n",
      "Cache Config\n",
      "0+4096\n",
      "1+4095\n",
      "2+4094\n",
      "4+4092\n",
      "8+4088\n",
      "\n",
      "Llama-2-7B\n",
      "3359.95\n",
      "11.88\n",
      "10.51\n",
      "9.59\n",
      "9.54\n",
      "\n",
      "Why do various autoregressive LLMs, such as Llama-2, MPT, Falcon, and Pythia, consistently focus on initial tokens as their attention sinks, rather than other tokens? Our explanation is straightforward: Due to the sequential nature of autoregressive language modeling, initial tokens are visible to all subsequent tokens, while later tokens are only visible to a limited set of subsequent tokens. As a result, initial tokens are more easily trained to serve as attention sinks, capturing unnecessary attention.\n",
      "\n",
      "We’ve noted that LLMs are typically trained to utilize multiple initial tokens as attention sinks rather than just one. As illustrated in Figure 2, the introduction of four initial tokens, as attention sinks, suffices to restore the LLM’s performance. In contrast, adding just one or two doesn’t achieve full recovery. We believe this pattern emerges because these models didn’t include a consistent starting token across all input samples during pre-training. Although Llama-2 does prefix each paragraph with a “<s>\" token, it’s applied before text chunking, resulting in a mostly random token occupying the zeroth position. This lack of a uniform starting token leads the model to use several initial tokens as attention sinks. We hypothesize that by incorporating a stable learnable token at the start of all training samples, it could singularly act as a committed attention sink, eliminating the need for multiple initial tokens to ensure consistent streaming. We will validate this hypothesis in Section 3.3.\n",
      "\n",
      "3.2 Rolling KV Cache with Attention Sinks\n",
      "Figure 4: The KV cache of StreamingLLM.\n",
      "\n",
      "To enable LLM streaming in already trained LLMs, we propose a straightforward method that can recover window attention’s perplexity without any model finetuning. Alongside the current sliding window tokens, we reintroduce a few starting tokens’ KV in the attention computation. The KV cache in StreamingLLM can be conceptually divided into two parts, as illustrated in Figure 4: (1) Attention sinks (four initial tokens) stabilize the attention computation; 2) Rolling KV Cache retains the most recent tokens, crucial for language modeling. StreamingLLM’ design is versatile and can be seamlessly incorporated into any autoregressive language model that employs relative positional encoding, such as RoPE (Su et al., 2021) and ALiBi (Press et al., 2022).\n",
      "\n",
      "When determining the relative distance and adding positional information to tokens, StreamingLLM focuses on positions within the cache rather than those in the original text. This distinction is crucial for StreamingLLM’s performance. For instance, if the current cache (Figure 4) has tokens [0, 1, 2, 3, 6, 7, 8] and is in the process of decoding the 9th token, the positions assigned are [0, 1, 2, 3, 4, 5, 6, 7], rather than the positions in the original text, which would be [0, 1, 2, 3, 6, 7, 8, 9].\n",
      "\n",
      "For encoding like RoPE, we cache the Keys of tokens prior to introducing the rotary transformation. Then, we apply position transformation to the keys in the rolling cache at each decoding phase. On the other hand, integrating with ALiBi is more direct. Here, the contiguous linear bias is applied instead of a ’jumping’ bias to the attention scores. This method of assigning positional embedding within the cache is crucial to StreamingLLM’s functionality, ensuring that the model operates efficiently even beyond its pre-training attention window size.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m논문에서 토큰을 4번 넣으면서 확인하고자 했던 것은 다음과 같습니다:\n",
      "\n",
      "1. **초기 토큰의 중요성 확인**: 초기 토큰을 제거하면 언어 모델의 성능이 급격히 저하되는 이유를 확인하기 위해, 초기 4개의 토큰을 제거하고 그 자리에 줄바꿈 토큰(\"\\n\")을 넣어 실험을 진행했습니다. 이 실험을 통해 초기 토큰의 절대적인 위치가 모델 성능에 중요한 역할을 한다는 것을 확인했습니다.\n",
      "\n",
      "2. **주의 싱크(attention sink)의 역할**: 초기 토큰이 주의 싱크로 작용하여 모델의 주의 점수를 안정화시키는 역할을 한다는 것을 확인했습니다. 초기 토큰을 제거하면 주의 점수의 분포가 크게 변하여 모델 성능이 저하되지만, 초기 토큰을 다시 추가하면 성능이 회복되는 현상을 관찰했습니다.\n",
      "\n",
      "3. **최적의 초기 토큰 수**: 초기 토큰의 수가 모델 성능에 미치는 영향을 확인하기 위해, 1개, 2개, 4개, 8개의 초기 토큰을 추가하여 실험을 진행했습니다. 그 결과, 4개의 초기 토큰을 추가하는 것이 모델 성능을 회복하는 데 가장 효과적이라는 것을 발견했습니다.\n",
      "\n",
      "이 실험을 통해 초기 토큰이 주의 싱크로 작용하여 모델의 주의 점수를 안정화시키고, 이를 통해 스트리밍 환경에서도 효율적으로 작동할 수 있다는 것을 확인했습니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"저 논문에서 \\n 토큰을 4번 넣으면서 확인하고 싶었던 게 뭐야?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS', 'type': 'citation'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'paperId': '4ea5ca620122e6a9a2b000444d36491cebf49c7c', 'title': 'Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey', 'abstract': 'Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at https://github.com/Strivin0311/long-llms-learning.', 'influentialCitationCount': 4, 'publicationDate': '2023-11-21'}, {'paperId': 'a3e000e0d7f64c1d094c2a8bf6f43992cbabe91b', 'title': 'KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache', 'abstract': \"Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm, named KIVI. With the hardware-friendly implementation, KIVI can enable Llama (Llama-2), Falcon, and Mistral models to maintain almost the same quality while using $\\\\mathbf{2.6\\\\times}$ less peak memory usage (including the model weight). This reduction in memory usage enables up to $\\\\mathbf{4\\\\times}$ larger batch size, bringing $\\\\mathbf{2.35\\\\times \\\\sim 3.47\\\\times}$ throughput on real LLM inference workload. The source code is available at https://github.com/jy-yuan/KIVI.\", 'influentialCitationCount': 5, 'publicationDate': '2024-02-05'}, {'paperId': 'f288e2238ac8725baa7ca9874bbc3fed1e89a632', 'title': 'Data Engineering for Scaling Language Models to 128K Context', 'abstract': \"We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \\\\textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \\\\textit{quantity} and \\\\textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \\\\textit{domain balance} and \\\\textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.\", 'influentialCitationCount': 5, 'publicationDate': '2024-02-15'}, {'paperId': '3e4ed3b3790980efbb537d381c7bc020eefed53f', 'title': 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'abstract': 'Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. State Space Models (SSMs), and more specifically Selective SSMs (S6), with efficient hardware-aware implementation, have shown promising potential for long causal sequence modeling. They, however, use separate blocks for each channel and fail to filter irrelevant channels and capture inter-channel dependencies. Natural attempt to mix information across channels using MLP, attention, or SSMs results in further instability in the training of SSMs for large networks and/or nearly double the number of parameters. We present the MambaMixer block, a new SSM-based architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels-called Selective Token and Channel Mixer. To mitigate doubling the number of parameters, we present a new non-causal heuristic of the S6 block with a hardware-friendly implementation. We further present an efficient variant of MambaMixer, called QSMixer, that mixes information along both sequence and embedding dimensions. As a proof of concept, we design Vision MambaMixer (ViM2) and Vision QSMixer (ViQS) architectures. To enhance their ability to capture spatial information in images, we present Switch of Scans (SoS) that dynamically uses a set of useful image scans to traverse image patches. We evaluate the performance of our methods in image classification, segmentation, and object detection. Our results underline the importance of selectively mixing across both tokens and channels and show the competitive (resp. superior) performance of our methods with well-established vision models (resp. SSM-based models).', 'influentialCitationCount': 3, 'publicationDate': '2024-03-29'}, {'paperId': 'd8b51d518f2dd62943762ceaa8961d3b1bfbcc1a', 'title': \"RULER: What's the Real Context Size of Your Long-Context Language Models?\", 'abstract': 'The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the\"needle\") from long distractor texts (the\"haystack\"), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate ten long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only four models (GPT-4, Command-R, Yi-34B, and Mixtral) can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.', 'influentialCitationCount': 5, 'publicationDate': '2024-04-09'}]\u001b[0m\u001b[32;1m\u001b[1;3m다음은 논문 \"EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS\"의 최신 후속 연구들입니다:\n",
      "\n",
      "1. **Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey**\n",
      "   - **출판일**: 2023-11-21\n",
      "   - **요약**: 이 논문은 Transformer 기반 대형 언어 모델(LLM)의 긴 문맥 처리 능력을 향상시키기 위한 최근의 발전을 종합적으로 조사합니다. 긴 문맥 입력과 출력을 처리하는 문제를 분석하고, 이를 해결하기 위한 Transformer 아키텍처의 업그레이드를 분류하고 설명합니다. 또한, 긴 문맥 LLM을 평가하기 위한 데이터셋, 메트릭, 베이스라인 모델 및 최적화 도구를 조사합니다.\n",
      "\n",
      "2. **KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache**\n",
      "   - **출판일**: 2024-02-05\n",
      "   - **요약**: 이 논문은 대형 언어 모델(LLM)의 키-값(KV) 캐시를 효율적으로 양자화하여 메모리 사용량을 줄이고 추론 속도를 향상시키는 방법을 제안합니다. KV 캐시의 요소 분포를 연구하여 키 캐시는 채널별로, 값 캐시는 토큰별로 양자화하는 것이 효과적임을 발견했습니다. 이를 바탕으로 하드웨어 친화적인 2비트 KV 캐시 양자화 알고리즘인 KIVI를 개발했습니다.\n",
      "\n",
      "3. **Data Engineering for Scaling Language Models to 128K Context**\n",
      "   - **출판일**: 2024-02-15\n",
      "   - **요약**: 이 논문은 언어 모델의 문맥 길이를 128K로 확장하기 위한 데이터 엔지니어링 방법을 연구합니다. 대규모 사전 학습을 통해 이미 획득한 긴 문맥 모델링 능력을 적절한 데이터 혼합을 통해 경량의 지속적 사전 학습으로 확장할 수 있음을 보여줍니다. 1B-5B 토큰의 데이터를 사용한 지속적 사전 학습이 128K 문맥 길이로 확장하는 데 효과적임을 입증합니다.\n",
      "\n",
      "4. **MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection**\n",
      "   - **출판일**: 2024-03-29\n",
      "   - **요약**: 이 논문은 긴 시퀀스 모델링을 위한 효율적인 선택적 상태 공간 모델(SSM) 기반 아키텍처인 MambaMixer를 제안합니다. 토큰과 채널을 선택적으로 혼합하는 메커니즘을 사용하여 SSM의 훈련 불안정을 해결하고, 하드웨어 친화적인 구현을 통해 성능을 향상시킵니다. 이미지 분류, 세분화 및 객체 탐지에서 경쟁력 있는 성능을 보입니다.\n",
      "\n",
      "5. **RULER: What's the Real Context Size of Your Long-Context Language Models?**\n",
      "   - **출판일**: 2024-04-09\n",
      "   - **요약**: 이 논문은 긴 문맥 언어 모델의 실제 문맥 크기를 평가하기 위한 새로운 벤치마크인 RULER를 제안합니다. 다양한 유형과 수량의 정보를 포함하는 테스트를 통해 모델의 긴 문맥 이해 능력을 종합적으로 평가합니다. 32K 이상의 문맥 길이를 주장하는 모델 중 일부만이 만족스러운 성능을 유지할 수 있음을 발견했습니다.\n",
      "\n",
      "이 연구들은 긴 문맥 처리와 효율적인 메모리 사용을 중심으로 대형 언어 모델의 성능을 향상시키기 위한 다양한 접근 방식을 탐구하고 있습니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"이 논문의 후속 연구를 알려줘 최신순으로\"},\n",
    "    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Paper_assistant(message, session_id = 'temp'):\n",
    "    output = agent_with_chat_history.invoke(\n",
    "        {\"input\": message},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    display(Markdown(output['output']))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION', 'type': 'citation'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'abstract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mPaper_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION 후속 논문 추천해줘\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36mPaper_assistant\u001b[0;34m(message, session_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPaper_assistant\u001b[39m(message, session_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43magent_with_chat_history\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     display(Markdown(output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain_core/runnables/base.py:4588\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4584\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4585\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4586\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4587\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4589\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4590\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain_core/runnables/base.py:4588\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4584\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4585\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4586\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4587\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4589\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4590\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain_core/runnables/base.py:2507\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2506\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2507\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain_core/runnables/branch.py:212\u001b[0m, in \u001b[0;36mRunnableBranch.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbranch:default\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain_core/runnables/base.py:4588\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4584\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4585\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4586\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4587\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4589\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4590\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain/agents/agent.py:1433\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1433\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1442\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1443\u001b[0m         )\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain/agents/agent.py:1139\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1132\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1139\u001b[0m         [\n\u001b[1;32m   1140\u001b[0m             a\n\u001b[1;32m   1141\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1142\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1143\u001b[0m                 color_mapping,\n\u001b[1;32m   1144\u001b[0m                 inputs,\n\u001b[1;32m   1145\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1146\u001b[0m                 run_manager,\n\u001b[1;32m   1147\u001b[0m             )\n\u001b[1;32m   1148\u001b[0m         ]\n\u001b[1;32m   1149\u001b[0m     )\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain/agents/agent.py:1139\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1132\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1139\u001b[0m         [\n\u001b[1;32m   1140\u001b[0m             a\n\u001b[1;32m   1141\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1142\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1143\u001b[0m                 color_mapping,\n\u001b[1;32m   1144\u001b[0m                 inputs,\n\u001b[1;32m   1145\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1146\u001b[0m                 run_manager,\n\u001b[1;32m   1147\u001b[0m             )\n\u001b[1;32m   1148\u001b[0m         ]\n\u001b[1;32m   1149\u001b[0m     )\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain/agents/agent.py:1224\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain/agents/agent.py:1246\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[0;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain_core/tools.py:452\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(observation, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain_core/tools.py:409\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[1;32m    407\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    408\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 409\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, \u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    414\u001b[0m     )\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[0;32m/c1/yumin/anaconda3/envs/sprlla/lib/python3.8/site-packages/langchain_core/tools.py:750\u001b[0m, in \u001b[0;36mStructuredTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[1;32m    742\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[1;32m    745\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    746\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    747\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    748\u001b[0m         )\n\u001b[1;32m    749\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[0;32m--> 750\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m     )\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/c1/yumin/tool/kubig19th-conference-llm/papers_llm/api_pool.py:382\u001b[0m, in \u001b[0;36mquery2recommend_paper\u001b[0;34m(query, type)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mtype에 따라 target paper에 대한 citation 혹은 reference를 추천\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcitation\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcitation_recommend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecommend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reference_recommend(query\u001b[38;5;241m=\u001b[39mquery, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mapi_key, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, recommend\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/c1/yumin/tool/kubig19th-conference-llm/papers_llm/api_pool.py:348\u001b[0m, in \u001b[0;36mcitation_recommend\u001b[0;34m(query, num, threshold, recommend, api_key)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m## target 논문과 20개의 citation 사이의 유사도 계산\u001b[39;00m\n\u001b[1;32m    347\u001b[0m abs_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 348\u001b[0m abs_dict[target_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m citation_response:\n\u001b[1;32m    351\u001b[0m     paper_id, title \u001b[38;5;241m=\u001b[39m keyword[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaperId\u001b[39m\u001b[38;5;124m'\u001b[39m], keyword[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abstract'"
     ]
    }
   ],
   "source": [
    "Paper_assistant('PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION 후속 논문 추천해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization'}`\n",
      "\n",
      "\n",
      "\u001b[0mError fetching the URL: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?\n",
      "Page limit reached at 6\n",
      "\u001b[36;1m\u001b[1;3mAgent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization Wenqi Zhang1,∗, Ke Tang2,3,4,7,∗, Hai Wu2,3,5,7, Mengna Wang2,6, Yongliang Shen1 Guiyang Hou1, Zeqi Tan1, Peng Li2,3,7,†, Yueting Zhuang1, Weiming Lu1,† 1College of Computer Science and Technology, Zhejiang University 2Institute of Software, Chinese Academy of Sciences3Nanjing Institute of Software Technology 4Nanjing University of Posts and Telecommunications 5Nanjing University of Information Science and Technology 6Beijing University of Technology7University of Chinese Academy of Sciences, Nanjing {zhangwenqi, luwm}@zju.edu.cn, lipeng@iscas.ac.cn Abstract Large Language Models (LLMs) exhibit ro- bust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophis- ticated prompt engineering, rather than agents capable of learning and evolving through inter- actions. These task solvers necessitate manu- ally crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitat- ing to address complex dynamic scenarios e.g., large interactive games. In light of this, we pro- pose Agent-Pro : an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behav- ioral policy. Specifically, it involves a dynamic belief generation and reflection process for pol- icy evolution. Rather than action-level reflec- tion, Agent-Pro iteratively reflects on past tra- jectories and beliefs, \"fine-tuning\" its irrational beliefs for a better policy. Moreover, a depth- first search is employed for policy optimization, ensuring continual enhancement in policy pay- offs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold’em, outperforming vanilla LLM and specialized models. Our re- sults show Agent-Pro can learn and evolve in complex and dynamic scenes, which also bene- fits numerous LLM-based applications1. 1 Introduction Designing a human-level agent with robust problem-solving abilities has long been a vision in the academic community. This necessitates the agent to possess learning and generalization capa- bilities across a diverse array of tasks. The advent of Large Language Models (LLMs) (Chowdhery *The first two authors have equal contributions. This work was done when the first author was an intern at Institute of Software, Chinese Academy of Sciences. †Corresponding author. 1Code is in https://github.com/zwq2018/Agent-Pro Please defeat your opponents in the game…TaskEnvironmentObskPrompt1LLMPolicyEvolution Goal:To win the game, you need to win more chips in winning matches and lose fewer chips in losing matches…Strategy:Firstly, you calculate the value of your cardsandthen analyze your opponents’ behaviors. If conservative players consistently raise their bets, it means they may have a strong hand.Demonstrations:somegoodandbadexamples…Behavior PolicyYou’re playing Texas Hold’emwith three players…TaskDescriptionBehavior PolicyYou’re playing limited Texas Hold’emwith three players. Player-1seems to have an aggressive style with some deceptive bluffs, while Player 2is usually conservativewith many early folds.TaskDescription InteractiveScenario Actionk … ImperfectInformationGame•Environment•OtherPlayers•RulesWorld-BeliefSelf-Belief•Plan•State•RiskBeliefk Beliefk-1Policy1Policy2PolicyNAgentwithPolicy1•Task Description•Behavior Policy•OutputFormat AgentwithPolicyN•Task Description•Behavior Policy•OutputFormat PromptNLLMPastExperienceTrajectoryResultBeliefPolicy-LevelReflectionNewPromptPolicyEvolutionMulti-AgentCoexistence Figure 1: For interactive tasks, e.g., imperfect- information games, we propose a versatile agent frame- work capable of self-learning and evolving. Firstly, our agent constructs beliefs about itself and the envi- ronment. Then it autonomously updates its prompts through policy-level reflection on past trajectories and beliefs, evolving a better behavioral strategy. et al., 2022; Zhang et al., 2022a; Zeng et al., 2023; Touvron et al., 2023a; OpenAI, 2022, 2023; Tou- vron et al., 2023b) has shed light on this vision, especially they can be rapidly generalized across a wide range of tasks with only a few demonstra- tions (Wei et al., 2022a,b). Benefiting from this, many systems built upon LLMs have showcased markedly enhanced performance such as question- answering (Yao et al., 2022; Schick et al., 2023; Shen et al., 2023a; Zhang et al., 2023d; Madaan et al., 2023; Zhang et al., 2022b, 2023e), code gen- eration (Hong et al., 2023; Wu et al., 2023b), and real-world application (Qin et al., 2023b; Zhang et al., 2023b). Despite these achievements, building a human- level agent remains a challenging endeavor. First, most LLM-based agents are designed for specific tasks through sophisticated prompts, including de- tailed task descriptions and behavioral specifica-arXiv:2402.17574v3 [cs.AI] 6 Jun 2024tions. However, numerous real-world tasks, e.g., business, company negotiations, and security, are more intricate with imperfect information, necessi- tating laborious efforts to design strategic behavior. Second, most LLM-based agents do not consider interacting with task scenarios, and more critically, cannot learn from past experiences and evolve their behavioral strategies during interactions. In con- trast, humans often learn and adjust their behaviors through interaction, especially in novel scenarios. In light of these, a promising yet under-explored topic emerges: Can LLM-based agents learn and elevate behavioral strategies by interacting with the environment like humans ? It should be an indispensable ability of a human-level agent. Recently, numerous studies (Shinn et al., 2023; Wang et al., 2023a; Zhang et al., 2023a; Zhao et al., 2023; Qian et al., 2024) undertake intriguing explo- rations, e.g., utilizing feedback for self-correction at the action-level. Besides, several efforts also ex- plore deploying LLM in interactive games, includ- ing StarCraft (Ma et al., 2023), Minecraft (Wang et al., 2023a), strategy-based gaming (Bakhtin et al., 2022; Guo et al., 2023a; Xu et al., 2023a,b). Similarly, we first evaluate LLM-based agents with the self-correction strategy in dynamic interac- tive scenarios, such as multi-player Texas Hold’em, which is a zero-sum game with imperfect informa- tion. However, we observe that it loses most of the rounds to its opponents, even the most advanced LLMs. Upon examining its reasoning thoughts and actions, we find that it often adopts irrational be- haviors and is unable to deduce effective strategies from long action sequences. To answer the above question, the Theory of Mind (ToM) (Premack and Woodruff, 1978) may provide some insight. In this framework, each hu- man develops perceptions of himself (self-belief) and the external environment (social-belief) in the social context, and then grounds their decisions on these beliefs, or adjusts incorrect beliefs in re- sponse to external feedback. Inspired by this, we advocate Agent-Pro : a LLM-based Agent with Policy-level Reflection and Optimization. Agent- Pro is endowed with the capacity to learn and evolve within environments, i.e., autonomously re- flect on past experiences, calibrate its beliefs about itself and the environment, and optimize its behav- ior policy without parameter tuning. Concretely, as shown in Figure 1, an LLM-based agent involves an LLM as the foundational model and some instructions in the prompt to regulateits behavior (policy). Upon observing partial in- formation from the scenarios, Agent-Pro first up- dates its self-belief and world-belief, then makes decisions based on these beliefs. After exploring tasks, Agent-Pro performs a policy-level reflection and optimization on past trajectories, beliefs, and results. It autonomously \"fine-tunes\" its beliefs, searches for useful prompt instructions, and con- solidates them into a new behavior policy. The experiments in two zero-sum games, Black- jack and Texas Hold’em, demonstrate that Agent- Pro, after evolution, can defeat vanilla LLMs and specialized models, improving the game’s payoffs. It indicates that Agent-Pro enhances its capabili- ties through interaction and reflection without hu- man guidance. As depicted in Figure 1, the initial prompt is quite simple (Left Bottom), but after learning and evolution, the Agent-Pro generates many practical instructions (Right Bottom). For instance, Agent-Pro records estimations of each op- ponent’s style in Task Description and adds spe- cific Goals, Strategies inBehavior Policy . Our Agent-Pro is different from previous strate- gies, like Reflexion (Shinn et al., 2023). Firstly, Policy-level reflection is designed for policy up- dating in long-horizon tasks. It is aimed at long- horizon policy updating rather than immediate ac- tion correction. The input is a sequence of actions and delayed feedback, while the output is an opti- mized strategy, rather than a specific action. There- fore, policy-level reflection corrects irrational be- liefs and optimizes the old policy into the new one. As introduced in Section 3.2, our policy-level re- flection includes belief calibration, policy updates by refining behavioral guidelines and world model- ing, and policy verification. Besides, we innovatively distill long-term mem- ory into Behavioral Guidelines and World Models through prompt optimization. Most previous strate- gies store historical experience as verbal long-term memory and use it for text-based reasoning. In con- trast, we further construct an optimizable policy from long-term interactions, i.e., Behavioral Guide- lines and Environmental Models. This includes self-summarized game objectives and rules, effec- tive strategies derived from reflection, and demon- strative trajectories. The contributions of our work are as follows: •We introduce Agent-Pro, a framework capa- ble of learning and evolving within interactive games, empowering LLM-based agents to effi-ciently adapt to more complex dynamic tasks. •We devise a belief-aware decision-making pro- cess with self and world-belief, enhancing its capabilities for intricate tasks, i.e., generating more rational actions in interactive scenarios. •We utilize policy-level reflection and optimiza- tion to iteratively update prompt instructions, which empower Agent-Pro to progressively evolve from a novice to a skilled veteran with many strategic behaviors. •After learning, Agent-Pro is evaluated in mul- tiplayer games and defeats specialized models, gaining notable progress. It develops strategic skills like humans, e.g., actively cutting losses, bluffing, or disguising to influence others. Not just in card games, similar scenarios abound in the real world as well. Through self-learning and evolution, Agent-Pro can enhance deployment effectiveness in those scenarios, expanding the ca- pability boundaries of LLM-based agents notably. 2 Problem Definition Our study focuses on multi-player imperfect infor- mation interactive games, with two characteristics: Imperfect Information. Unlike perfect infor- mation games (e.g., chess), imperfect information scenarios are characterized by agents only having access to their own states and public information, without knowing the states of others, e.g., in Texas Hold’em, players cannot observe others’ cards, which is dissimilar to many LLM-based tasks. Dynamic Interaction. There may be multiple agents in the environment, and they may influence each other. That is, the actions of one agent may lead to changes in the environment, which are un- predictable for other agents. In real-world contexts, such as competition, com- pany negotiations, and security, these scenarios can often be abstracted as multi-agent interactive sce- narios with imperfect information. Research on this can offer viable solutions to many real-world problems. We select two games as our testbed: Blackjack and Limit Texas Hold’em with multi- player. Please refer to Appendix B for details. 3 Methods To empower agents to learn in interactive con- texts, a typical method is reinforcement learn- ing (Zhang et al., 2021, 2022c). This involvesexploring highly rewarding actions through trial and error and solidifying these experiences into model parameters. Nonetheless, the training over- head for LLMs is substantial. Therefore, we em- ploy a gradient-free \"exploration-learning\" strategy that enables LLM-based agents to learn through in-context learning. Specifically, we convert the policy learning into a prompt optimization process, i.e., LLM autonomously reflects and updates the prompt’s instructions based on its exploration expe- rience, solidifying the high-reward strategies into the prompts. Benefiting from LLM’s generaliza- tion capabilities, our agent can summarize rules and learn specialized skills from a small number of samples like humans, making it well-suited for many real-world scenarios. As shown in Figure 2, Agent-Pro comprises three components: (1) A Belief-Aware Decision- Making process. It first updates beliefs about the world and itself, rendering more coherent and con- sistent decisions in dynamic and imperfect game scenarios. (2) A Policy-Level Reflection. Rather than reflecting on a single action, our design em- powers LLMs to self-reflect on irrational beliefs from failed experiences. Then, it summarizes these erroneous beliefs into specific prompt instructions, like acting strategy (Behavioral Guideline), descrip- tions of the task world, and conjectures about other players (World Modeling), etc, which can calibrate its incorrect beliefs, evolving into a better policy. (3) A Prompt Optimization process ensures that the agent’s policy evolves for a higher payoff following a DFS-based search. 3.1 Belief-aware Decision-Making Process To develop an LLM-based agent better suited for interactive environments, we draw inspiration from the Theory of Mind (ToM) (Premack and Woodruff, 1978; Li et al., 2023b; Guo et al., 2023a). In this framework, human condenses perceptions of them- selves (self-belief) and the external environment (social-belief) and then ground their decisions on these beliefs, or adjust incorrect beliefs in response to external feedback. We also design a belief-aware decision-making process for Agent-Pro, simulating human cognitive processes in social contexts. First, we need to define the policy of an LLM- based agent, which refers to a specific behavioral strategy guiding the agent to interact and com- plete tasks. It often involves complex prompts designed by experts, covering task rules, strate- gies, and output formats. In a zero-sum game!!…!\"\"!#\"!$\"!#$!$$!\"$…Score: -1.7Score: 0.90.40.80.30.81.2… … …Policy GenerationPolicy EvaluationSearching Path:!!!\"\"!\"$!$$!#$!$\"!#\"….Policy Evaluation For K2 timesBelieft+1Actiont+1Belieft+2Actiont+2 Agent-ProOtherAgentsEnvironment!!\"#!!\"#$%PrivateStatePublicStatePrivateState\"!\"##!\"#\"!\"&#!\"&ObservableUnobservables!\"##!\"#Belieft...ResultsTrajectoryt Policy-LevelReflection:•Please check that the belief is consistentwith the final result.•Make sure the action and the belief are consistentateachstep…•Assess the rationalityof eachbelief… .Correctany erroneous beliefs…•Whether the beliefs accurately reflect the underlying intentions behind the opponentsActionEnvFeedbackAction-LevelReflection ActionEnvFeedbackPolicy-LevelReflectionWorld-BeliefSelf-BeliefBehavioralGuidelineWorld ModelingBeliefNewAction PolicyNewPolicy Verification World ModelingI perceive that Player 1 seems adept at psychological warfare.Despite holding a weak hand, he managed to bluff effectively, coercing me into folding prematurely…Next time, I must be cautious and not be misled by him.Player 2 also likes to raise, it seems like his style is more aggressive. Behavioral GuidelineGoal:To win more chips, Ineedtomaximize gains when holdinga strong hand and minimize losses when holding a weakhand…Strategy:I need to learn to bluff. Even if my hand is weak, I can test my opponents' reactionsby raising it in the early stages...Demonstration:This game is typical. I should record it for future reference.{Label:Igotaweakhand...,Detail:…}Self-Belief:Currently, my hand is weak(State). I need to wait for the next community card reveal. Besides, I must observe my opponent’s actions closely(Plan). If they appear strong, keeping calling may lead to more losses (Risk).……World-Belief:In my impression, player-1 is relatively conservative. However, he has been consistently raising, which may indicate a strong hand(Opponent). The final community card will be revealed shortly,andit might still be a weak one(Environment). If Player-1raises again, according to the rules, I can only raise, call, or fold(Rule). BehavioralGuidelineWorld Modeling Scores Player1Player2Agent-ProPlayer3 Player1Player2Agent-ProPlayer3 SwapCardSwapPosition Player1Player2Player3Agent-ProDealerBlindDealerBlindDealerBlindDFS-basedPolicyEvolutionBelief-aware Decision-Making Policy-Level ReflectionFigure 2: In a competitive multiplayer game with imperfect information, Agent-Pro designs a dynamic belief to enhance decision-making capabilities. It first updates its beliefs about the world and itself, then generates more coherent actions. To achieve policy-level reflection, Agent-Pro examines the beliefs associated with failed trajectories. It then summarizes prompt instructions, including World Modeling and Behavioral Guideline to calibrate incorrect beliefs. Lastly, Agent-Pro employs a DFS-based search to incrementally enhance policy effectiveness. with K+1players (assuming playing order is (op1, our, op 2, .., op K)), we denote the policy of our agent as πwith some observable information, containing agent’s private information st, public information ot, our own action at, and the actions of all opponents aop1 t, aop2 t, .., aopK t, where tmeans t-th rounds of a game. Therefore a complete game trajectory spanning trounds: H0:t={(s0, o0, aop1 0, a0, aop2 0, ..., aopK 0), ... (st, ot, aop1 t, at, aop2 t, ..., aopK t)}(1) As shown in Figure 2, when making a deci- sion, Agent-Pro first generates a dynamic beliefξabout itself (self-belief) and opponents (world- belief) in natural language. Then, it predicts an action based on the latest beliefs. For instance, for Texas Hold’em, Agent-Pro’s understanding of its hand cards, plan, and potential risk constitutes its self-belief, while the conjectures about the op- ponents form its world-belief. These beliefs are updated in each decision-making cycle. Equipped with this, Agent-Pro can generate more coherent and consistent actions: ξt+1, at+1∼π(H0:t, st+1, ot+1, aop1 t+1, ξt)(2) When a game is over, we acquire the observable stateR(e.g., private hand cards after showdown) and the final scores Sof all players. The objective is to find an optimal π∗to maximize S(our).3.2 Policy-Level Reflection Equipped with an initial policy (a simple prompt) and a dynamic belief, Agent-Pro already possesses basic capabilities for game exploration. To fur- ther enhance Agent-Pro’s capabilities, we design a learning mechanism via a policy-level reflection. Specifically, many text-based tasks have em- ployed reflection strategies and immediate environ- mental feedback to correct prior actions. However, in many typical interaction scenarios with longer decision-making processes, action-level reflections are not directly applicable due to delayed feedback. Therefore, for such a long-horizon interaction pro- cess, Agent-Pro is instructed to focus on the ratio- nality of beliefs and underlying behavioral policies rather than individual actions. Belief Calibration As depicted in Figure 2, un- der the guidance of the current behavior policy, Agent-Pro generates actions based on self-belief and world-belief. If these beliefs are inaccurate, they may lead to irrational actions and eventual failure. Therefore, Agent-Pro examines the ratio- nality of these beliefs based on the final results and reflectss on the reasons for the final failure. Correctness : Whether its beliefs about itself , the game , and its opponents align with the final results . Consistency : Whether each belief and action is self - contradictory . Rationality : Whether the beliefs accurately reflect the underlying intentions behind the opponents . Reasons : Reflect on why it lost to its opponents , which beliefs are problematic , and what the underlying reasons are . Lastly, to calibrate the incorrect beliefs, Agent- Pro summarizes these reflections and analyses about itself and the external world into specific instructions: Behavioral Guideline andWorld Modeling , where the former represents general- ized behavioral strategies for this task, and the latter signifies its understanding and conjectures about the game world. For instance, in Texas Hold’em, Agent-Pro summarizes the following contents: Behavioral Guideline 1- Please summarize a detailed goal based on your reflection on beliefs . {Goal} 2- What strategy helps you build correct belief and win at similar .. {Strategy} 3- Can this game be considered a typical example for future ... {Demonstration} World Modeling 1- Accurately model each player to help build more precise beliefs about them , including action , and style .{Opponent} 2- Describe any game rules or details that are easy to overlook ...{Rule}Agent-Pro summarizes high-level strategies within theBehavioral Guideline and describes the task and opponents in World Modeling . These instruc- tions can calibrate previous incorrect beliefs and improve policy performance. The entire process can be formalized as follows: Instructionn+1←LLM (Hn 0:T,{ξn 1, ξn 2..},Rn,Sn) (3) where Hn 0:Tdenotes a complete trajectory at the n-th match, {ξn 1, ξn 2, ...}denotes the belief se- quence, RnandSnmeans the final results and score. Instructionn+1denotes new generated Behavioral Guideline andWorld Modeling . Verification After extracting these instructions, Agent-Pro verifies its efficacy. Agent-Pro incorpo- rates these generated Behavioral Guideline and World Modeling into the prompt and then replays the same game again, i.e., the same opponents and initial conditions. If the final score improves, we retain them in the prompt. Otherwise, we regener- ate a new one. If it fails to pass verification after three retries, we discard this trajectory Hn: πn+1V erify← − − − − πn∪Instructionn+1 (4) where ∪means incorporates new instructions into the previous prompt for πn+1. This new policy encompasses more effective instructions, empower- ing Agent-Pro to establish accurate self- and world beliefs and generate more rational actions. 3.3 DFS-based Policy Evolution To iteratively update the policy, we devise a policy optimization process based on depth-first search (DFS). It encompasses a policy evaluation process to assess the generalization ability of the new policy in novel game scenarios and a search mechanism to progressively find a better policy. Policy Evaluation Each time the policy is updated, Agent-Pro is required to evaluate the new strategies. This evaluation process is dis- tinct from the previous Verification step, as the Verification repeatedly utilizes the \"training\" data for evaluation and can not ensure the gener- alizability of the new policy. Hence, Agent-Pro conducts a thorough assessment of the new policy in novel trajectories. Besides, it is imperative to eliminate the influence of random factors when pol- icy evaluation, e.g., a poor initial hand due to bad luck or an unfavorable playing order. Therefore, we first randomly generate a new game for K+1players. Then we sequentially\u001b[0m\u001b[32;1m\u001b[1;3m**초록 요약:**\n",
      "\n",
      "\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문은 대형 언어 모델(LLM)을 기반으로 한 에이전트인 Agent-Pro를 소개합니다. 이 에이전트는 상호작용 경험을 통해 학습하고 행동 정책을 점진적으로 향상시킬 수 있습니다. 기존의 LLM 기반 에이전트는 특정 작업 해결에 초점을 맞추고 복잡한 프롬프트 엔지니어링을 필요로 하지만, Agent-Pro는 정책 수준의 반성과 최적화를 통해 스스로 학습하고 진화할 수 있습니다. 이 에이전트는 블랙잭과 텍사스 홀덤 게임에서 평가되었으며, 기존의 LLM 및 전문 모델을 능가하는 성능을 보였습니다. Agent-Pro는 상호작용과 반성을 통해 복잡하고 동적인 장면에서 학습하고 진화할 수 있으며, 이는 다양한 LLM 기반 응용 프로그램에 유익할 수 있습니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**초록 요약:**\n",
       "\n",
       "\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문은 대형 언어 모델(LLM)을 기반으로 한 에이전트인 Agent-Pro를 소개합니다. 이 에이전트는 상호작용 경험을 통해 학습하고 행동 정책을 점진적으로 향상시킬 수 있습니다. 기존의 LLM 기반 에이전트는 특정 작업 해결에 초점을 맞추고 복잡한 프롬프트 엔지니어링을 필요로 하지만, Agent-Pro는 정책 수준의 반성과 최적화를 통해 스스로 학습하고 진화할 수 있습니다. 이 에이전트는 블랙잭과 텍사스 홀덤 게임에서 평가되었으며, 기존의 LLM 및 전문 모델을 능가하는 성능을 보였습니다. Agent-Pro는 상호작용과 반성을 통해 복잡하고 동적인 장면에서 학습하고 진화할 수 있으며, 이는 다양한 LLM 기반 응용 프로그램에 유익할 수 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문 초록 요약해',\n",
       " 'chat_history': [],\n",
       " 'output': '**초록 요약:**\\n\\n\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문은 대형 언어 모델(LLM)을 기반으로 한 에이전트인 Agent-Pro를 소개합니다. 이 에이전트는 상호작용 경험을 통해 학습하고 행동 정책을 점진적으로 향상시킬 수 있습니다. 기존의 LLM 기반 에이전트는 특정 작업 해결에 초점을 맞추고 복잡한 프롬프트 엔지니어링을 필요로 하지만, Agent-Pro는 정책 수준의 반성과 최적화를 통해 스스로 학습하고 진화할 수 있습니다. 이 에이전트는 블랙잭과 텍사스 홀덤 게임에서 평가되었으며, 기존의 LLM 및 전문 모델을 능가하는 성능을 보였습니다. Agent-Pro는 상호작용과 반성을 통해 복잡하고 동적인 장면에서 학습하고 진화할 수 있으며, 이는 다양한 LLM 기반 응용 프로그램에 유익할 수 있습니다.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paper_assistant('\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문 초록 요약해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpaper import GetPaper\n",
    "import os\n",
    "\n",
    "ss_api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "\n",
    "\n",
    "# Updated loadpaer tool\n",
    "# model can use this tool several times, to get the list of the section, and then see the detail content of the paper.\n",
    "# if the ar5iv_mode is False, \n",
    "getpapermodule = GetPaper(ss_api_key, ar5iv_mode = True, path_db = './papers_db', page_limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNetworkError\u001b[0m                              Traceback (most recent call last)\n",
      "File \u001b[0;32m/c1/yumin/tool/kubig19th-conference-llm/papers_llm/getpaper.py:225\u001b[0m, in \u001b[0;36mGetPaper.load_paper\u001b[0;34m(self, title, sections)\u001b[0m\n",
      "\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 225\u001b[0m     arxiv_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_paper_info_by_title_ss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NetworkError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[0;32m/c1/yumin/tool/kubig19th-conference-llm/papers_llm/getpaper.py:62\u001b[0m, in \u001b[0;36mGetPaper.get_paper_info_by_title_ss\u001b[0;34m(self, title)\u001b[0m\n",
      "\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NetworkError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo paper found for the given title\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mNetworkError\u001b[0m: No paper found for the given title\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43mgetpapermodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_paper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAgent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/c1/yumin/tool/kubig19th-conference-llm/papers_llm/getpaper.py:228\u001b[0m, in \u001b[0;36mGetPaper.load_paper\u001b[0;34m(self, title, sections)\u001b[0m\n",
      "\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NetworkError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 228\u001b[0m         arxiv_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_paper_info_by_title_arxiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m NetworkError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: No paper was founded in database\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\n",
      "File \u001b[0;32m/c1/yumin/tool/kubig19th-conference-llm/papers_llm/getpaper.py:77\u001b[0m, in \u001b[0;36mGetPaper.get_paper_info_by_title_arxiv\u001b[0;34m(self, query, max_results)\u001b[0m\n",
      "\u001b[1;32m     75\u001b[0m feed \u001b[38;5;241m=\u001b[39m feedparser\u001b[38;5;241m.\u001b[39mparse(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feed:\n",
      "\u001b[0;32m---> 77\u001b[0m     paper \u001b[38;5;241m=\u001b[39m \u001b[43mfeed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;32m     78\u001b[0m     arxiv_id \u001b[38;5;241m=\u001b[39m paper\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arxiv_id\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "getpapermodule.load_paper(title = \"Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
