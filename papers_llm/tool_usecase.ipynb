{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /c1/yumin/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /c1/yumin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import os\n",
    "from pprint import pprint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "import tool_pool as tp\n",
    "\n",
    "dotenv_path = '/Users/minkyuramen/Desktop/project/env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# load tool\n",
    "tools = [tp.loadpaper, tp.recommendpaper, tp.loadfigure]\n",
    "# load Agent prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM that will drive the agent\n",
    "# Only certain models support this\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"explain about the math expression about the Filtering api calls in the 3 page of the paper 'Language Models Can Teach Themselves to Use Tools' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"summary the method of the paper 'Large Language Model Connected with Massive APIs' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recompendpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"recommend the future works of the paper 'Facilitating Large Language Models to Master 16000+ Real-world APIs' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"Could you recommend previous works to read before this papaer : 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadfigure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor.invoke({\"input\": \"explain about the 'Figure 2: Pass Rate evaluation in ToolBench paper.' in the 3 page of the paper 'StableToolBench' 한글로 말해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory in Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "# prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "memory = ChatMessageHistory(session_id=\"test-session\")\n",
    "# message_history = RedisChatMessageHistory(\n",
    "#     url=\"redis://127.0.0.1:6379/0\", ttl=600, session_id=\"my-session\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    lambda session_id: memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Summary the abstract of the paper 'EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS'\"},\n",
    "    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"저 논문에서 \\n 토큰을 4번 넣으면서 확인하고 싶었던 게 뭐야?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"이 논문의 후속 연구를 알려줘 최신순으로\"},\n",
    "    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Paper_assistant(message, session_id = 'temp'):\n",
    "    output = agent_with_chat_history.invoke(\n",
    "        {\"input\": message},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    display(Markdown(output['output']))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paper_assistant('PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION 후속 논문 추천해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n",
      "title\n",
      "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Problem Definition\n",
      "  3 Methods\n",
      "    3.1 Belief-aware Decision-Making Process\n",
      "    3.2 Policy-Level Reflection\n",
      "    3.3 DFS-based Policy Evolution\n",
      "  4 Game: Blackjack\n",
      "    4.1 Results\n",
      "    4.2 Analyisis\n",
      "  5 Game: Limit Texas Hold’em\n",
      "    5.1 Results\n",
      "    5.2 Analysis on Learning Process\n",
      "    5.3 Analysis on Policy Evolution\n",
      "  6 Conclusion\n",
      "  Limitations\n",
      "  References\n",
      "  Appendix A Experiment Details\n",
      "    A.1 LLMs\n",
      "    A.2 Baselines\n",
      "    A.3 Setups For Blackjack\n",
      "    A.4 Detailed Setup For Texas Hold’em\n",
      "  Appendix B Introduction of Two Games\n",
      "    B.1 Blackjack\n",
      "    B.2 Limit Texas Hold’em\n",
      "    B.3 The Challenging of two Games\n",
      "  Appendix C Complementary Experiments\n",
      "    C.1 Whether Few-shot Learning Can Handle Such Complex Interaction\n",
      "    C.2 Policy-Level Reflection without Belief\n",
      "    C.3 Detailed Analysis Experiments\n",
      "  Appendix D Related Works\n",
      "    D.1 LLM-based Application\n",
      "    D.2 LLMs For Interactive Scenarios\n",
      "    D.3 Improving the Quality of LLM Responses\n",
      "    D.4 Automatic Prompt Optimization\n",
      "  Appendix E Detailed Prompts\n",
      "    E.1 Baseline’s Prompts For Blackjack\n",
      "    E.2 Agent-Pro’s Prompt For Blackjack\n",
      "    E.3 Baseline’s Prompts For Limited Texas Hold’em\n",
      "    E.4 Agent-Pro’s Prompt For Limited Texas Hold’em\n",
      "  Appendix F Case Study\n",
      "    F.1 Cases For Blackjack\n",
      "    F.2 Cases For Texas Hold’em\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': ['Abstract']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAbstractLarge Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, ”fine-tuning” its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold’em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications 111Code is in https://github.com/zwq2018/Agent-Pro.\u001b[0m\u001b[32;1m\u001b[1;3m**Abstract Summary:**\n",
      "\n",
      "Large Language Models (LLMs) are powerful problem solvers but are often designed for specific tasks using complex prompt engineering. These task-specific designs limit their ability to handle dynamic scenarios like large interactive games. To address this, the paper introduces Agent-Pro, an LLM-based agent that evolves its policy through interactions. Agent-Pro uses a dynamic belief generation and reflection process for policy evolution, focusing on policy-level rather than action-level reflection. It employs depth-first search for policy optimization, leading to continuous improvement. Evaluated in Blackjack and Texas Hold’em, Agent-Pro outperforms standard LLMs and specialized models, demonstrating its ability to learn and adapt in complex environments.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Abstract Summary:**\n",
       "\n",
       "Large Language Models (LLMs) are powerful problem solvers but are often designed for specific tasks using complex prompt engineering. These task-specific designs limit their ability to handle dynamic scenarios like large interactive games. To address this, the paper introduces Agent-Pro, an LLM-based agent that evolves its policy through interactions. Agent-Pro uses a dynamic belief generation and reflection process for policy evolution, focusing on policy-level rather than action-level reflection. It employs depth-first search for policy optimization, leading to continuous improvement. Evaluated in Blackjack and Texas Hold’em, Agent-Pro outperforms standard LLMs and specialized models, demonstrating its ability to learn and adapt in complex environments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문 초록 요약해',\n",
       " 'chat_history': [],\n",
       " 'output': '**Abstract Summary:**\\n\\nLarge Language Models (LLMs) are powerful problem solvers but are often designed for specific tasks using complex prompt engineering. These task-specific designs limit their ability to handle dynamic scenarios like large interactive games. To address this, the paper introduces Agent-Pro, an LLM-based agent that evolves its policy through interactions. Agent-Pro uses a dynamic belief generation and reflection process for policy evolution, focusing on policy-level rather than action-level reflection. It employs depth-first search for policy optimization, leading to continuous improvement. Evaluated in Blackjack and Texas Hold’em, Agent-Pro outperforms standard LLMs and specialized models, demonstrating its ability to learn and adapt in complex environments.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paper_assistant('\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문 초록 요약해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': []}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSection list was incorrect.\n",
      "Here is the title and section of the paper\n",
      "title\n",
      "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Problem Definition\n",
      "  3 Methods\n",
      "    3.1 Belief-aware Decision-Making Process\n",
      "    3.2 Policy-Level Reflection\n",
      "    3.3 DFS-based Policy Evolution\n",
      "  4 Game: Blackjack\n",
      "    4.1 Results\n",
      "    4.2 Analyisis\n",
      "  5 Game: Limit Texas Hold’em\n",
      "    5.1 Results\n",
      "    5.2 Analysis on Learning Process\n",
      "    5.3 Analysis on Policy Evolution\n",
      "  6 Conclusion\n",
      "  Limitations\n",
      "  References\n",
      "  Appendix A Experiment Details\n",
      "    A.1 LLMs\n",
      "    A.2 Baselines\n",
      "    A.3 Setups For Blackjack\n",
      "    A.4 Detailed Setup For Texas Hold’em\n",
      "  Appendix B Introduction of Two Games\n",
      "    B.1 Blackjack\n",
      "    B.2 Limit Texas Hold’em\n",
      "    B.3 The Challenging of two Games\n",
      "  Appendix C Complementary Experiments\n",
      "    C.1 Whether Few-shot Learning Can Handle Such Complex Interaction\n",
      "    C.2 Policy-Level Reflection without Belief\n",
      "    C.3 Detailed Analysis Experiments\n",
      "  Appendix D Related Works\n",
      "    D.1 LLM-based Application\n",
      "    D.2 LLMs For Interactive Scenarios\n",
      "    D.3 Improving the Quality of LLM Responses\n",
      "    D.4 Automatic Prompt Optimization\n",
      "  Appendix E Detailed Prompts\n",
      "    E.1 Baseline’s Prompts For Blackjack\n",
      "    E.2 Agent-Pro’s Prompt For Blackjack\n",
      "    E.3 Baseline’s Prompts For Limited Texas Hold’em\n",
      "    E.4 Agent-Pro’s Prompt For Limited Texas Hold’em\n",
      "  Appendix F Case Study\n",
      "    F.1 Cases For Blackjack\n",
      "    F.2 Cases For Texas Hold’em\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': ['3.1 Belief-aware Decision-Making Process']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.1 Belief-aware Decision-Making Process\n",
      "To develop an LLM-based agent better suited for interactive environments, we draw inspiration from the Theory of Mind (ToM) (Premack and Woodruff, 1978; Li et al., 2023b; Guo et al., 2023a). In this framework, human condenses perceptions of themselves (self-belief) and the external environment (social-belief) and then ground their decisions on these beliefs, or adjust incorrect beliefs in response to external feedback. We also design a belief-aware decision-making process for Agent-Pro, simulating human cognitive processes in social contexts.\n",
      "\n",
      "First, we need to define the policy of an LLM-based agent, which refers to a specific behavioral strategy guiding the agent to interact and complete tasks. It often involves complex prompts designed by experts, covering task rules, strategies, and output formats. In a zero-sum game with K​+​1𝐾+1K\\text{+}1 players (assuming playing order is (op1,our,op2,..,opK)(op_{1},our,op_{2},..,op_{K})), we denote the policy of our agent as π𝜋\\pi with some observable information, containing agent’s private information stsubscript𝑠𝑡{\\color[rgb]{1,0,0}s_{t}}, public information otsubscript𝑜𝑡{\\color[rgb]{0,0,1}o_{t}}, our own action atsubscript𝑎𝑡{\\color[rgb]{0,1,1}a_{t}}, and the actions of all opponents ato​p1,ato​p2,..,ato​pKa^{op_{1}}_{t},a^{op_{2}}_{t},..,a^{op_{K}}_{t}, where t𝑡t means t𝑡t-th rounds of a game. Therefore a complete game trajectory spanning t𝑡t rounds:\n",
      "\n",
      "ℋ0:t={\\displaystyle\\mathcal{H}_{0:t}=\\{\n",
      "(s0,o0,a0o​p1,a0,a0o​p2,…,a0o​pK),subscript𝑠0subscript𝑜0subscriptsuperscript𝑎𝑜subscript𝑝10subscript𝑎0subscriptsuperscript𝑎𝑜subscript𝑝20…subscriptsuperscript𝑎𝑜subscript𝑝𝐾0\\displaystyle({\\color[rgb]{1,0,0}s_{0}},{\\color[rgb]{0,0,1}o_{0}},a^{op_{1}}_{0},{\\color[rgb]{0,1,1}a_{0}},a^{op_{2}}_{0},...,a^{op_{K}}_{0}),\n",
      "\n",
      "(1)\n",
      "\n",
      "⋮⋮\\displaystyle\\quad\\quad\\quad\\quad\\quad\\vdots\n",
      "\n",
      "(st,ot,ato​p1,at,ato​p2,…,ato​pK)}\\displaystyle({\\color[rgb]{1,0,0}s_{t}},{\\color[rgb]{0,0,1}o_{t}},a^{op_{1}}_{t},{\\color[rgb]{0,1,1}a_{t}},a^{op_{2}}_{t},...,a^{op_{K}}_{t})\\}\n",
      "\n",
      "As shown in Figure 2, when making a decision, Agent-Pro first generates a dynamic belief ξ𝜉{\\color[rgb]{.75,0,.25}\\xi} about itself (self-belief) and opponents (world-belief) in natural language. Then, it predicts an action based on the latest beliefs. For instance, for Texas Hold’em, Agent-Pro’s understanding of its hand cards, plan, and potential risk constitutes its self-belief, while the conjectures about the opponents form its world-belief. These beliefs are updated in each decision-making cycle. Equipped with this, Agent-Pro can generate more coherent and consistent actions:\n",
      "\n",
      "ξt+1,at+1∼π​(ℋ0:t,st+1,ot+1,at+1o​p1,ξt)similar-tosubscript𝜉𝑡1subscript𝑎𝑡1𝜋subscriptℋ:0𝑡subscript𝑠𝑡1subscript𝑜𝑡1subscriptsuperscript𝑎𝑜subscript𝑝1𝑡1subscript𝜉𝑡\\displaystyle{\\color[rgb]{.75,0,.25}\\xi_{t+1}},{\\color[rgb]{0,1,1}a_{t+1}}\\sim\\pi(\\mathcal{H}_{0:t},{\\color[rgb]{1,0,0}s_{t+1}},{\\color[rgb]{0,0,1}o_{t+1}},a^{op_{1}}_{t+1},{\\color[rgb]{.75,0,.25}\\xi_{t}})\n",
      "\n",
      "(2)\n",
      "\n",
      "When a game is over, we acquire the observable state R𝑅R (e.g., private hand cards after showdown) and the final scores S𝑆S of all players. The objective is to find an optimal π∗superscript𝜋\\pi^{*} to maximize S​(o​u​r)𝑆𝑜𝑢𝑟S(our).\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': ['3.2 Policy-Level Reflection']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.2 Policy-Level Reflection\n",
      "Equipped with an initial policy (a simple prompt) and a dynamic belief, Agent-Pro already possesses basic capabilities for game exploration. To further enhance Agent-Pro’s capabilities, we design a learning mechanism via a policy-level reflection.\n",
      "\n",
      "Specifically, many text-based tasks have employed reflection strategies and immediate environmental feedback to correct prior actions. However, in many typical interaction scenarios with longer decision-making processes, action-level reflections are not directly applicable due to delayed feedback. Therefore, for such a long-horizon interaction process, Agent-Pro is instructed to focus on the rationality of beliefs and underlying behavioral policies rather than individual actions.\n",
      "\n",
      "Belief Calibration As depicted in Figure 2, under the guidance of the current behavior policy, Agent-Pro generates actions based on self-belief and world-belief. If these beliefs are inaccurate, they may lead to irrational actions and eventual failure. Therefore, Agent-Pro examines the rationality of these beliefs based on the final results and reflectss on the reasons for the final failure.\n",
      "\n",
      "⬇\n",
      "\n",
      "Correctness: Whether its beliefs about itself, the game, and its opponents align with the final results.\n",
      "\n",
      "Consistency: Whether each belief and action is self-contradictory.\n",
      "\n",
      "Rationality: Whether the beliefs accurately reflect the underlying intentions behind the opponents.\n",
      "\n",
      "Reasons: Reflect on why it lost to its opponents, which beliefs are problematic, and what the underlying reasons are.\n",
      "\n",
      "Lastly, to calibrate the incorrect beliefs, Agent-Pro summarizes these reflections and analyses about itself and the external world into specific instructions: Behavioral Guideline and World Modeling, where the former represents generalized behavioral strategies for this task, and the latter signifies its understanding and conjectures about the game world. For instance, in Texas Hold’em, Agent-Pro summarizes the following contents:\n",
      "\n",
      "⬇\n",
      "\n",
      "Behavioral Guideline\n",
      "\n",
      "1-Please summarize a detailed goal based on your reflection on beliefs. {Goal}\n",
      "\n",
      "2-What strategy helps you build correct belief and win at similar.. {Strategy}\n",
      "\n",
      "3-Can this game be considered a typical example for future... {Demonstration}\n",
      "\n",
      "World Modeling\n",
      "\n",
      "1-Accurately model each player to help build more precise beliefs about them, including action, and style.{Opponent}\n",
      "\n",
      "2-Describe any game rules or details that are easy to overlook...{Rule}\n",
      "\n",
      "Agent-Pro summarizes high-level strategies within the Behavioral Guideline and describes the task and opponents in World Modeling. These instructions can calibrate previous incorrect beliefs and improve policy performance. The entire process can be formalized as follows:\n",
      "\n",
      "Instructionn+1superscriptInstruction𝑛1\\displaystyle\\text{Instruction}^{n+1}\n",
      "←LLM(ℋ0:Tn,{ξ1n,ξ2n..},Rn,Sn)\\displaystyle\\leftarrow\\!\\text{LLM}\\left(\\mathcal{H}^{n}_{0:T},\\!\\{{\\color[rgb]{.75,0,.25}\\xi^{n}_{1},\\xi^{n}_{2}}..\\},\\!R^{n},\\!S^{n}\\right)\n",
      "\n",
      "(3)\n",
      "\n",
      "where ℋ0:Tnsubscriptsuperscriptℋ𝑛:0𝑇\\mathcal{H}^{n}_{0:T} denotes a complete trajectory at the n𝑛n-th match, {ξ1n,ξ2n,…}superscriptsubscript𝜉1𝑛superscriptsubscript𝜉2𝑛…\\{{\\color[rgb]{.75,0,.25}\\xi_{1}^{n}},{\\color[rgb]{.75,0,.25}\\xi_{2}^{n}},...\\} denotes the belief sequence, Rnsuperscript𝑅𝑛R^{n} and Snsuperscript𝑆𝑛S^{n} means the final results and score. Instructionn+1superscriptInstruction𝑛1\\text{Instruction}^{n+1} denotes new generated Behavioral Guideline and World Modeling.\n",
      "\n",
      "Verification After extracting these instructions, Agent-Pro verifies its efficacy. Agent-Pro incorporates these generated Behavioral Guideline and World Modeling into the prompt and then replays the same game again, i.e., the same opponents and initial conditions. If the final score improves, we retain them in the prompt. Otherwise, we regenerate a new one. If it fails to pass verification after three retries, we discard this trajectory ℋnsuperscriptℋ𝑛\\mathcal{H}^{n}:\n",
      "\n",
      "πn+1superscript𝜋𝑛1\\displaystyle\\pi^{n+1}\n",
      "←V​e​r​i​f​yπn∪Instructionn+1𝑉𝑒𝑟𝑖𝑓𝑦←absentsuperscript𝜋𝑛superscriptInstruction𝑛1\\displaystyle\\!\\xleftarrow{Verify}\\!\\pi^{n}\\cup\\text{Instruction}^{n+1}\n",
      "\n",
      "(4)\n",
      "\n",
      "where ∪\\cup means incorporates new instructions into the previous prompt for πn+1superscript𝜋𝑛1\\pi^{n+1}. This new policy encompasses more effective instructions, empowering Agent-Pro to establish accurate self- and world beliefs and generate more rational actions.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': ['3.3 DFS-based Policy Evolution']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.3 DFS-based Policy Evolution\n",
      "To iteratively update the policy, we devise a policy optimization process based on depth-first search (DFS). It encompasses a policy evaluation process to assess the generalization ability of the new policy in novel game scenarios and a search mechanism to progressively find a better policy.\n",
      "\n",
      "Policy Evaluation Each time the policy is updated, Agent-Pro is required to evaluate the new strategies. This evaluation process is distinct from the previous Verification step, as the Verification repeatedly utilizes the ”training” data for evaluation and can not ensure the generalizability of the new policy. Hence, Agent-Pro conducts a thorough assessment of the new policy in novel trajectories. Besides, it is imperative to eliminate the influence of random factors when policy evaluation, e.g., a poor initial hand due to bad luck or an unfavorable playing order.\n",
      "\n",
      "Therefore, we first randomly generate a new game for K​+​1𝐾+1K\\text{+}1 players. Then we sequentially swap both the hand cards and the playing order of each player, generating a total of (K​+​1)2superscript𝐾+12(K\\text{+}1)^{2} combinations. To eliminate randomness, we concurrently use these (K​+​1)2superscript𝐾+12(K\\text{+}1)^{2} games to evaluate Agent-Pro’s new policy. We calculate the average score over the (K​+​1)2superscript𝐾+12(K\\text{+}1)^{2} games for each player. Since the influences of hand-card quality and playing order are mitigated, the average score of all combinations can represent the true capabilities of each player. Lastly, we calculate the evaluating metrics:\n",
      "\n",
      "Δ=1(K​+​1)2​∑j(K+1)2[𝒮j​(our)−maxi⁡𝒮j​(opi)]Δ1superscript𝐾+12superscriptsubscript𝑗superscript𝐾12delimited-[]subscript𝒮𝑗oursubscript𝑖subscript𝒮𝑗subscriptop𝑖\\displaystyle\\Delta\\!=\\!\\frac{1}{(K\\text{+}1)^{2}}\\!\\!\\sum_{j}^{(K+1)^{2}}\\!\\!\\!\\left[\\mathcal{S}_{j}(\\text{our})\\!-\\!\\max_{i}\\mathcal{S}_{j}(\\text{op}_{i})\\right]\n",
      "\n",
      "(5)\n",
      "\n",
      "where i∈{1,…,K}𝑖1…𝐾i\\in\\{1,\\ldots,K\\} denotes the index of an opponent, and j𝑗j denotes the index of the games within (K​+​1)2superscript𝐾+12(K\\text{+}1)^{2} combinations. The ΔΔ\\Delta assesses both the absolute gains of the policy and its gains relative to the strongest opponent, providing a comprehensive evaluation in multiplayer gaming scenarios.\n",
      "\n",
      "Policy Search\n",
      "Inevitably, sometimes the new policy does not bring an improvement in ΔΔ\\Delta in the new scenario. In such cases, we employ DFS to search for a better policy from other branches (i.e., other candidate policies). As shown in Figure 2, when updating old policy πnsuperscript𝜋𝑛\\pi^{n}, we generate B𝐵B candidate policies {π1n+1\\{\\pi^{n+1}_{1}, π2n+1subscriptsuperscript𝜋𝑛12\\pi^{n+1}_{2},…, πBn+1}\\pi^{n+1}_{B}\\}, forming B𝐵B branches. Then, we first calculate Δ1n+1subscriptsuperscriptΔ𝑛11\\Delta^{n+1}_{1} for new policy π1n+1subscriptsuperscript𝜋𝑛11\\pi^{n+1}_{1} and compare it with ΔnsuperscriptΔ𝑛\\Delta^{n}. If Δ1n+1subscriptsuperscriptΔ𝑛11\\Delta^{n+1}_{1} is greater than ΔnsuperscriptΔ𝑛\\Delta^{n}, we accept this evolutionary. Otherwise, we reject π1n+1subscriptsuperscript𝜋𝑛11\\pi^{n+1}_{1} and consider π2n+1subscriptsuperscript𝜋𝑛12\\pi^{n+1}_{2}. If none of the B𝐵B candidate policies πn+1superscript𝜋𝑛1\\pi^{n+1} enhance Agent-Pro’s performance, we backtrack to πnsuperscript𝜋𝑛\\pi^{n} and consider its sibling nodes π2nsubscriptsuperscript𝜋𝑛2\\pi^{n}_{2}. Similarly, Agent-Pro explores the environment using π2nsubscriptsuperscript𝜋𝑛2\\pi^{n}_{2}, then also updates B𝐵B candidate policies and searches in a depth-first manner. Ultimately, we select the policy with the highest ΔΔ\\Delta across the entire policy tree.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m### Methods Summary\n",
      "\n",
      "#### 3.1 Belief-aware Decision-Making Process\n",
      "- **Inspiration**: Draws from the Theory of Mind (ToM), where humans form beliefs about themselves (self-belief) and the environment (social-belief) to guide decisions.\n",
      "- **Policy Definition**: The policy of an LLM-based agent involves complex prompts covering task rules, strategies, and output formats.\n",
      "- **Dynamic Belief Generation**: Agent-Pro generates dynamic beliefs about itself and opponents in natural language, updating these beliefs in each decision-making cycle.\n",
      "- **Decision Making**: Actions are predicted based on the latest beliefs, aiming for coherent and consistent actions.\n",
      "- **Objective**: To find an optimal policy that maximizes the agent's score.\n",
      "\n",
      "#### 3.2 Policy-Level Reflection\n",
      "- **Initial Capabilities**: Agent-Pro starts with a basic policy and dynamic belief for game exploration.\n",
      "- **Reflection Mechanism**: Focuses on the rationality of beliefs and underlying behavioral policies rather than individual actions.\n",
      "- **Belief Calibration**: Agent-Pro examines the correctness, consistency, and rationality of its beliefs based on final results and reflects on the reasons for failure.\n",
      "- **Instruction Generation**: Summarizes reflections into Behavioral Guidelines and World Modeling to calibrate incorrect beliefs and improve policy performance.\n",
      "- **Verification**: Incorporates new instructions into the prompt and replays the same game to verify improvements. If the score improves, the instructions are retained; otherwise, they are regenerated or discarded after three retries.\n",
      "\n",
      "#### 3.3 DFS-based Policy Evolution\n",
      "- **Policy Evaluation**: Evaluates new policies in novel game scenarios to ensure generalizability, eliminating the influence of random factors by generating multiple game combinations.\n",
      "- **Evaluation Metrics**: Calculates the average score over all combinations to represent true capabilities and assesses both absolute gains and relative gains to the strongest opponent.\n",
      "- **Policy Search**: Uses depth-first search (DFS) to explore and update policies. Generates multiple candidate policies and selects the one with the highest performance improvement. If no improvement is found, backtracks to consider sibling nodes and continues the search.\n",
      "\n",
      "These methods collectively enable Agent-Pro to evolve its policy through dynamic belief generation, reflection, and optimization, allowing it to adapt and improve in complex interactive environments.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Methods Summary\n",
       "\n",
       "#### 3.1 Belief-aware Decision-Making Process\n",
       "- **Inspiration**: Draws from the Theory of Mind (ToM), where humans form beliefs about themselves (self-belief) and the environment (social-belief) to guide decisions.\n",
       "- **Policy Definition**: The policy of an LLM-based agent involves complex prompts covering task rules, strategies, and output formats.\n",
       "- **Dynamic Belief Generation**: Agent-Pro generates dynamic beliefs about itself and opponents in natural language, updating these beliefs in each decision-making cycle.\n",
       "- **Decision Making**: Actions are predicted based on the latest beliefs, aiming for coherent and consistent actions.\n",
       "- **Objective**: To find an optimal policy that maximizes the agent's score.\n",
       "\n",
       "#### 3.2 Policy-Level Reflection\n",
       "- **Initial Capabilities**: Agent-Pro starts with a basic policy and dynamic belief for game exploration.\n",
       "- **Reflection Mechanism**: Focuses on the rationality of beliefs and underlying behavioral policies rather than individual actions.\n",
       "- **Belief Calibration**: Agent-Pro examines the correctness, consistency, and rationality of its beliefs based on final results and reflects on the reasons for failure.\n",
       "- **Instruction Generation**: Summarizes reflections into Behavioral Guidelines and World Modeling to calibrate incorrect beliefs and improve policy performance.\n",
       "- **Verification**: Incorporates new instructions into the prompt and replays the same game to verify improvements. If the score improves, the instructions are retained; otherwise, they are regenerated or discarded after three retries.\n",
       "\n",
       "#### 3.3 DFS-based Policy Evolution\n",
       "- **Policy Evaluation**: Evaluates new policies in novel game scenarios to ensure generalizability, eliminating the influence of random factors by generating multiple game combinations.\n",
       "- **Evaluation Metrics**: Calculates the average score over all combinations to represent true capabilities and assesses both absolute gains and relative gains to the strongest opponent.\n",
       "- **Policy Search**: Uses depth-first search (DFS) to explore and update policies. Generates multiple candidate policies and selects the one with the highest performance improvement. If no improvement is found, backtracks to consider sibling nodes and continues the search.\n",
       "\n",
       "These methods collectively enable Agent-Pro to evolve its policy through dynamic belief generation, reflection, and optimization, allowing it to adapt and improve in complex interactive environments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': '논문 Method의 각 요소를 요약해줘',\n",
       " 'chat_history': [HumanMessage(content='\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문 초록 요약해'),\n",
       "  AIMessage(content='**Abstract Summary:**\\n\\nLarge Language Models (LLMs) are powerful problem solvers but are often designed for specific tasks using complex prompt engineering. These task-specific designs limit their ability to handle dynamic scenarios like large interactive games. To address this, the paper introduces Agent-Pro, an LLM-based agent that evolves its policy through interactions. Agent-Pro uses a dynamic belief generation and reflection process for policy evolution, focusing on policy-level rather than action-level reflection. It employs depth-first search for policy optimization, leading to continuous improvement. Evaluated in Blackjack and Texas Hold’em, Agent-Pro outperforms standard LLMs and specialized models, demonstrating its ability to learn and adapt in complex environments.')],\n",
       " 'output': \"### Methods Summary\\n\\n#### 3.1 Belief-aware Decision-Making Process\\n- **Inspiration**: Draws from the Theory of Mind (ToM), where humans form beliefs about themselves (self-belief) and the environment (social-belief) to guide decisions.\\n- **Policy Definition**: The policy of an LLM-based agent involves complex prompts covering task rules, strategies, and output formats.\\n- **Dynamic Belief Generation**: Agent-Pro generates dynamic beliefs about itself and opponents in natural language, updating these beliefs in each decision-making cycle.\\n- **Decision Making**: Actions are predicted based on the latest beliefs, aiming for coherent and consistent actions.\\n- **Objective**: To find an optimal policy that maximizes the agent's score.\\n\\n#### 3.2 Policy-Level Reflection\\n- **Initial Capabilities**: Agent-Pro starts with a basic policy and dynamic belief for game exploration.\\n- **Reflection Mechanism**: Focuses on the rationality of beliefs and underlying behavioral policies rather than individual actions.\\n- **Belief Calibration**: Agent-Pro examines the correctness, consistency, and rationality of its beliefs based on final results and reflects on the reasons for failure.\\n- **Instruction Generation**: Summarizes reflections into Behavioral Guidelines and World Modeling to calibrate incorrect beliefs and improve policy performance.\\n- **Verification**: Incorporates new instructions into the prompt and replays the same game to verify improvements. If the score improves, the instructions are retained; otherwise, they are regenerated or discarded after three retries.\\n\\n#### 3.3 DFS-based Policy Evolution\\n- **Policy Evaluation**: Evaluates new policies in novel game scenarios to ensure generalizability, eliminating the influence of random factors by generating multiple game combinations.\\n- **Evaluation Metrics**: Calculates the average score over all combinations to represent true capabilities and assesses both absolute gains and relative gains to the strongest opponent.\\n- **Policy Search**: Uses depth-first search (DFS) to explore and update policies. Generates multiple candidate policies and selects the one with the highest performance improvement. If no improvement is found, backtracks to consider sibling nodes and continues the search.\\n\\nThese methods collectively enable Agent-Pro to evolve its policy through dynamic belief generation, reflection, and optimization, allowing it to adapt and improve in complex interactive environments.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paper_assistant('논문 Method의 각 요소를 요약해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m### 방법 요약\n",
      "\n",
      "#### 3.1 신념 기반 의사결정 과정\n",
      "- **영감**: 인간이 자신과 환경에 대한 신념을 형성하여 결정을 내리는 마음 이론(ToM)에서 영감을 받음.\n",
      "- **정책 정의**: LLM 기반 에이전트의 정책은 작업 규칙, 전략 및 출력 형식을 포함하는 복잡한 프롬프트로 구성됨.\n",
      "- **동적 신념 생성**: Agent-Pro는 자연어로 자신과 상대방에 대한 동적 신념을 생성하고, 각 의사결정 주기마다 이러한 신념을 업데이트함.\n",
      "- **의사결정**: 최신 신념을 기반으로 행동을 예측하여 일관되고 일치하는 행동을 목표로 함.\n",
      "- **목표**: 에이전트의 점수를 최대화하는 최적의 정책을 찾는 것.\n",
      "\n",
      "#### 3.2 정책 수준 반성\n",
      "- **초기 능력**: Agent-Pro는 기본 정책과 게임 탐색을 위한 동적 신념으로 시작함.\n",
      "- **반성 메커니즘**: 개별 행동보다는 신념의 합리성과 기본 행동 정책에 중점을 둠.\n",
      "- **신념 보정**: 최종 결과를 기반으로 신념의 정확성, 일관성 및 합리성을 검토하고 실패 원인에 대해 반성함.\n",
      "- **지침 생성**: 반성을 행동 지침과 세계 모델링으로 요약하여 잘못된 신념을 보정하고 정책 성능을 향상시킴.\n",
      "- **검증**: 새로운 지침을 프롬프트에 통합하고 동일한 게임을 다시 플레이하여 개선 여부를 확인함. 점수가 향상되면 지침을 유지하고, 그렇지 않으면 세 번의 재시도 후 재생성하거나 폐기함.\n",
      "\n",
      "#### 3.3 DFS 기반 정책 진화\n",
      "- **정책 평가**: 새로운 정책을 새로운 게임 시나리오에서 평가하여 일반화 가능성을 보장하고, 무작위 요소의 영향을 제거하기 위해 여러 게임 조합을 생성함.\n",
      "- **평가 지표**: 모든 조합에 대한 평균 점수를 계산하여 실제 능력을 나타내고, 절대 이득과 가장 강력한 상대에 대한 상대적 이득을 평가함.\n",
      "- **정책 탐색**: 깊이 우선 탐색(DFS)을 사용하여 정책을 탐색하고 업데이트함. 여러 후보 정책을 생성하고 성능 향상이 가장 큰 정책을 선택함. 개선이 없으면 형제 노드를 고려하여 백트래킹하고 탐색을 계속함.\n",
      "\n",
      "이 방법들은 Agent-Pro가 동적 신념 생성, 반성 및 최적화를 통해 정책을 진화시켜 복잡한 상호작용 환경에서 적응하고 개선할 수 있도록 합니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### 방법 요약\n",
       "\n",
       "#### 3.1 신념 기반 의사결정 과정\n",
       "- **영감**: 인간이 자신과 환경에 대한 신념을 형성하여 결정을 내리는 마음 이론(ToM)에서 영감을 받음.\n",
       "- **정책 정의**: LLM 기반 에이전트의 정책은 작업 규칙, 전략 및 출력 형식을 포함하는 복잡한 프롬프트로 구성됨.\n",
       "- **동적 신념 생성**: Agent-Pro는 자연어로 자신과 상대방에 대한 동적 신념을 생성하고, 각 의사결정 주기마다 이러한 신념을 업데이트함.\n",
       "- **의사결정**: 최신 신념을 기반으로 행동을 예측하여 일관되고 일치하는 행동을 목표로 함.\n",
       "- **목표**: 에이전트의 점수를 최대화하는 최적의 정책을 찾는 것.\n",
       "\n",
       "#### 3.2 정책 수준 반성\n",
       "- **초기 능력**: Agent-Pro는 기본 정책과 게임 탐색을 위한 동적 신념으로 시작함.\n",
       "- **반성 메커니즘**: 개별 행동보다는 신념의 합리성과 기본 행동 정책에 중점을 둠.\n",
       "- **신념 보정**: 최종 결과를 기반으로 신념의 정확성, 일관성 및 합리성을 검토하고 실패 원인에 대해 반성함.\n",
       "- **지침 생성**: 반성을 행동 지침과 세계 모델링으로 요약하여 잘못된 신념을 보정하고 정책 성능을 향상시킴.\n",
       "- **검증**: 새로운 지침을 프롬프트에 통합하고 동일한 게임을 다시 플레이하여 개선 여부를 확인함. 점수가 향상되면 지침을 유지하고, 그렇지 않으면 세 번의 재시도 후 재생성하거나 폐기함.\n",
       "\n",
       "#### 3.3 DFS 기반 정책 진화\n",
       "- **정책 평가**: 새로운 정책을 새로운 게임 시나리오에서 평가하여 일반화 가능성을 보장하고, 무작위 요소의 영향을 제거하기 위해 여러 게임 조합을 생성함.\n",
       "- **평가 지표**: 모든 조합에 대한 평균 점수를 계산하여 실제 능력을 나타내고, 절대 이득과 가장 강력한 상대에 대한 상대적 이득을 평가함.\n",
       "- **정책 탐색**: 깊이 우선 탐색(DFS)을 사용하여 정책을 탐색하고 업데이트함. 여러 후보 정책을 생성하고 성능 향상이 가장 큰 정책을 선택함. 개선이 없으면 형제 노드를 고려하여 백트래킹하고 탐색을 계속함.\n",
       "\n",
       "이 방법들은 Agent-Pro가 동적 신념 생성, 반성 및 최적화를 통해 정책을 진화시켜 복잡한 상호작용 환경에서 적응하고 개선할 수 있도록 합니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': '한글로 요약해',\n",
       " 'chat_history': [HumanMessage(content='\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문 초록 요약해'),\n",
       "  AIMessage(content='**Abstract Summary:**\\n\\nLarge Language Models (LLMs) are powerful problem solvers but are often designed for specific tasks using complex prompt engineering. These task-specific designs limit their ability to handle dynamic scenarios like large interactive games. To address this, the paper introduces Agent-Pro, an LLM-based agent that evolves its policy through interactions. Agent-Pro uses a dynamic belief generation and reflection process for policy evolution, focusing on policy-level rather than action-level reflection. It employs depth-first search for policy optimization, leading to continuous improvement. Evaluated in Blackjack and Texas Hold’em, Agent-Pro outperforms standard LLMs and specialized models, demonstrating its ability to learn and adapt in complex environments.'),\n",
       "  HumanMessage(content='논문 Method의 각 요소를 요약해줘'),\n",
       "  AIMessage(content=\"### Methods Summary\\n\\n#### 3.1 Belief-aware Decision-Making Process\\n- **Inspiration**: Draws from the Theory of Mind (ToM), where humans form beliefs about themselves (self-belief) and the environment (social-belief) to guide decisions.\\n- **Policy Definition**: The policy of an LLM-based agent involves complex prompts covering task rules, strategies, and output formats.\\n- **Dynamic Belief Generation**: Agent-Pro generates dynamic beliefs about itself and opponents in natural language, updating these beliefs in each decision-making cycle.\\n- **Decision Making**: Actions are predicted based on the latest beliefs, aiming for coherent and consistent actions.\\n- **Objective**: To find an optimal policy that maximizes the agent's score.\\n\\n#### 3.2 Policy-Level Reflection\\n- **Initial Capabilities**: Agent-Pro starts with a basic policy and dynamic belief for game exploration.\\n- **Reflection Mechanism**: Focuses on the rationality of beliefs and underlying behavioral policies rather than individual actions.\\n- **Belief Calibration**: Agent-Pro examines the correctness, consistency, and rationality of its beliefs based on final results and reflects on the reasons for failure.\\n- **Instruction Generation**: Summarizes reflections into Behavioral Guidelines and World Modeling to calibrate incorrect beliefs and improve policy performance.\\n- **Verification**: Incorporates new instructions into the prompt and replays the same game to verify improvements. If the score improves, the instructions are retained; otherwise, they are regenerated or discarded after three retries.\\n\\n#### 3.3 DFS-based Policy Evolution\\n- **Policy Evaluation**: Evaluates new policies in novel game scenarios to ensure generalizability, eliminating the influence of random factors by generating multiple game combinations.\\n- **Evaluation Metrics**: Calculates the average score over all combinations to represent true capabilities and assesses both absolute gains and relative gains to the strongest opponent.\\n- **Policy Search**: Uses depth-first search (DFS) to explore and update policies. Generates multiple candidate policies and selects the one with the highest performance improvement. If no improvement is found, backtracks to consider sibling nodes and continues the search.\\n\\nThese methods collectively enable Agent-Pro to evolve its policy through dynamic belief generation, reflection, and optimization, allowing it to adapt and improve in complex interactive environments.\")],\n",
       " 'output': '### 방법 요약\\n\\n#### 3.1 신념 기반 의사결정 과정\\n- **영감**: 인간이 자신과 환경에 대한 신념을 형성하여 결정을 내리는 마음 이론(ToM)에서 영감을 받음.\\n- **정책 정의**: LLM 기반 에이전트의 정책은 작업 규칙, 전략 및 출력 형식을 포함하는 복잡한 프롬프트로 구성됨.\\n- **동적 신념 생성**: Agent-Pro는 자연어로 자신과 상대방에 대한 동적 신념을 생성하고, 각 의사결정 주기마다 이러한 신념을 업데이트함.\\n- **의사결정**: 최신 신념을 기반으로 행동을 예측하여 일관되고 일치하는 행동을 목표로 함.\\n- **목표**: 에이전트의 점수를 최대화하는 최적의 정책을 찾는 것.\\n\\n#### 3.2 정책 수준 반성\\n- **초기 능력**: Agent-Pro는 기본 정책과 게임 탐색을 위한 동적 신념으로 시작함.\\n- **반성 메커니즘**: 개별 행동보다는 신념의 합리성과 기본 행동 정책에 중점을 둠.\\n- **신념 보정**: 최종 결과를 기반으로 신념의 정확성, 일관성 및 합리성을 검토하고 실패 원인에 대해 반성함.\\n- **지침 생성**: 반성을 행동 지침과 세계 모델링으로 요약하여 잘못된 신념을 보정하고 정책 성능을 향상시킴.\\n- **검증**: 새로운 지침을 프롬프트에 통합하고 동일한 게임을 다시 플레이하여 개선 여부를 확인함. 점수가 향상되면 지침을 유지하고, 그렇지 않으면 세 번의 재시도 후 재생성하거나 폐기함.\\n\\n#### 3.3 DFS 기반 정책 진화\\n- **정책 평가**: 새로운 정책을 새로운 게임 시나리오에서 평가하여 일반화 가능성을 보장하고, 무작위 요소의 영향을 제거하기 위해 여러 게임 조합을 생성함.\\n- **평가 지표**: 모든 조합에 대한 평균 점수를 계산하여 실제 능력을 나타내고, 절대 이득과 가장 강력한 상대에 대한 상대적 이득을 평가함.\\n- **정책 탐색**: 깊이 우선 탐색(DFS)을 사용하여 정책을 탐색하고 업데이트함. 여러 후보 정책을 생성하고 성능 향상이 가장 큰 정책을 선택함. 개선이 없으면 형제 노드를 고려하여 백트래킹하고 탐색을 계속함.\\n\\n이 방법들은 Agent-Pro가 동적 신념 생성, 반성 및 최적화를 통해 정책을 진화시켜 복잡한 상호작용 환경에서 적응하고 개선할 수 있도록 합니다.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paper_assistant('한글로 요약해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
