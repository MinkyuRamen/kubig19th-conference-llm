{"cells":[{"cell_type":"code","source":["!pip install transformers langchain_community langchain python-dotenv tavily-python semanticscholar openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvRoG08alVVD","executionInfo":{"status":"ok","timestamp":1716976994780,"user_tz":-540,"elapsed":37840,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"ae04a7ca-dbd3-4012-bd86-d29433c5a3e7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n","Collecting langchain_community\n","  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain\n","  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting tavily-python\n","  Downloading tavily_python-0.3.3-py3-none-any.whl (5.4 kB)\n","Collecting semanticscholar\n","  Downloading semanticscholar-0.8.1-py3-none-any.whl (24 kB)\n","Collecting openai\n","  Downloading openai-1.30.4-py3-none-any.whl (320 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n","Collecting langchain-core<0.3.0,>=0.2.0 (from langchain_community)\n","  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n","  Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n","  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n","Collecting tiktoken<1,>=0.5.2 (from tavily-python)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from semanticscholar)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (1.6.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (2024.2.2)\n","Collecting httpcore==1.* (from httpx->semanticscholar)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->semanticscholar)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain_community)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting packaging>=20.0 (from transformers)\n","  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_community)\n","  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m841.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, packaging, orjson, mypy-extensions, jsonpointer, h11, typing-inspect, tiktoken, marshmallow, jsonpatch, httpcore, tavily-python, langsmith, httpx, dataclasses-json, semanticscholar, openai, langchain-core, langchain-text-splitters, langchain, langchain_community\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 24.0\n","    Uninstalling packaging-24.0:\n","      Successfully uninstalled packaging-24.0\n","Successfully installed dataclasses-json-0.6.6 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0 langchain_community-0.2.1 langsmith-0.1.63 marshmallow-3.21.2 mypy-extensions-1.0.0 openai-1.30.4 orjson-3.10.3 packaging-23.2 python-dotenv-1.0.1 semanticscholar-0.8.1 tavily-python-0.3.3 tiktoken-0.7.0 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ko0ECE2DlSps","executionInfo":{"status":"ok","timestamp":1716977011342,"user_tz":-540,"elapsed":16566,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"a4943544-a50f-4090-994a-49bf94e28750"},"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import torch\n","\n","import os\n","import numpy as np\n","import requests\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from pprint import pprint\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n","from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n","from transformers import LlamaForCausalLM\n","from transformers import PreTrainedTokenizerFast\n","from huggingface_hub import login\n","login(token=\"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\")\n","\n","import os\n","from dotenv import load_dotenv\n","import pickle\n","import IPython\n","\n","import requests\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.tokenize import word_tokenize\n","import numpy as np\n","\n","from langchain_community.retrievers import WikipediaRetriever\n","from langchain.chains import ConversationalRetrievalChain\n","# from langchain_openai import ChatOpenAI\n","from langchain.adapters.openai import convert_openai_messages\n","from langchain_community.chat_models import ChatOpenAI\n","from tavily import TavilyClient\n","from langchain_community.retrievers import TavilySearchAPIRetriever\n","\n","from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n","from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n","\n","from langchain_community.utilities import GoogleSerperAPIWrapper\n","\n","from langchain.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain.callbacks import get_openai_callback\n","\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","import torch.nn.functional as F\n","\n","import datetime\n","\n","from langchain.llms.base import LLM\n","from langchain.callbacks.manager import CallbackManagerForLLMRun\n","from typing import Optional, List, Mapping, Any\n","\n","from dotenv import load_dotenv\n","\n","\n","from datetime import datetime"]},{"cell_type":"code","source":["!pip install semanticscholar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sd2GH3DKoOBR","executionInfo":{"status":"ok","timestamp":1716977025156,"user_tz":-540,"elapsed":13849,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"90a78a19-40d4-46b3-c5eb-3362af3bb06a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: semanticscholar in /usr/local/lib/python3.10/dist-packages (0.8.1)\n","Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (8.3.0)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (0.27.0)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (1.6.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (1.0.5)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (3.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->semanticscholar) (0.14.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->semanticscholar) (1.2.1)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vlL8kGuomRw","executionInfo":{"status":"ok","timestamp":1716977049197,"user_tz":-540,"elapsed":24086,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"cd409ac5-2df2-4327-e248-7ba0ad96e81a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Projects/kubig19th-conference-llm/ym"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bxKx-G6o0Sa","executionInfo":{"status":"ok","timestamp":1716977049669,"user_tz":-540,"elapsed":478,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"54623eff-3c63-4910-cb76-ac8655c836ef"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Projects/kubig19th-conference-llm/ym\n"]}]},{"cell_type":"code","source":["import semantic_scholoar_api as ss"],"metadata":{"id":"C8mpxXgQoMXt","executionInfo":{"status":"ok","timestamp":1716977055753,"user_tz":-540,"elapsed":6088,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"colab":{"base_uri":"https://localhost:8080/","height":302,"referenced_widgets":["fa38c9636e854277a5cd1d41d97d2dce","0b789400b0af41dfaaddd231ba586a95","d6a24b1a1a6244569a52b6af9c3656f1","971adde303f6483795b339c0115c7d94","c70434340ff24d8f8d18b15f8eb64364","73ba67b286b34e0e8806f192908f7a2e","95a40de37ccf4f2eb9565cb005a68efe","7a451cc62ee14f3597ec0c787fdc1ce3","5eb81579a10848018fb99f2ef2535e90","ceccf734d0f44e6d8c8d33e2cf042cae","fac4ed733ac945fe903c789a20aa0acb","28d6e41c27314210910b957649a61433","ef2a465a3fdb42c09e334609a7b7c994","fe118937612d487296807c15c84bbce7","1e256a8d2b28418e8f20a95bb246dec6","442865e15ce5408dbb3aeb457a139afd","201dae1b7fea4eeaae3a65649a9a8323","95e8500a72064e50bcfe4f4fdbcc66fa","cb70a981ba464115bd87c31e3ae4eb07","c18ba66e9e9949bdbcda325297674d50","40935025d74a435dacbfd366dd1324ca","ac094f1fce444af09b3f3c4af6b275ec","72769211827f4ea0b6847360665a577b","39acd3ccf7b14790883a542dabfb6075","6c3da03c1341494c9f55d26be22ee580","521214760aa74b95b4fd4fb048ca8c93","6aa8376fda4a4ea7afb30bf23522cee5","3f6acf546c464cd2b155f9211dbc520c","5f5dc555d6dd4450aada9585d2251e22","9e8902d5c630449cb5a1eaef8aa073ea","c7df3b71792442c6a8d2a2b2dd9e54e2","de557eae54474d769a5828e4371c3018","3982b22fe18d427592be448bf91dce55","caf880807c584df89a3301f96e4c5a8b","7f9d4b69e41c44cdb624034e8fb4b3e7","06d332146e34467b9fe460d1ed05c66d","1a266be8f24845eb98469bc4b7ef0e4b","0d1136efdca840d28cd8ec993234b83c","41e7a22d0bf041c596ef74e32f4d8cfc","8be4ce3eb510405d850510f01009305d","92fc195eed044ac3acf6a3fffa90c29a","0a91c23ddb4745fe98ba194553a961c1","781ed289bb2940108ce0091afa4a42c2","dcea85781d014bcea0ffaf5f1852e66c","4f7b19e72feb481f97a51f899813d05d","e3fe182f10a2445aab1509ba7b9d9540","a10861c746304f3a814a78ceb08bd491","6f7bc536b6ad4f01a8107dad8e12314b","599f007c541f4f2faa3c03c769fc1881","3b96b8bd3f8148b8b019725c827f824a","8e74f605b7b048b08ecb37f86229d0af","00dcbd29b45c49888f0eb80946aa2d09","0bb097a01a174b15af9717911c1992b8","138983fb4ed34e76a216e08019053822","1be1e4e49dc742c988e5faaec5bb8a21","9b566dd5da324dac8b835da73884e691","123fc8f3d4004c4da4fc8d91bd6236e3","a2f588fc1a5c46a3936fb027aeff6508","b91c6629fc4947ae956a415b5c283d63","966905699ee341c6a44e130cce8dcdf8","e484f61007e64dc4aae494a59c2c9ea9","8aed86cf700e44e68038439446348074","c3567bbd98a5487eb3377cd152bd1dd1","9ee86e2a1ca049a8a3cf4cfb6de1f35e","5b6dcc12f97a4c19ac24c42e16f395d7","f9b776dca6b64e0bb34746ce2de6dbfe"]},"outputId":"5461421b-4447-4388-b5c0-da5a5983f9c2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa38c9636e854277a5cd1d41d97d2dce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d6e41c27314210910b957649a61433"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72769211827f4ea0b6847360665a577b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caf880807c584df89a3301f96e4c5a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f7b19e72feb481f97a51f899813d05d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b566dd5da324dac8b835da73884e691"}},"metadata":{}}]},{"cell_type":"code","source":["dotenv_path = '/content/drive/MyDrive/.env'\n","load_dotenv(dotenv_path)\n","openai_api = os.getenv(\"OPENAI_API_KEY\")\n","api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n","# api_key"],"metadata":{"id":"msKjY8-qn1oh","executionInfo":{"status":"ok","timestamp":1716977056278,"user_tz":-540,"elapsed":532,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffMzk5BVlSqC","executionInfo":{"status":"ok","timestamp":1716977072202,"user_tz":-540,"elapsed":15931,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"7545a57e-9c0b-41e0-d4e9-8403b35b8fc1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0',\n","  'title': 'Language Models are Few-Shot Learners',\n","  'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\",\n","  'influentialCitationCount': 3099,\n","  'publicationDate': '2020-05-28',\n","  'intent': 'background result methodology',\n","  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n"," {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb',\n","  'title': 'PaLM: Scaling Language Modeling with Pathways',\n","  'abstract': 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.',\n","  'influentialCitationCount': 300,\n","  'publicationDate': '2022-04-05',\n","  'intent': 'background',\n","  'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'},\n"," {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221',\n","  'title': 'OPT: Open Pre-trained Transformer Language Models',\n","  'abstract': 'Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.',\n","  'influentialCitationCount': 266,\n","  'publicationDate': '2022-05-02',\n","  'intent': 'result',\n","  'context': 'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'},\n"," {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n","  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n","  'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n","  'influentialCitationCount': 18133,\n","  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n","  'intent': 'background',\n","  'context': 'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.'},\n"," {'paperId': '9405cc0d6169988371b2755e573cc28650d14dfe',\n","  'title': 'Language Models are Unsupervised Multitask Learners',\n","  'abstract': 'Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.',\n","  'influentialCitationCount': 2812,\n","  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n","  'intent': 'methodology',\n","  'context': 'To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively.'}]"]},"metadata":{},"execution_count":8}],"source":["query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n","num = 20\n","threshold = 0.6\n","recommend = 5\n","\n","# recommend 개의 가장 연관된 reference 추천\n","reference = ss.reference_recommend(query=query, num=num, threshold=threshold, recommend=recommend, api_key=api_key)\n","reference"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"LV2RkcwNlSqI","outputId":"f8d322f4-c6cc-4e85-821c-f04884193d06","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716977073119,"user_tz":-540,"elapsed":12,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Language Models are Few-Shot Learners': '2020-05-28',\n"," 'PaLM: Scaling Language Modeling with Pathways': '2022-04-05',\n"," 'OPT: Open Pre-trained Transformer Language Models': '2022-05-02',\n"," 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),\n"," 'Language Models are Unsupervised Multitask Learners': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}"]},"metadata":{},"execution_count":9}],"source":["{ref['title'] : ref['publicationDate'] for ref in reference}"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"QrFKXe-qlSqO","outputId":"4b530750-c1a5-4a0d-bedb-c86ec78f5bbf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716977084521,"user_tz":-540,"elapsed":11412,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens Used: 2658\n","\tPrompt Tokens: 2099\n","\tCompletion Tokens: 559\n","Successful Requests: 1\n","Total Cost (USD): $0.0042665\n"]}],"source":["parser = StrOutputParser()\n","content = reference\n","\n","# 논문 제목 (날짜순서로 정렬)\n","ref_paper = list({ref['title'] : ref['publicationDate'] for ref in reference}.keys())[0]\n","\n","# 프롬프트\n","system_template = \"\"\"\n","    You are an AI research assistant tasked with creating structured reports.\n","    Your sole purpose is to write well written, critically acclaimed.\n","    objective and structured reports on given text.\n","    \"\"\"\n","human_template = \"\"\"\n","    Content: {content}\n","    Query: Answering below questions based on the content.\n","            1. Explain why the {ref_paper} was mentioned in {query}. using \"intent\" & \"context part\n","            2. Briefly explain the abstract of the paper {ref_paper}\n","    \"\"\"\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", system_template),\n","    (\"human\", human_template)\n","])\n","model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n","\n","with get_openai_callback() as cb:\n","    chain =  prompt | model | parser\n","    report = chain.invoke({\n","    \"content\": content,\n","    \"ref_paper\": ref_paper,\n","    \"query\": query\n","    })\n","print(cb)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"oS9OWBj-lSqS","outputId":"060fe758-7713-42e1-a000-8c5bca170ff8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716977084522,"user_tz":-540,"elapsed":29,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["('**Report:**\\n'\n"," '\\n'\n"," '1. **Explanation of why \"Language Models are Few-Shot Learners\" was '\n"," 'mentioned in \"Toolformer: Language Models Can Teach Themselves to Use '\n"," 'Tools\"**:\\n'\n"," '\\n'\n"," 'In the context of the paper \"Toolformer: Language Models Can Teach '\n"," 'Themselves to Use Tools,\" the mention of \"Language Models are Few-Shot '\n"," 'Learners\" is likely to provide background and support for the idea of '\n"," 'language models being able to quickly adapt and learn from limited examples. '\n"," 'The intent of mentioning the paper could be to highlight the advancements in '\n"," 'natural language processing (NLP) tasks achieved by pre-training large '\n"," 'language models and fine-tuning them on specific tasks. This method enables '\n"," 'language models to exhibit few-shot learning capabilities, where they can '\n"," 'perform new tasks with minimal training data, akin to how humans can learn '\n"," 'from a few examples or simple instructions.\\n'\n"," '\\n'\n"," 'The context part of the mention may emphasize the significance of large '\n"," 'language models in achieving impressive zero and few-shot results across '\n"," 'various NLP tasks, underscoring the importance of understanding and '\n"," 'leveraging these capabilities in the development of tools or systems like '\n"," 'the Toolformer project.\\n'\n"," '\\n'\n"," '2. **Abstract of the paper \"Language Models are Few-Shot Learners\"**:\\n'\n"," '\\n'\n"," 'The abstract of the paper \"Language Models are Few-Shot Learners\" outlines '\n"," 'the advancements in language modeling achieved through scaling up models, '\n"," 'particularly focusing on GPT-3, an autoregressive language model with 175 '\n"," 'billion parameters. The paper demonstrates that by increasing the scale of '\n"," 'language models, their task-agnostic few-shot performance greatly improves, '\n"," 'sometimes even matching or surpassing prior state-of-the-art fine-tuning '\n"," 'approaches.\\n'\n"," '\\n'\n"," 'Key points from the abstract include:\\n'\n"," '- Highlighting the challenges faced by current NLP systems in performing new '\n"," 'language tasks with limited examples compared to human capabilities.\\n'\n"," '- Describing how GPT-3 achieves strong few-shot performance on various NLP '\n"," 'datasets without any gradient updates or fine-tuning, solely relying on text '\n"," 'interaction with the model.\\n'\n"," \"- Showcasing GPT-3's performance in tasks such as translation, \"\n"," 'question-answering, cloze tasks, on-the-fly reasoning, domain adaptation, '\n"," 'and more.\\n'\n"," '- Identifying datasets where GPT-3 struggles with few-shot learning and '\n"," 'methodological issues related to training on large web corpora.\\n'\n"," \"- Noting GPT-3's ability to generate news article samples indistinguishable \"\n"," 'from human-written articles and discussing the broader societal impacts of '\n"," 'this capability.\\n'\n"," '\\n'\n"," 'Overall, the abstract highlights the significant advancements in few-shot '\n"," 'learning capabilities of large language models and the potential '\n"," 'implications for various NLP tasks and societal implications.\\n'\n"," '\\n'\n"," 'This report provides a structured analysis of the relevance of \"Language '\n"," 'Models are Few-Shot Learners\" in the context of the Toolformer project and a '\n"," 'brief overview of the abstract of the paper.')\n"]}],"source":["pprint(report)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"C-rrieuklSqV","outputId":"ccfe30ea-4bb7-48a2-b84f-1becbdbf61a1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716977095076,"user_tz":-540,"elapsed":10580,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'paperId': '7d8905a1fd288068f12c8347caeabefd36d0dd6c',\n","  'title': 'Gorilla: Large Language Model Connected with Massive APIs',\n","  'abstract': \"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\",\n","  'influentialCitationCount': 26,\n","  'publicationDate': '2023-05-24'},\n"," {'paperId': '58f8925a8b87054ad0635a6398a7fe24935b1604',\n","  'title': 'Mind2Web: Towards a Generalist Agent for the Web',\n","  'abstract': 'We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.',\n","  'influentialCitationCount': 22,\n","  'publicationDate': '2023-06-09'},\n"," {'paperId': '0bfc804e31eecfd77f45e4ee7f4d629fffdcd628',\n","  'title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs',\n","  'abstract': 'Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.',\n","  'influentialCitationCount': 47,\n","  'publicationDate': '2023-07-31'},\n"," {'paperId': '5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0',\n","  'title': 'Qwen Technical Report',\n","  'abstract': 'Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.',\n","  'influentialCitationCount': 54,\n","  'publicationDate': '2023-09-28'},\n"," {'paperId': 'd1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43',\n","  'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face',\n","  'abstract': 'Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.',\n","  'influentialCitationCount': 49,\n","  'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}]"]},"metadata":{},"execution_count":12}],"source":["query = 'Toolformer: Language Models Can Teach Themselves to Use Tools'\n","num = 20\n","threshold = 0.6\n","recommend = 5\n","\n","# recommend 개의 가장 연관된 citation 추천\n","citation = ss.citation_recommend(query=query, num=num, threshold=threshold, recommend=recommend, api_key=api_key)\n","citation"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"7huSWtqElSqX","outputId":"05db1672-4384-4555-eda6-9110ec155740","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716977095393,"user_tz":-540,"elapsed":321,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Gorilla: Large Language Model Connected with Massive APIs': '2023-05-24',\n"," 'Mind2Web: Towards a Generalist Agent for the Web': '2023-06-09',\n"," 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs': '2023-07-31',\n"," 'Qwen Technical Report': '2023-09-28',\n"," 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}"]},"metadata":{},"execution_count":13}],"source":["{cit['title'] : cit['publicationDate'] for cit in citation}"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Sp3dlN21lSqa","outputId":"2db33656-c691-4ada-bfc3-9c4f7c1a38dd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716977108865,"user_tz":-540,"elapsed":13475,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens Used: 2852\n","\tPrompt Tokens: 2107\n","\tCompletion Tokens: 745\n","Successful Requests: 1\n","Total Cost (USD): $0.0046505\n"]}],"source":["parser = StrOutputParser()\n","content = citation\n","\n","# 논문 제목 (날짜순서로 정렬)\n","cit_paper = list({cit['title'] : cit['publicationDate'] for cit in citation}.keys())\n","\n","# 프롬프트\n","system_template = \"\"\"\n","    You are an AI research assistant tasked with creating structured reports.\n","    Your sole purpose is to write well written, critically acclaimed.\n","    objective and structured reports on given text.\n","    \"\"\"\n","human_template = \"\"\"\n","    Content: {content}\n","    Query: Answering below questions based on the content.\n","            1. Explain why the {query} was mententioned in {cit_paper}.\n","            2. Briefly explain the abstract of the paper {cit_paper}\n","    \"\"\"\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", system_template),\n","    (\"human\", human_template)\n","])\n","model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n","\n","with get_openai_callback() as cb:\n","    chain =  prompt | model | parser\n","    report = chain.invoke({\n","    \"content\": content,\n","    \"cit_paper\": cit_paper,\n","    \"query\": query\n","    })\n","print(cb)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Vu0JtBNQlSqd","outputId":"b01f5d76-2d33-4997-b777-f1a49b352179","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716977108866,"user_tz":-540,"elapsed":40,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["('**1. Explanation of the Toolformer Mention in the Given Papers:**\\n'\n"," '\\n'\n"," 'The Toolformer: Language Models Can Teach Themselves to Use Tools is '\n"," 'referenced in the context of the advancements in large language models '\n"," '(LLMs) and their capabilities in utilizing tools via API calls. The papers '\n"," \"['Gorilla: Large Language Model Connected with Massive APIs', 'Mind2Web: \"\n"," \"Towards a Generalist Agent for the Web', 'ToolLLM: Facilitating Large \"\n"," \"Language Models to Master 16000+ Real-world APIs', 'Qwen Technical Report', \"\n"," \"'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face'] \"\n"," 'all discuss the enhancement of LLMs in understanding and executing tasks '\n"," 'involving tool usage, similar to the concept of Toolformer.\\n'\n"," '\\n'\n"," 'In the mentioned papers, the authors introduce novel models and frameworks, '\n"," 'such as Gorilla, Mind2Web, ToolLLM, Qwen, and HuggingGPT, that aim to '\n"," \"improve LLMs' ability to effectively interact with tools and APIs. These \"\n"," 'models focus on refining the tool-use capabilities of LLMs, addressing '\n"," 'challenges like accurate input argument generation, handling API calls, '\n"," 'adapting to dynamic document changes, and mitigating hallucination errors. '\n"," 'The reference to Toolformer likely signifies the shared goal of empowering '\n"," 'LLMs to autonomously learn and adapt to using tools efficiently, aligning '\n"," \"with the broader theme of enhancing LLMs' practical applicability and \"\n"," 'reliability in real-world tasks.\\n'\n"," '\\n'\n"," '**2. Brief Abstract Summary of the Selected Papers:**\\n'\n"," '\\n'\n"," \"**'Gorilla: Large Language Model Connected with Massive APIs'**:\\n\"\n"," 'The paper introduces Gorilla, a finetuned LLaMA-based model that excels in '\n"," 'writing API calls by surpassing the performance of GPT-4. Gorilla, when '\n"," 'combined with a document retriever, showcases adaptability to document '\n"," 'changes, reduces hallucination, and enhances the accuracy of tool '\n"," 'utilization by LLMs.\\n'\n"," '\\n'\n"," \"**'Mind2Web: Towards a Generalist Agent for the Web'**:\\n\"\n"," 'This work presents Mind2Web, a dataset for developing generalist agents for '\n"," 'the web. It features real-world tasks from diverse websites, enabling the '\n"," 'evaluation of large language models in completing complex web-based '\n"," 'instructions. The study explores the effectiveness of LLMs in web task '\n"," 'completion and emphasizes the need for further advancements in generalizable '\n"," 'web agents.\\n'\n"," '\\n'\n"," \"**'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world \"\n"," \"APIs'**:\\n\"\n"," \"The paper introduces ToolLLM, a framework designed to enhance LLMs' tool-use \"\n"," 'capabilities by providing a dataset (ToolBench) for instruction tuning with '\n"," 'real-world APIs. ToolLLM leverages a decision tree algorithm and an '\n"," 'automatic evaluator (ToolEval) to fine-tune LLaMA for improved tool '\n"," 'utilization, showcasing strong performance in executing complex instructions '\n"," 'and generalizing to unseen APIs.\\n'\n"," '\\n'\n"," \"**'Qwen Technical Report'**:\\n\"\n"," 'Qwen is a series of large language models tailored for diverse tasks, '\n"," 'including base language models and chat models finetuned with human '\n"," 'alignment techniques. The models exhibit superior performance in various '\n"," 'downstream tasks, showcasing advanced tool-use and planning capabilities, '\n"," 'particularly in creating agent applications and handling complex tasks '\n"," 'efficiently.\\n'\n"," '\\n'\n"," \"**'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging \"\n"," \"Face'**:\\n\"\n"," 'The paper introduces HuggingGPT, an LLM-powered agent that leverages ChatGPT '\n"," 'to connect various AI models in solving complex AI tasks. By utilizing '\n"," 'ChatGPT for task planning and integrating with diverse AI models in Hugging '\n"," 'Face, HuggingGPT demonstrates impressive results in language understanding, '\n"," 'vision, speech, and other AI tasks, contributing towards artificial general '\n"," 'intelligence development.')\n"]}],"source":["pprint(report)"]},{"cell_type":"code","source":["# 1 논문의 각 파트 요약해줘 → introduction , background&method , experiments\n","!pip install feedparser PyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O3jc5cydF_O3","executionInfo":{"status":"ok","timestamp":1716977118464,"user_tz":-540,"elapsed":9634,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"f22da99c-52c3-4319-b0d3-f7c54dbdab8b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting feedparser\n","  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sgmllib3k (from feedparser)\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: sgmllib3k\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=108b64c88937f7878bbc7fdc5d0516096e9b891eb225d7c5900ff3a7fca3cc19\n","  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n","Successfully built sgmllib3k\n","Installing collected packages: sgmllib3k, PyPDF2, feedparser\n","Successfully installed PyPDF2-3.0.1 feedparser-6.0.11 sgmllib3k-1.0.0\n"]}]},{"cell_type":"code","source":["import requests\n","import feedparser\n","import PyPDF2\n","import re\n","\n","# arxiv api 이용, paper search\n","def search_arxiv(query, max_results=1):\n","    base_url = 'http://export.arxiv.org/api/query?'\n","    query_url = f'search_query=all:{query}&start=0&max_results={max_results}'\n","    response = requests.get(base_url + query_url)\n","\n","    if response.status_code != 200:\n","        raise Exception('Error fetching data from arXiv API')\n","\n","    feed = feedparser.parse(response.content)\n","    return feed.entries\n","\n","# query paper pdf download\n","def download_pdf(arxiv_id, save_path):\n","    pdf_url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n","    response = requests.get(pdf_url)\n","\n","    if response.status_code != 200:\n","        raise Exception('Error downloading PDF from arXiv')\n","\n","    with open(save_path, 'wb') as file:\n","        file.write(response.content)\n","\n","# paper pdf read\n","# todo : start_page, end_page\n","def read_pdf(file_path, limit_page=9):\n","    pdf_content = \"\"\n","    with open(file_path, 'rb') as file:\n","        reader = PyPDF2.PdfReader(file)\n","        for page_num in range(len(reader.pages)):\n","            page = reader.pages[page_num]\n","            pdf_content += page.extract_text()\n","            if page_num == limit_page:\n","                print('page limit is 9')\n","                break\n","\n","    pdf_content = re.sub(r'\\s+', ' ', pdf_content).strip()\n","    return pdf_content\n","\n","def query_to_content(query):\n","    papers = search_arxiv(query, max_results=1)\n","    for paper in papers:\n","        print(f'Title: {paper.title}')\n","        print(f'Authors: {paper.author}')\n","        print(f'Published: {paper.published}')\n","        print(f'ID: {paper.id.split(\"/\")[-1]}')\n","\n","        arxiv_id = paper.id.split('/')[-1]\n","        save_path = f'{arxiv_id}.pdf'\n","        download_pdf(arxiv_id, save_path)\n","        print(f'PDF downloaded to {save_path}')\n","        content = read_pdf(save_path)\n","    return content"],"metadata":{"id":"92s5Oa3OF_RC","executionInfo":{"status":"ok","timestamp":1716977118464,"user_tz":-540,"elapsed":6,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# issue : arxiv query doesn't work well.\n","\n","query = 'Language Models Can Teach Themselves to Use Tools'\n","parser = StrOutputParser()\n","content = query_to_content(query)\n","pprint(content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26daBg_uLTM8","executionInfo":{"status":"ok","timestamp":1716978300131,"user_tz":-540,"elapsed":3533,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"49ad0a31-a0e2-4e21-dcd1-f4b34bdec5de"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Title: Toolformer: Language Models Can Teach Themselves to Use Tools\n","Authors: Thomas Scialom\n","Published: 2023-02-09T16:49:57Z\n","ID: 2302.04761v1\n","PDF downloaded to 2302.04761v1.pdf\n","page limit is 9\n","('Toolformer: Language Models Can Teach Themselves to Use Tools Timo Schick '\n"," 'Jane Dwivedi-Yu Roberto DessìyRoberta Raileanu Maria Lomeli Luke Zettlemoyer '\n"," 'Nicola Cancedda Thomas Scialom Meta AI ResearchyUniversitat Pompeu Fabra '\n"," 'Abstract Language models (LMs) exhibit remarkable abilities to solve new '\n"," 'tasks from just a few examples or textual instructions, especially at scale. '\n"," 'They also, paradoxically, struggle with basic functionality, such as '\n"," 'arithmetic or fac- tual lookup, where much simpler and smaller models excel. '\n"," 'In this paper, we show that LMs can teach themselves to use external tools '\n"," 'via simple APIs and achieve the best of both worlds. We introduce Toolformer '\n"," ', a model trained to decide which APIs to call, when to call them, what '\n"," 'arguments to pass, and how to best incorporate the results into future token '\n"," 'prediction. This is done in a self-supervised way, requiring nothing more '\n"," 'than a handful of demonstrations for each API. We incorporate a range of '\n"," 'tools, including a calculator, a Q&A system, a search engine, a translation '\n"," 'system, and a calendar. Toolformer achieves substan- tially improved '\n"," 'zero-shot performance across a variety of downstream tasks, often competi- '\n"," 'tive with much larger models, without sacriﬁc- ing its core language '\n"," 'modeling abilities. 1 Introduction Large language models achieve impressive '\n"," 'zero- and few-shot results on a variety of natural lan- guage processing '\n"," 'tasks (Brown et al., 2020; Chowd- hery et al., 2022, i.a.) and show several '\n"," 'emergent capabilities (Wei et al., 2022). However, all of these models have '\n"," 'several inherent limitations that can at best be partially addressed by '\n"," 'further scal- ing. These limitations include an inability to access '\n"," 'up-to-date information on recent events (Komeili et al., 2022) and the '\n"," 'related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., '\n"," '2022), difﬁcul- ties in understanding low-resource languages (Lin et al., '\n"," '2021), a lack of mathematical skills to per- form precise calculations '\n"," '(Patel et al., 2021) and an unawareness of the progression of time (Dhingra '\n"," 'et al., 2022). The New England Journal of Medicine is a registered trademark '\n"," 'of [QA(“Who is the publisher of The New England Journal of Medicine?”) → '\n"," 'Massachusetts Medical Society] the MMS. Out of 1400 participants, 400 (or '\n"," '[Calculator(400 / 1400) → 0.29] 29%) passed the test. The name derives from '\n"," '“la tortuga”, the Spanish word for [MT(“tortuga”) → turtle] turtle. The '\n"," 'Brown Act is California’s law [WikiSearch(“Brown Act”) → The Ralph M. Brown '\n"," 'Act is an act of the California State Legislature that guarantees the '\n"," \"public's right to attend and participate in meetings of local legislative \"\n"," 'bodies.] that requires legislative bodies, like city councils, to hold their '\n"," 'meetings open to the public. Figure 1: Exemplary predictions of Toolformer. '\n"," 'The model autonomously decides to call different APIs (from top to bottom: a '\n"," 'question answering system, a calculator, a machine translation system, and a '\n"," 'Wikipedia search engine) to obtain information that is useful for completing '\n"," 'a piece of text. A simple way to overcome these limitations of today’s '\n"," 'language models is to give them the abil- ity to use external tools such as '\n"," 'search engines, calculators, or calendars. However, existing ap- proaches '\n"," 'either rely on large amounts of human annotations (Komeili et al., 2022; '\n"," 'Thoppilan et al., 2022) or limit tool use to task-speciﬁc settings only '\n"," '(e.g., Gao et al., 2022; Parisi et al., 2022), hinder- ing a more widespread '\n"," 'adoption of tool use in LMs. Therefore, we propose Toolformer , a model that '\n"," 'learns to use tools in a novel way, which fulﬁlls the following desiderata: '\n"," '•The use of tools should be learned in a self-supervised way without '\n"," 'requiring large amounts of human annotations . This is '\n"," 'impor-arXiv:2302.04761v1 [cs.CL] 9 Feb 2023x1: i-1 = Pittsburgh is also '\n"," 'known as xi: n = the Steel City x* = Pittsburgh is also known as [QA(What …? '\n"," '→ Steel City)] the Steel City. ci1 = What other name is Pittsburgh known by? '\n"," 'ci2 = Which country is Pittsburgh in? ri1 = Steel City ri2 = United States '\n"," 'Li( ci1 → Steel City ) < min( Li( ci1 → ε), Li(ε)) Li( ci2 → United States ) '\n"," '> min( Li( ci2 → ε), Li(ε))1 Sample API Calls 2 Execute API Calls 3 Filter '\n"," 'API Calls LM Dataset LM Dataset with API Calls Figure 2: Key steps in our '\n"," 'approach, illustrated for a question answering tool: Given an input text x, '\n"," 'we ﬁrst sample a position iand corresponding API call candidates c1 i;c2 '\n"," 'i;:::;ck i. We then execute these API calls and ﬁlter out all calls which do '\n"," 'not reduce the loss Liover the next tokens. All remaining API calls are '\n"," 'interleaved with the original text, resulting in a new text x\\x03. tant not '\n"," 'only because of the costs associated with such annotations, but also because '\n"," 'what humans ﬁnd useful may be different from what a model ﬁnds useful. •The '\n"," 'LM should not lose any of its generality and should be able to decide for '\n"," 'itself when andhow to use which tool. In contrast to existing approaches, '\n"," 'this enables a much more comprehensive use of tools that is not tied to '\n"," 'speciﬁc tasks. Our approach for achieving these goals is based on the recent '\n"," 'idea of using large LMs with in- context learning (Brown et al., 2020) to '\n"," 'generate entire datasets from scratch (Schick and Schütze, 2021b; Honovich '\n"," 'et al., 2022; Wang et al., 2022): Given just a handful of human-written '\n"," 'examples of how an API can be used, we let a LM annotate a huge language '\n"," 'modeling dataset with potential API calls. We then use a self-supervised '\n"," 'loss to determine which of these API calls actually help the model in '\n"," 'predicting future tokens. Finally, we ﬁnetune the LM itself on the API calls '\n"," 'that it con- siders useful. As illustrated in Figure 1, through this simple '\n"," 'approach, LMs can learn to control a va- riety of tools, and to choose for '\n"," 'themselves which tool to use when and how. As our approach is agnostic of '\n"," 'the dataset be- ing used, we can apply it to the exact same dataset that was '\n"," 'used to pretrain a model in the ﬁrst place. This ensures that the model does '\n"," 'not lose any of its generality and language modeling abilities. We conduct '\n"," 'experiments on a variety of differ- ent downstream tasks, demonstrating that '\n"," 'after learning to use tools, Toolformer, which is based on a pretrained '\n"," 'GPT-J model (Wang and Komat- suzaki, 2021) with 6.7B parameters, achieves '\n"," 'much stronger zero-shot results, clearly outperforming a much larger GPT-3 '\n"," 'model (Brown et al., 2020) andseveral other baselines on various tasks. 2 '\n"," 'Approach Our aim is to equip a language model Mwith the ability to use '\n"," 'different tools by means of API calls. We require that inputs and outputs '\n"," 'for each API can be represented as text sequences. This allows seamless '\n"," 'insertion of API calls into any given text, using special tokens to mark the '\n"," 'start and end of each such call. We represent each API call as a tuple c= '\n"," '(ac;ic) whereacis the name of the API and icis the cor- responding input. '\n"," 'Given an API call cwith a cor- responding result r, we denote the linearized '\n"," 'se- quences of the API call not including and including its result, '\n"," 'respectively, as: e(c) =<API>ac(ic) </API> e(c;r) =<API>ac(ic)!r</API> where '\n"," '“ <API> ”, “</API> ” and “!” are special tokens.1Some examples of linearized '\n"," 'API calls inserted into text sequences are shown in Figure 1. Given a '\n"," 'datasetC=fx1;:::; xjCjgof plain texts, we ﬁrst convert this dataset into a '\n"," 'dataset C\\x03augmented with API calls. This is done in three steps, '\n"," 'illustrated in Figure 2: First, we exploit the in-context learning ability '\n"," 'of Mto sample a large number of potential API calls. We then execute these '\n"," 'API calls and ﬁnally check whether the ob- tained responses are helpful for '\n"," 'predicting future tokens; this is used as a ﬁltering criterion. After '\n"," 'ﬁltering, we merge API calls for different tools, resulting in the augmented '\n"," 'dataset C\\x03, and ﬁnetune 1In practice, we use the token sequences “ [”, '\n"," '“]” and “->” to represent “ <API> ”, “</API> ” and “!”, respec- tively. This '\n"," 'enables our approach to work without modifying the existing LM’s vocabulary. '\n"," 'For reasons of readability, we still refer to them as “ <API> ”, “</API> ” '\n"," 'and “!” through- out this section.Your task is to add calls to a Question '\n"," 'Answering API to a piece of text. The questions should help you get '\n"," 'information required to complete the text. You can call the API by writing '\n"," '\"[QA(question)]\" where \"question\" is the question you want to ask. Here are '\n"," 'some examples of API calls: Input: Joe Biden was born in Scranton, '\n"," 'Pennsylvania. Output: Joe Biden was born in [QA(\"Where was Joe Biden '\n"," 'born?\")] Scranton, [QA(\"In which state is Scranton?\")] Pennsylvania. Input: '\n"," 'Coca-Cola, or Coke, is a carbonated soft drink manufactured by the Coca-Cola '\n"," 'Company. Output: Coca-Cola, or [QA(\"What other name is Coca-Cola known '\n"," 'by?\")] Coke, is a carbonated soft drink manufactured by [QA(\"Who '\n"," 'manufactures Coca-Cola?\")] the Coca-Cola Company. Input: x Output: Figure 3: '\n"," 'An exemplary prompt P(x)used to generate API calls for the question '\n"," 'answering tool. Mitself on this dataset. Each of these steps is described in '\n"," 'more detail below. Sampling API Calls For each API, we write a '\n"," 'promptP(x)that encourages the LM to anno- tate an example x=x1;:::;x nwith '\n"," 'API calls. An example of such a prompt for a question an- swering tool is '\n"," 'shown in Figure 3; all prompts used are shown in Appendix A.2. Let pM(zn+1j '\n"," 'z1;:::;z n)be the probability that Massigns to tokenzn+1as a continuation '\n"," 'for the sequence z1;:::;z n. We ﬁrst sample up to kcandidate posi- tions for '\n"," 'doing API calls by computing, for each i2f1;:::;ng, the probability '\n"," 'pi=pM(<API>jP(x);x1:i\\x001) thatMassigns to starting an API call at position '\n"," 'i. Given a sampling threshold s, we keep all po- sitionsI=fijpi> sg; if '\n"," 'there are more than k such positions, we only keep the top k. For each '\n"," 'position i2I, we then obtain up to m API callsc1 i;:::;cm iby sampling from '\n"," 'Mgiven the sequence [P(x);x1;:::;x i\\x001;<API> ]as a preﬁx and</API> as an '\n"," 'end-of-sequence token.2 2We discard all examples where Mdoes not generate '\n"," 'the </API> token.Executing API Calls As a next step, we execute all API '\n"," 'calls generated by Mto obtain the corre- sponding results. How this is done '\n"," 'depends entirely on the API itself – for example, it can involve call- ing '\n"," 'another neural network, executing a Python script or using a retrieval '\n"," 'system to perform search over a large corpus. The response for each API call '\n"," 'cineeds to be a single text sequence ri. Filtering API Calls Letibe the '\n"," 'position of the API callciin the sequence x=x1;:::;x n, and let ribe the '\n"," 'response from the API. Further, given a sequence (wiji2N)ofweights , let '\n"," 'Li(z) =\\x00nX j=iwj\\x00i\\x01logpM(xjjz;x1:j\\x001) be the weighted cross '\n"," 'entropy loss for Mover the tokensxi;:::;x nif the model is preﬁxed with z. '\n"," 'We compare two different instantiations of this loss: L+ i=Li(e(ci;ri)) '\n"," 'L\\x00 i= min (Li(\");Li(e(ci;\"))) where\"denotes an empty sequence. The former '\n"," 'is the weighted loss over all tokens xi;:::;x nif the API call and its '\n"," 'result are given to Mas a preﬁx;3 the latter is the minimum of the losses '\n"," 'obtained from (i) doing no API call at all and (ii) doing an API call, but '\n"," 'not providing the response. Intuitively, an API call is helpful to Mif '\n"," 'providing it with both the input andthe output of this call makes it easier '\n"," 'for the model to predict future tokens, compared to not receiving the API '\n"," 'call at all, or receiving only its input. Given a ﬁltering threshold f, we '\n"," 'thus only keep API calls for which L\\x00 i\\x00L+ i\\x15 f holds, i.e., adding '\n"," 'the API call and its result reduces the loss by at least f, compared to not '\n"," 'doing any API call or obtaining no result from it. Model Finetuning After '\n"," 'sampling and ﬁltering calls for all APIs, we ﬁnally merge the remaining API '\n"," 'calls and interleave them with the original inputs. That is, for an input '\n"," 'text x=x1;:::;x n with a corresponding API call and result (ci;ri)at '\n"," 'positioni, we construct the new sequence x\\x03= 3We provide e(ci;ri)as a '\n"," 'preﬁx instead of inserting it at positionibecauseMis not yet ﬁnetuned on any '\n"," 'examples containing API calls, so inserting it in the middle of xwould '\n"," 'interrupt the ﬂow and not align with patterns in the pretraining corpus, '\n"," 'thus hurting perplexity.x1:i\\x001;e(ci;ri);xi:n; we proceed analogously for '\n"," 'texts with multiple API calls. Doing this for all x2 Cresults in the new '\n"," 'dataset C\\x03augmented with API calls. We use this new dataset to ﬁnetune M, '\n"," 'using a standard language modeling objective. Crucially, apart from inserted '\n"," 'API calls the augmented dataset C\\x03contains the exact same texts as C, the '\n"," 'original dataset. As a consequence, ﬁnetuning MonC\\x03 exposes it to the '\n"," 'same content as ﬁnetuning on C. Moreover, as API calls are inserted in '\n"," 'exactly those positions and with exactly those inputs that help Mpredict '\n"," 'future tokens, ﬁnetuning on C\\x03enables the language model to decide when '\n"," 'and how to use which tool, based purely on its own feedback. Inference When '\n"," 'generating text with Mafter ﬁnetuning with our approach, we perform regular '\n"," 'decoding until Mproduces the “!” token, indicat- ing that it next expects '\n"," 'the response for an API call. At this point, we interrupt the decoding '\n"," 'process, call the appropriate API to get a response, and con- tinue the '\n"," 'decoding process after inserting both the response and the </API> token. 3 '\n"," 'Tools We explore a variety of tools to address different shortcomings of '\n"," 'regular LMs. The only constraints we impose on these tools is that (i) both '\n"," 'their inputs and outputs can be represented as text sequences, and (ii) we '\n"," 'can obtain a few demonstrations of their intended use. Concretely, we '\n"," 'explore the fol- lowing ﬁve tools: a question answering system, a Wikipedia '\n"," 'search engine, a calculator, a calendar, and a machine translation system. '\n"," 'Some examples of potential calls and return strings for the APIs associated '\n"," 'with each of these tools are shown in Table 1. We brieﬂy discuss all tools '\n"," 'below; further details can be found in Appendix A. Question Answering Our '\n"," 'ﬁrst tool is a question answering system based on another LM that can an- '\n"," 'swer simple factoid questions. Speciﬁcally, we use Atlas (Izacard et al., '\n"," '2022), a retrieval-augmented LM ﬁnetuned on Natural Questions (Kwiatkowski '\n"," 'et al., 2019). Calculator As a second tool, we use a calculator that can '\n"," 'perform simple numeric calculations; we only support the four basic '\n"," 'arithmetic operations. Results are always rounded to two decimal places. '\n"," 'Wikipedia Search Our third tool is a search en- gine that, given a search '\n"," 'term, returns short textsnippets from Wikipedia. Compared to our ques- tion '\n"," 'answering tool, this search enables a model to get more comprehensive '\n"," 'information on a sub- ject, but requires it to extract the relevant parts by '\n"," 'itself. As our search engine, we use a BM25 re- triever (Robertson et al., '\n"," '1995; Baeza-Yates et al., 1999) that indexes the Wikipedia dump from KILT '\n"," '(Petroni et al., 2021). Machine Translation System Our fourth tool is a '\n"," 'machine translation system based on a LM that can translate a phrase from '\n"," 'any language into En- glish. More concretely, we use the 600M parameter NLLB '\n"," '(Costa-jussà et al., 2022) as our multilingual machine translation model '\n"," 'that works for 200 lan- guages (including low-resource ones). The source '\n"," 'language is automatically detected using the fast- Textclassiﬁer (Joulin et '\n"," 'al., 2016), while the target language is always set to English. Calendar Our '\n"," 'ﬁnal tool is a calendar API that, when queried, returns the current date '\n"," 'without tak- ing any input. This provides temporal context for predictions '\n"," 'that require some awareness of time. 4 Experiments We investigate whether '\n"," 'our approach enables a model to use tools without any further supervision '\n"," 'and to decide for itself when and how to call which of the available tools. '\n"," 'To test this, we select a vari- ety of downstream tasks where we assume at '\n"," 'least one of the considered tools to be useful, and evalu- ate performance '\n"," 'in zero-shot settings (Section 4.2). Beyond that, we also ensure that our '\n"," 'approach does not hurt the model’s core language modeling abili- ties; we '\n"," 'verify this by looking at perplexity on two language modeling datasets '\n"," '(Section 4.3). Finally, we investigate how the ability to learn using tools '\n"," 'is affected by model size (Section 4.4). 4.1 Experimental Setup Dataset '\n"," 'Generation Throughout all of our ex- periments, we use a subset of CCNet '\n"," '(Wenzek et al., 2020) as our language modeling dataset Cand GPT- J (Wang and '\n"," 'Komatsuzaki, 2021) as our language modelM. To reduce the computational cost '\n"," 'of annotatingCwith API calls, we deﬁne heuristics for some APIs to get a '\n"," 'subset of Cfor which API calls are more likely to be helpful than for an av- '\n"," 'erage text. For example, we only consider texts for the calculator tool if '\n"," 'they contain at least three numbers. Details of the heuristics used are '\n"," 'given inAPI Name Example Input Example Output Question Answering Where was '\n"," 'the Knights of Columbus founded?New Haven, Connecticut Wikipedia Search '\n"," 'Fishing Reel Types Spin ﬁshing > Spin ﬁshing is distinguished between ﬂy '\n"," 'ﬁshing and bait cast ﬁshing by the type of rod and reel used. There are two '\n"," 'types of reels used when spin ﬁshing, the open faced reel and the closed '\n"," 'faced reel. Calculator 27 + 4 * 2 35 Calendar \" Today is Monday, January 30, '\n"," '2023. Machine Translation sûreté nucléaire nuclear safety Table 1: Examples '\n"," 'of inputs and outputs for all APIs used. Number of Examples API f= 0:5 f= '\n"," '1:0 f= 2:0 Question Answering 51,987 18,526 5,135 Wikipedia Search 207,241 '\n"," '60,974 13,944 Calculator 3,680 994 138 Calendar 61,811 20,587 3,007 Machine '\n"," 'Translation 3,156 1,034 229 Table 2: Number of examples with API calls in '\n"," 'C\\x03for different values of our ﬁltering threshold f. Appendix A. For '\n"," 'obtaining C\\x03fromC, we perform all steps described in Section 2 and '\n"," 'additionally ﬁlter out all examples for which all API calls were eliminated '\n"," 'in the ﬁltering step.4For the weighting function, we use wt=~wtP s2N~wswith '\n"," '~wt= max(0;1\\x000:2\\x01t) to make sure that API calls happen close to where '\n"," 'the information provided by the API is actually helpful for the model. The '\n"," 'thresholds sand fare chosen individually for each tool to ensure a sufﬁ- '\n"," 'ciently larger number of examples; see Appendix A for details. Table 2 shows '\n"," 'relevant statistics of our ﬁnal dataset augmented with API calls. Model '\n"," 'Finetuning We ﬁnetune MonC\\x03using a batch size of 128 and a learning rate '\n"," 'of 1\\x0110\\x005 with linear warmup for the ﬁrst 10% of training. Details of '\n"," 'our ﬁnetuning procedure are given in Appendix B. Baseline Models Throughout '\n"," 'the remainder of this section, we mainly compare the following mod- els: '\n"," '4While this ﬁltering alters the distribution of training exam- ples, we '\n"," 'assume that the remaining examples are close enough to the original '\n"," 'distribution so that M’s language modeling abilities remain unaffected. This '\n"," 'assumption is empirically validated in Section 4.3.•GPT-J : A regular GPT-J '\n"," 'model without any ﬁnetuning. •GPT-J + CC : GPT-J ﬁnetuned on C, our sub- set '\n"," 'of CCNet without any API calls. •Toolformer : GPT-J ﬁnetuned on C\\x03, our '\n"," 'sub- set of CCNet augmented with API calls. •Toolformer (disabled) : The '\n"," 'same model as Toolformer, but API calls are disabled during decoding.5 For '\n"," 'most tasks, we additionally compare to OPT (66B) (Zhang et al., 2022) and '\n"," 'GPT-36(175B) (Brown et al., 2020), two models that are about 10 and 25 times '\n"," 'larger than our other baseline mod- els, respectively. 4.2 Downstream Tasks '\n"," 'We evaluate all models on a variety of downstream tasks. In all cases, we '\n"," 'consider a prompted zero- shot setup – i.e., models are instructed to solve '\n"," 'each task in natural language, but we do not pro- vide any in-context '\n"," 'examples. This is in contrast to prior work on tool use (e.g., Gao et al., '\n"," '2022; Parisi et al., 2022), where models are provided with dataset-speciﬁc '\n"," 'examples of how a tool can be used to solve a concrete task. We choose the '\n"," 'more challenging zero-shot setup as we are interested in seeing whether '\n"," 'Toolformer works in precisely those cases where a user does not specify in '\n"," 'ad- vance which tools should be used in which way for solving a speciﬁc '\n"," 'problem. We use standard greedy decoding, but with one modiﬁcation for '\n"," 'Toolformer: We let the model start an API call not just when <API> is the '\n"," 'most likely 5This is achieved by manually setting the probability of '\n"," 'the<API> token to 0. 6We use the original davinci variant that is not '\n"," 'ﬁnetuned on any instructions.token, but whenever it is one of the kmost '\n"," 'likely tokens. For k= 1, this corresponds to regular greedy decoding; we '\n"," 'instead use k= 10 to in- crease the disposition of our model to make use of '\n"," 'the APIs that it has access to. At the same time, we only at most one API '\n"," 'call per input to make sure the model does not get stuck in a loop where it '\n"," 'constantly calls APIs without producing any ac- tual output. The effect of '\n"," 'these modiﬁcations is explored in Section 5. 4.2.1 LAMA We evaluate our '\n"," 'models on the SQuAD, Google- RE and T-REx subsets of the LAMA benchmark '\n"," '(Petroni et al., 2019). For each of these subsets, the task is to complete a '\n"," 'short statement with a miss- ing fact (e.g., a date or a place). As LAMA was '\n"," 'originally designed to evaluate masked language models (e.g., Devlin et al., '\n"," '2019), we ﬁlter out ex- amples where the mask token is not the ﬁnal token, '\n"," 'so that the remaining examples can be processed in a left-to-right fashion. '\n"," 'To account for different tokenizations and added complexity from not in- '\n"," 'forming the model that a single word is required, we use a slightly more '\n"," 'lenient evaluation criterion than exact match and simply check whether the '\n"," 'correct word is within the ﬁrst ﬁve words predicted by the model. As LAMA is '\n"," 'based on statements obtained directly from Wikipedia, we prevent Tool- '\n"," 'former from using the Wikipedia Search API to avoid giving it an unfair '\n"," 'advantage. Results for all models can be seen in Table 3. All GPT-J models '\n"," 'without tool use achieve similar performance. Crucially, Toolformer clearly '\n"," 'outper- forms these baseline models, improving upon the best baseline by '\n"," '11.7, 5.2 and 18.6 points, respec- tively. It also clearly outperforms OPT '\n"," '(66B) and GPT-3 (175B), despite both models being much larger. This is '\n"," 'achieved because the model inde- pendently decides to ask the question '\n"," 'answering tool for the required information in almost all cases (98.1%); for '\n"," 'only very few examples, it uses a dif- ferent tool (0.7%) or no tool at all '\n"," '(1.2%). 4.2.2 Math Datasets We test mathematical reasoning abilities on '\n"," 'ASDiv (Miao et al., 2020), SV AMP (Patel et al., 2021) and the MAWPS '\n"," 'benchmark (Koncel-Kedziorski et al., 2016). We again account for the fact '\n"," 'that we test all models in a zero-shot setup by using a more lenient '\n"," 'evaluation criterion: As the required output is always a number, we simply '\n"," 'check for the ﬁrstModel SQuAD Google-RE T-REx GPT-J 17.8 4.9 31.9 GPT-J + CC '\n"," '19.2 5.6 33.2 Toolformer (disabled) 22.1 6.3 34.9 Toolformer 33.8 11.5 53.5 '\n"," 'OPT (66B) 21.6 2.9 30.1 GPT-3 (175B) 26.8 7.0 39.8 Table 3: Results on '\n"," 'subsets of LAMA. Toolformer uses the question answering tool for most '\n"," 'examples, clearly outperforming all baselines of the same size and achiev- '\n"," 'ing results competitive with GPT-3 (175B). Model ASDiv SVAMP MAWPS GPT-J 7.5 '\n"," '5.2 9.9 GPT-J + CC 9.6 5.0 9.3 Toolformer (disabled) 14.8 6.3 15.0 '\n"," 'Toolformer 40.4 29.4 44.0 OPT (66B) 6.0 4.9 7.9 GPT-3 (175B) 14.0 10.0 19.8 '\n"," 'Table 4: Results for various benchmarks requiring mathematical reasoning. '\n"," 'Toolformer makes use of the calculator tool for most examples, clearly '\n"," 'outperform- ing even OPT (66B) and GPT-3 (175B). number predicted by the '\n"," 'model.7 Table 4 shows results for all benchmarks. While GPT-J and GPT-J + CC '\n"," 'perform about the same, Toolformer achieves stronger results even when API '\n"," 'calls are disabled. We surmise that this is be- cause the model is ﬁnetuned '\n"," 'on many examples of API calls and their results, improving its own '\n"," 'mathematical capabilities. Nonetheless, allowing the model to make API calls '\n"," 'more than doubles per- formance for all tasks, and also clearly outperforms '\n"," 'the much larger OPT and GPT-3 models. This is because across all benchmarks, '\n"," 'for 97.9% of all examples the model decides to ask the calculator tool for '\n"," 'help. 4.2.3 Question Answering We look at Web Questions (Berant et al., '\n"," '2013), Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et '\n"," 'al., 2017), the three question an- swering datasets considered by Brown et '\n"," 'al. (2020). For evaluation, we check whether the ﬁrst 20 words predicted by '\n"," 'a model contain the correct answer instead of requiring an exact match. For '\n"," 'Tool- former, we disable the question answering tool as 7An exception to '\n"," 'this is if the model’s prediction contains an equation (e.g., “The correct '\n"," 'answer is 5+3=8”), in which case we consider the ﬁrst number after the “=” '\n"," 'sign to be its prediction.Model WebQS NQ TriviaQA GPT-J 18.5 12.8 43.9 GPT-J '\n"," '+ CC 18.4 12.2 45.6 Toolformer (disabled) 18.9 12.6 46.7 Toolformer 26.3 '\n"," '17.7 48.8 OPT (66B) 18.6 11.4 45.7 GPT-3 (175B) 29.0 22.6 65.9 Table 5: '\n"," 'Results for various question answering dataset. Using the Wikipedia search '\n"," 'tool for most examples, Toolformer clearly outperforms baselines of the same '\n"," 'size, but falls short of GPT-3 (175B). this would make solving the tasks '\n"," 'trivial, especially given that the underlying QA system was ﬁnetuned on '\n"," 'Natural Questions. Results are shown in Table 5. Once again, Toolformer '\n"," 'clearly outperforms all other models based on GPT-J, this time mostly '\n"," 'relying on the Wikipedia search API (99.3%) to ﬁnd relevant in- formation. '\n"," 'However, Toolformer still lags behind the much larger GPT-3 (175B) model. '\n"," 'This is likely due to both the simplicity of our search engine (in many '\n"," 'cases, it returns results that are clearly not a good match for a given '\n"," 'query) and the inability of Toolformer to interact with it, e.g., by refor- '\n"," 'mulating its query if results are not helpful or by browsing through '\n"," 'multiple of the top results. We believe that adding this functionality is an '\n"," 'exciting direction for future work. 4.2.4 Multilingual Question Answering We '\n"," 'evaluate Toolformer and all baseline models on MLQA (Lewis et al., 2019), a '\n"," 'multilingual question-answering benchmark. A context para- graph for each '\n"," 'question is provided in English, while the question can be in Arabic, '\n"," 'German, Span- ish, Hindi, Vietnamese, or Simpliﬁed Chinese. In order to '\n"," 'solve the task, the model needs to be able to understand both the paragraph '\n"," 'and the question, so it may beneﬁt from translating the question into '\n"," 'English. Our evaluation metric is the percentage of times the model’s '\n"," 'generation, capped at 10 words, contains the correct answer. Results are '\n"," 'shown in Table 6. Using API calls consistently improves Toolformer’s '\n"," 'performance for all languages, suggesting that it has learned to make use of '\n"," 'the machine translation tool. Depend- ing on the language, this tool is used '\n"," 'for 63.8% to 94.9% of all examples; the only exception to this is Hindi, for '\n"," 'which the machine translation tool is used in only 7.3% of cases. However, '\n"," 'Tool-Model Es De Hi Vi Zh Ar GPT-J 15.2 16.5 1.3 8.2 18.2 8.2 GPT-J + CC '\n"," '15.7 14.9 0.5 8.3 13.7 4.6 Toolformer (disabled) 19.8 11.9 1.2 10.1 15.0 3.1 '\n"," 'Toolformer 20.6 13.5 1.410.6 16.8 3.7 OPT (66B) 0.3 0.1 1.1 0.2 0.7 0.1 '\n"," 'GPT-3 (175B) 3.4 1.1 0.1 1.7 17.7 0.1 GPT-J (All En) 24.3 27.0 23.9 23.3 '\n"," '23.1 23.6 GPT-3 (All En) 24.7 27.2 26.1 24.9 23.6 24.0 Table 6: Results on '\n"," 'MLQA for Spanish (Es), German (De), Hindi (Hi), Vietnamese (Vi), Chinese '\n"," '(Zh) and Arabic (Ar). While using the machine translation tool to translate '\n"," 'questions is helpful across all languages, further pretraining on CCNet '\n"," 'deteriorates performance; consequently, Toolformer does not consistently '\n"," 'outper- form GPT-J. The ﬁnal two rows correspond to models that are given '\n"," 'contexts and questions in English. former does not consistently outperform '\n"," 'vanilla GPT-J. This is mainly because for some languages, ﬁnetuning on CCNet '\n"," 'deteriorates performance; this might be due to a distribution shift compared '\n"," 'to GPT-J’s original pretraining data. OPT and GPT-3 perform surprisingly '\n"," 'weak across all languages, mostly because they fail to provide an answer in '\n"," 'English despite being in- structed to do so. A potential reason for GPT-J '\n"," 'not suffering from this problem is that it was trained on more multilingual '\n"," 'data than both OPT and GPT-3, including the EuroParl corpus (Koehn, 2005; '\n"," 'Gao et al., 2020). As an upper bound, we also evaluate GPT-J and GPT-3 on a '\n"," 'variant of MLQA where both the context and the question are provided in '\n"," 'English. In this setup, GPT-3 performs better than all other models, '\n"," 'supporting our hypothesis that its subpar performance on MLQA is due to the '\n"," 'multilingual aspect of the task. 4.2.5 Temporal Datasets To investigate the '\n"," 'calendar API’s utility, we eval- uate all models on TEMPLAMA (Dhingra et '\n"," 'al., 2022) and a new dataset that we call DATESET . TEMPLAMA is a dataset '\n"," 'built from Wikidata that contains cloze queries about facts that change with '\n"," 'time (e.g., “Cristiano Ronaldo plays for ___”) as well as the correct answer '\n"," 'for the years be- tween 2010 and 2020. DATESET , described in Appendix D, is '\n"," 'also generated through a series of templates, but populated using a '\n"," 'combination of random dates/durations (e.g., “What day of the week was it 30 '\n"," 'days ago?”). Critically, knowing the current date is required to answer '\n"," 'these questions.Model T EMPLAMA D ATESET GPT-J 13.7 3.9 GPT-J + CC 12.9 2.9 '\n"," 'Toolformer (disabled) 12.7 5.9 Toolformer 16.3 27.3 OPT (66B) 14.5 1.3 GPT-3 '\n"," '(175B) 15.5 0.8 Table 7: Results for the temporal datasets. Toolformer '\n"," 'outperforms all baselines, but does not make use of the calendar tool for T '\n"," 'EMPLAMA. For both tasks, we use the same evaluation as for the original LAMA '\n"," 'dataset. Results shown in Table 7 illustrate that Tool- former outperforms '\n"," 'all baselines for both TEM- PLAMA andDATESET . However, closer inspec- tion '\n"," 'shows that improvements on TEMPLAMA can not be attributed to the calendar '\n"," 'tool, which is only used for 0.2% of all examples, but mostly to the '\n"," 'Wikipedia search and question answering tools, which Toolformer calls the '\n"," 'most. This makes sense given that named entities in TEMPLAMA are often so '\n"," 'speciﬁc and rare that even knowing the exact date alone would be of little '\n"," 'help. The best course of action for this dataset – ﬁrst querying the calen- '\n"," 'dar API to get the current date, and then querying the question answering '\n"," 'system with this date – is not only prohibited by our restriction of using '\n"," 'at most one API call per example, but also hard to learn for Toolformer '\n"," 'given that all API calls in its training data are sampled independently. '\n"," 'ForDATESET , on the other hand, the consider- able improvement of Toolformer '\n"," 'compared to other models can be fully accredited to the calendar tool, which '\n"," 'it makes use of for 54.8% of all examples. 4.3 Language Modeling In addition '\n"," 'to verifying improved performance on various downstream tasks, we also want '\n"," 'to ensure that language modeling performance of Toolformer does not degrade '\n"," 'through our ﬁnetuning with API calls. To this end, we evaluate our models on '\n"," 'two language modeling datasets: WikiText (Mer- ity et al., 2017) and a '\n"," 'subset of 10,000 randomly selected documents from CCNet (Wenzek et al., '\n"," '2020) that were not used during training. Perplex- ities of various models '\n"," 'are shown in Table 8. As one would expect, ﬁnetuning on CCNet leads to '\n"," 'slightly improved performance on a different CC- Net subset, but it slightly '\n"," 'deteriorates performance on WikiText, presumably because the original '\n"," 'pre-Model WikiText CCNet GPT-J 9.9 10.6 GPT-J + CC 10.3 10.5 Toolformer '\n"," '(disabled) 10.3 10.5 Table 8: Perplexities of different models on WikiText '\n"," 'and our validation subset of CCNet. Adding API calls comes without a cost in '\n"," 'terms of perplexity for lan- guage modeling without any API calls. training '\n"," 'data for GPT-J is more similar to Wiki- Text than our randomly selected '\n"," 'subset of CCNet. Most importantly, however, training on C\\x03(our dataset '\n"," 'annotated with API calls) does not lead to an increase in perplexity '\n"," 'compared to training on Cwhen API calls are disabled at inference time.8 4.4 '\n"," 'Scaling Laws We investigate how the ability to ask external tools for help '\n"," 'affects performance as we vary the size of our LM. To this end, we apply our '\n"," 'approach not just to GPT-J, but also to four smaller mod- els from the GPT-2 '\n"," 'family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, '\n"," 're- spectively. We do so using only a subset of three tools: the question '\n"," 'answering system, the calcula- tor, and the Wikipedia search engine. Apart '\n"," 'from this, we follow the experimental setup described in Section 4.1. Figure '\n"," '4 shows that the ability to leverage the provided tools only emerges at '\n"," 'around 775M pa- rameters: smaller models achieve similar perfor- mance both '\n"," 'with and without tools. An exception to this is the Wikipedia search engine '\n"," 'used mostly for QA benchmarks; we hypothesize that this is because the API '\n"," 'is comparably easy to use. While models become better at solving tasks '\n"," 'without API calls as they grow in size, their ability to make good use of '\n"," 'the provided API improves at the same time. As a consequence, there remains '\n"," 'a large gap be- tween predictions with and without API calls even for our '\n"," 'biggest model. 5 Analysis Decoding Strategy We investigate the effect of our '\n"," 'modiﬁed decoding strategy introduced in Sec- tion 4.2, where instead of '\n"," 'always generating the 8We do not evaluate the perplexity of Toolformer with '\n"," 'API calls enabled as computing the probability pM(xtj x1;:::;x t\\x001)of '\n"," 'tokenxtgivenx1;:::;x t\\x001would require marginalizing over all potential '\n"," 'API calls that the model could make at position t, which is '\n"," 'intractable.051015202530 0200040006000Model Parameters (M)LAMA Toolformer '\n"," 'Toolformer (disabled) GPT30510152025303540 0200040006000Model Parameters '\n"," '(M)QA Benchmarks 051015202530 0200040006000Model Parameters (M)Math '\n"," 'BenchmarksFigure 4: Average performance on LAMA, our math benchmarks and our '\n"," 'QA benchmarks for GPT-2 models of different sizes and GPT-J ﬁnetuned with '\n"," 'our approach, both with and without API calls. While API calls are not '\n"," 'helpful to the smallest models, larger models learn how to make good use of '\n"," 'them. Even for bigger models, the gap between model predictions with and '\n"," 'without API calls remains high. most likely token, we generate the <API> '\n"," 'token if it is one of the kmost likely tokens. Table 9 shows performance on '\n"," 'the T-REx subset of LAMA and on WebQS for different values of k. As ex- '\n"," 'pected, increasing kleads to the model doing API calls for more examples – '\n"," 'from 40.3% and 8.5% withk= 1(i.e., regular greedy decoding) to 98.1% and '\n"," '100% for k= 10 . While for T-REx, there is already a clear improvement in '\n"," 'performance with greedy decoding, on WebQS our model only starts to make a '\n"," 'substantial number of API calls as we slightly increase k. Interestingly, '\n"," 'for k= 1 the model is calibrated to some extent: It decides to call APIs for '\n"," 'examples that it would perform partic- ularly badly on without making API '\n"," 'calls. This can be seen from the fact that performance on examples where it '\n"," 'decides notto make an API call (44.3 and 19.9) is higher than average '\n"," 'performance if no API calls are made at all (34.9 and 18.9). However, this '\n"," 'calibration is lost for higher values of k. Data Quality We qualitatively '\n"," 'analyze some API calls generated with our approach for different APIs. Table '\n"," '10 shows some examples of texts from CCNet augmented with API calls, as well '\n"," 'as the corresponding score L\\x00 i\\x00L+ ithat is used as a ﬁl- tering '\n"," 'criterion, and whether the API calls made by the model are intuitively '\n"," 'useful in the given context. As can be seen, high values of L\\x00 i\\x00L+ '\n"," 'itypically correspond to useful API calls, whereas low values correspond to '\n"," 'API calls that do not provide any in- formation that is useful for '\n"," 'predicting future tokens. There are some exceptions, e.g., an API call '\n"," 'forT-REx WebQS k All AC NC % All AC NC % 0 34.9 – 34.9 0.0 18.9 – 18.9 0.0 1 '\n"," '47.8 53.0 44.3 40.3 19.3 17.1 19.9 8.5 3 52.9 58.0 29.0 82.8 26.3 26.5 6.6 '\n"," '99.3 10 53.5 54.0 22.5 98.1 26.3 26.4 – 100.0 Table 9: Toolformer results on '\n"," 'the T-REx subset of LAMA and on WebQS for different values of kused during '\n"," 'decoding. Numbers shown are overall perfor- mance (All), performance on the '\n"," 'subset where the model decides to make an API call (AC) and all re- maining '\n"," 'examples (NC), as well as the percentage of examples for which the model '\n"," 'decides to call an API (%). “Fast train success” in the fourth example that '\n"," 'does not give any relevant information but still reduces perplexity. '\n"," 'However, some amount of noise in the API calls that are not ﬁltered can '\n"," 'actually be useful as it forces the model ﬁnetuned on C\\x03to not always '\n"," 'blindly follow the results of each call it makes. 6 Related Work Language '\n"," 'Model Pretraining There are various approaches that augment language models '\n"," 'with some form of additional textual information during pretraining, '\n"," 'including various forms of metadata (Keskar et al., 2019), HTML tags '\n"," '(Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or '\n"," 'related texts obtained from an informa- tion retrieval system (Guu et al., '\n"," '2020; Borgeaud et al., 2021; Izacard et al., 2022). For all of theseExample '\n"," 'L\\x00 i\\x00L+ iUseful The Flodden Window (a war memorial dedicated to The '\n"," 'Middleton Archers), in the Grade I-listed Church of St Leonard in Middleton '\n"," 'is said to be the oldest war memorial in the United King- dom. <API> '\n"," 'WikiSearch(War memorial Flodden) !Battle of Flodden > Commemoration > The '\n"," 'stained-glass Flodden Window in Middleton Parish Church [. . . ] was '\n"," 'constructed by Sir Richard Assheton in memory of the Battle of Flodden and '\n"," 'the archers from Middleton who fought in it. </API> Sir Richard Assheton of '\n"," 'Middleton (who built St Leonard) was granted knighthood [. . . ]5.49 3 Note: '\n"," 'The WL will be open on Friday, <API> Calendar()!Today is Thursday, March 9, '\n"," '2017. </API> March 10, and Sunday, March 19 for regular hours.2.11 3 The '\n"," 'Nile has an approximate length of <API> QA(What is the approximate length of '\n"," 'the Nile?) !6,853 km </API> 6,853 kilometers, the White Nile being its main '\n"," 'source.2.08 3 If Venus had an atmosphere similar to Earth’s then you would '\n"," 'expect Venus’ mean temperature to be 499 K (1.74 x 287) rather than 735 K '\n"," 'which is <API> Calculator(735 / 499) !1.47</API> 1.47 (735 / 499) times '\n"," 'hotter than it should be.1.59 3 You are here: Home / Featured / Catch this '\n"," 'fast train to success! <API> WikiSearch(Fast train success)!Fast Train > It '\n"," 'also peaked at #23 on the Canadian CHUM singles chart, on June 26, 1971. The '\n"," 'success of this single established Myles Goodwyn as the band’s main '\n"," 'songwriter, and made it possible for April Wine to record a second album. '\n"," '</API> Don’t wait weeks and [:::]0.92 7 Os Melhores Escolas em Jersey 2020 '\n"," '<API> MT(Os Melhores Escolas em Jersey) !The Best Schools in Jersey </API> '\n"," 'On this page you can search for Universities, Colleges and Business schools '\n"," 'in Jersey0.70 3 Enjoy these pictures from the <API> Calendar()!Today is '\n"," 'Friday, April 19, 2013. </API> Easter Egg Hunt.0.33 3 85 patients (23%) were '\n"," 'hospitalised alive and admitted to a hospital ward. Of them, <API> Calcula- '\n"," 'tor(85 / 23)!3.70</API> 65% had a cardiac aetiology [:::]\\x000.02 7 But hey, '\n"," 'after the <API> Calendar()!Today is Saturday, June 25, 2011. </API> '\n"," 'Disneyland ﬁasco with the ﬁre drill, I think it’s safe to say Chewey won’t '\n"," 'let anyone die in a ﬁre.\\x000.41 7 The last time I was with <API> QA(Who was '\n"," 'last time I was with?) !The Last Time </API> him I asked what he likes about '\n"," 'me and he said he would tell me one day.\\x001.23 7 Table 10: Examples of API '\n"," 'calls for different tools, sorted by the value of L\\x00 i\\x00L+ ithat is '\n"," 'used as a ﬁltering criterion. High values typically correspond to API calls '\n"," 'that are intuitively useful for predicting future tokens. approaches, '\n"," 'additional information is always pro- vided, regardless of whether it is '\n"," 'helpful or not. In contrast, Toolformer learns for itself to explicitly asks '\n"," 'for the right information. Tool Use Several approaches aim to equip LMs with '\n"," 'the ability to use external tools such as search engines (Komeili et al., '\n"," '2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; '\n"," 'Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et '\n"," 'al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., '\n"," '2022) and Python interpreters (Gao et al., 2022). The way these models learn '\n"," 'to use tools can roughly be divided into two approaches: Either they rely on '\n"," 'large amounts of human supervision (Komeili et al., 2022; Nakano et al., '\n"," '2021; Thoppilan et al., 2022) or they work by prompting the language model '\n"," 'in a few-shot setup tailored towards a speciﬁc task where it is known a '\n"," 'priori which tools needs to beused (Gao et al., 2022; Lazaridou et al., '\n"," '2022; Yao et al., 2022). In contrast, the self-supervised nature of '\n"," 'Toolformer enables it to learn how and when to use tools without requiring a '\n"," 'speciﬁc prompt that shows task-speciﬁc examples of how a tool could be used. '\n"," 'Perhaps most closely related to our work is TALM (Parisi et al., 2022), an '\n"," 'approach that uses a similar self-supervised objective for teach- ing a '\n"," 'model to use a calculator and a search engine, but explores this only in '\n"," 'settings where a model is ﬁnetuned for downstream tasks. Bootstrapping The '\n"," 'idea of using self-training and bootstrapping techniques to improve models '\n"," 'has been investigated in various contexts, rang- ing from word sense '\n"," 'disambiguation (Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein '\n"," 'and Gravano, 2000), parsing (McClosky et al., 2006; Reichart and Rappoport, '\n"," '2007), sequence generation (He et al., 2020), few-shot text classi-')\n"]}]},{"cell_type":"code","source":["paper_part = 'Approach'\n","# 프롬프트\n","system_template = \"\"\"\n","    You are an AI research assistant tasked with creating structured reports.\n","    You will be given the content of an AI research paper. Your task is to summarize the specified section of the paper in a clear and concise manner.\n","    \"\"\"\n","\n","human_template = \"\"\"\n","    Content: {content}\n","\n","    Summarize the {paper_part} section in bullet points. Make sure to capture the key points and main ideas.\n","    \"\"\"\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", system_template),\n","    (\"human\", human_template)\n","])\n","model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n","\n","with get_openai_callback() as cb:\n","    chain =  prompt | model | parser\n","    report = chain.invoke({\n","    \"content\": content,\n","    \"paper_part\": paper_part\n","    })\n","print(cb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JV_Xvq1eF_VR","executionInfo":{"status":"ok","timestamp":1716978542969,"user_tz":-540,"elapsed":5228,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"313f8fa2-b074-458d-8e4e-57e794b559c7"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens Used: 11018\n","\tPrompt Tokens: 10805\n","\tCompletion Tokens: 213\n","Successful Requests: 1\n","Total Cost (USD): $0.0166335\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MFxNpYxC5W3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pprint(report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nutfggVk4uLv","executionInfo":{"status":"ok","timestamp":1716978546735,"user_tz":-540,"elapsed":353,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"035480af-7c02-4e87-bc43-b59507c7b217"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["('- Toolformer는 Language Models에 외부 도구 사용 능력을 부여하는 방법\\n'\n"," '- Self-supervised 방식으로 작동하여 모델이 언제, 어떻게 어떤 도구를 사용할지 학습\\n'\n"," '- API 호출을 허용하여 모델이 추가 정보를 스스로 요청할 수 있음\\n'\n"," '- 큰 양의 인간 지도학습 없이 외부 도구 사용 능력을 갖춤\\n'\n"," '- API 호출을 통해 미래 토큰 예측에 유용한 정보를 제공받음\\n'\n"," '- 다양한 도구(질문 응답 시스템, 위키피디아 검색 엔진, 계산기 등)을 사용하여 성능 향상 달성')\n"]}]},{"cell_type":"code","source":["equation = '''L\n","+\n","i = Li(e(ci\n",", ri))\n","L\n","−\n","i = min (Li(ε), Li(e(ci\n",", ε)))\n","'''\n","\n","# 프롬프트\n","system_template = \"\"\"You are an AI research assistant tasked with creating structured reports.\n","You will be given the content of an AI research paper, including specific equations used within the paper.\n","Your task is to explain the specified equation in a clear and concise manner.\n","Ensure that your explanation covers the purpose of the equation, its components, and how it fits into the broader context of the paper.\n","Use formal language suitable for a research audience.\"\"\"\n","\n","human_template =  \"\"\"\n","    Content: {content}\n","    Equation {equation}\n","\n","    Provide a detailed explanation of Equation {equation} from the content provided.\n","    Your explanation should include:\n","    - The purpose of the equation within the context of the research.\n","    - A breakdown of each component of the equation.\n","    - How the equation contributes to the overall findings or methodology of the paper.\n","    First, write a equation again in Latex form\n","    \"\"\"\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", system_template),\n","    (\"human\", human_template)\n","])\n","model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n","\n","with get_openai_callback() as cb:\n","    chain =  prompt | model | parser\n","    report = chain.invoke({\n","    \"content\": content,\n","    \"equation\": equation\n","    })\n","print(cb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_n-8DrRSYNt","executionInfo":{"status":"ok","timestamp":1716978419466,"user_tz":-540,"elapsed":10618,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"c706394c-08a7-4900-bdf2-5f5880faea47"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens Used: 11318\n","\tPrompt Tokens: 10950\n","\tCompletion Tokens: 368\n","Successful Requests: 1\n","Total Cost (USD): $0.017161\n"]}]},{"cell_type":"code","source":["pprint(report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69nWbKNkF_Xn","executionInfo":{"status":"ok","timestamp":1716978419467,"user_tz":-540,"elapsed":33,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"e058b58b-5bcf-4adc-c193-480ebb7718ef"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["('The equation can be written in Latex form as:\\n'\n"," '\\n'\n"," '\\\\[\\n'\n"," 'L_{i}^{+} = L_{i}(e(ci, ri)) \\n'\n"," '\\\\]\\n'\n"," '\\\\[\\n'\n"," 'L_{i}^{-} = \\\\min (L_{i}(\\\\epsilon), L_{i}(e(ci, \\\\epsilon)))\\n'\n"," '\\\\]\\n'\n"," '\\n'\n"," 'This equation plays a crucial role in the context of the research as it is '\n"," 'used to determine the usefulness of an API call made by the model. \\n'\n"," '\\n'\n"," \"Here's a breakdown of each component:\\n\"\n"," '- \\\\(L_{i}^{+}\\\\): This represents the weighted cross entropy loss for the '\n"," 'model over the tokens if the API call and its result are provided as a '\n"," 'preﬁx.\\n'\n"," '- \\\\(L_{i}^{-}\\\\): This represents the minimum of two losses: one where no '\n"," 'API call is made at all (\\\\(L_{i}(\\\\epsilon)\\\\)), and the other where an API '\n"," 'call is made but the result is not provided (\\\\(L_{i}(e(ci, '\n"," '\\\\epsilon))\\\\)).\\n'\n"," '\\n'\n"," 'The equation is essential in determining whether an API call is beneficial '\n"," 'for the model in predicting future tokens. If the difference between the '\n"," 'loss with the API call and its result (\\\\(L_{i}^{+}\\\\)) and the minimum of '\n"," 'the losses without the API call and with the API call but no result '\n"," '(\\\\(L_{i}^{-}\\\\)) is greater than a certain threshold, it indicates that the '\n"," \"API call is useful and should be incorporated into the model's predictions.\\n\"\n"," '\\n'\n"," 'Overall, this equation contributes to the self-supervised learning process '\n"," 'of the model by enabling it to assess the utility of API calls based on '\n"," \"their impact on improving the model's token predictions. This evaluation \"\n"," 'mechanism helps the model learn to make effective use of external tools in a '\n"," 'way that enhances its language modeling abilities and performance on '\n"," 'downstream tasks.')\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zxQGBsPi480W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdzHA86ElSqf","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["f8983ad513194fdcbd9cf6d606a37d04","941d528db0fa45febe8a84056ed1cce7","6ffd0de542024e06b48de8dfd8b17133","46ed5e36f5cd49838a7bda5b09bda633","2125304c2a624a9b9564e4826ad41065","550ae926768746d0ae3ddfb878e73271","94310722603f4988aee80475e8980ceb","dd16de4b101e4e799ffe7fa9cfb94bfe","3ce607e886404ddab7a6571b5395eee3","890db37c6a1b4b698320900eaa267995","f04b23a3796c469790f09ccbf1f4b1c3","3476724478054d5580404866d6f76f4d","eee2d9053bfc415798460a20bc04900c","8f528705a3cd445998b3a7053c9aa168","e0c05fa9d3e940aca9d53b0126ebf118","16bb04afd218493bb646a9a9e75a3a97","c42151a876f64653a679015cfa6101de","99120cc07a7f451bac8e9bdd5360ff1f","58cd571609fc4e4ab49f26eba170f6bd","f856133ed87d4af1bac229b6c02566ef","199009d89041476f96cfb20341553f5e","109f8e7ca2814221a7ff481c2e34626d","3b75eb6fc754495994929d7f36e1945f","2ca07f81f8864cc58e6545b4e25f1430","df06f66a8769406e930b645442652334","cb35b5db525d41aca552f0e7427a32af","b0f2add67efd4ba9bb114729587c7116","37a70e851a794f6e801d2e61c54c3ef4","36c5f74dab144cb6b49ff105c84fdaec","b849f4da354c44c5a4109be5855d3a45","b80a2776e62a448eafe6ea31ccef3981","e22563212265462a9a05a6675c0ee9b9"]},"executionInfo":{"status":"ok","timestamp":1716967909118,"user_tz":-540,"elapsed":347,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"1fef6c97-cfce-4972-fd20-59936337ccb9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8983ad513194fdcbd9cf6d606a37d04"}},"metadata":{}}],"source":["# for blocked repo, you need huggingface.login()\n","import huggingface_hub\n","huggingface_hub.login()"]},{"cell_type":"code","source":["os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"_npgxpU4y420","executionInfo":{"status":"ok","timestamp":1716967909466,"user_tz":-540,"elapsed":4,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"e08603c2-7781-47e8-8378-fc672c88896c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'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'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["from langchain.llms import HuggingFaceEndpoint"],"metadata":{"id":"UueVgBxky6eU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","llm = HuggingFaceEndpoint(\n","        repo_id=repo_id,\n","        task=\"text-generation\",\n","        max_new_tokens= 512,\n","        do_sample= False,\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dsTBVGpuzO73","executionInfo":{"status":"ok","timestamp":1716372982159,"user_tz":-540,"elapsed":9,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"134729af-6d9d-48c9-9542-35d82de32947"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["from langchain.agents import create_json_chat_agent, AgentExecutor\n","from langchain.memory import ConversationBufferMemory\n","\n","# agent = create_json_chat_agent(\n","#     tools = tools,\n","#     llm = llm,\n","#     prompt = prompt,\n","#     stop_sequence = [\"STOP\"],\n","#     template_tool_response = \"{observation}\"\n","# )\n","\n","# custom_agent = AgentExecutor(agent=agent,\n","#                              tools=tools,\n","#                              verbose=True,\n","#                              handle_parsing_errors=True,\n","#                              return_intermediate_steps=True,\n","#                              max_iterations=5\n","#                              )"],"metadata":{"id":"_WKhKStrQxLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.schema import (\n","    HumanMessage,\n","    SystemMessage,\n",")\n","from langchain_community.chat_models.huggingface import ChatHuggingFace\n","\n","messages = [\n","    SystemMessage(content=\"You're a helpful assistant\"),\n","    HumanMessage(\n","        content=\"What happens when an unstoppable force meets an immovable object?\"\n","    ),\n","]\n","\n","chat_model = ChatHuggingFace(llm=llm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99kBO07WzO3v","executionInfo":{"status":"ok","timestamp":1716373060701,"user_tz":-540,"elapsed":990,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"ebab0747-f5c9-4af4-9be1-9710d4293410"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["system_template = \"\"\"\n","    You are an AI research assistant tasked with creating structured reports.\n","    Your sole purpose is to write well written, critically acclaimed.\n","    objective and structured reports on given text.\n","    \"\"\"\n","human_template = f\"\"\"\n","    Content: {content}\n","    Query: Answering below questions based on the content.\n","            1. Explain why the {query} was mententioned in {cit_paper}.\n","            2. Briefly explain the abstract of the paper {cit_paper}\n","    \"\"\""],"metadata":{"id":"_UsYwR9Yzwhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    SystemMessage(content=system_template),\n","    HumanMessage(\n","        content=human_template),\n","]\n","res = chat_model.invoke(messages)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dAhmJhYuzOt8","executionInfo":{"status":"ok","timestamp":1716373222277,"user_tz":-540,"elapsed":15694,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"11d0b7ea-3f28-4352-e460-e44a15df00b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is the answer to the query:\n","\n","**1. Why was Toolformer: Language Models Can Teach Themselves to Use Tools mentioned in the given papers?**\n","\n","The Toolformer: Language Models Can Teach Themselves to Use Tools was not mentioned in the given papers. However, the papers do discuss the topic of large language models (LLMs) using tools or APIs, which is a related concept.\n","\n","* In \"Gorilla: Large Language Model Connected with Massive APIs\", the authors release a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls, demonstrating the potential for LLMs to use tools more accurately.\n","* In \"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs\", the authors introduce a general tool-use framework encompassing data construction, model training, and evaluation, which enables LLMs to master real-world APIs.\n","* In \"Qwen Technical Report\", the authors discuss the development of coding-specialized models, Code-Qwen and Code-Qwen-Chat, which are built upon base language models and demonstrate improved performance in comparison with open-source models.\n","* In \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\", the authors propose an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks.\n","\n","These papers focus on the capabilities of LLMs in using tools or APIs, rather than the specific concept of \"Toolformer: Language Models Can Teach Themselves to Use Tools\".\n","\n","**2. Briefly explain the abstract of each paper:**\n","\n","* \"Gorilla: Large Language Model Connected with Massive APIs\" introduces a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls, enabling LLMs to use tools more accurately and adapt to test-time document changes.\n","* \"Mind2Web: Towards a Generalist Agent for the Web\" introduces a dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website, and explores the use of large language models (LLMs) for building generalist web agents.\n","* \"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs\" introduces a general tool-use framework encompassing data construction, model training, and evaluation, which enables LLM\n"]}]},{"cell_type":"code","source":["pprint(report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NTsIocss0Xno","executionInfo":{"status":"ok","timestamp":1716373265236,"user_tz":-540,"elapsed":10,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"844e7f70-a792-452c-c882-3f4ef4e83e33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('**1. Explanation of Toolformer Reference in the Given Content:**\\n'\n"," '\\n'\n"," 'The Toolformer paper is mentioned in the context of exploring the ability of '\n"," 'language models to learn how to effectively utilize tools, particularly '\n"," \"APIs, to fulfill human instructions. The papers ['Gorilla: Large Language \"\n"," \"Model Connected with Massive APIs', 'Mind2Web: Towards a Generalist Agent \"\n"," \"for the Web', 'ToolLLM: Facilitating Large Language Models to Master 16000+ \"\n"," \"Real-world APIs', 'Qwen Technical Report', 'HuggingGPT: Solving AI Tasks \"\n"," \"with ChatGPT and its Friends in Hugging Face'] all focus on advancing large \"\n"," 'language models (LLMs) in their tool-use capabilities. They aim to address '\n"," 'the challenge of enabling LLMs to effectively interact with tools and APIs '\n"," 'to enhance their performance in various tasks.\\n'\n"," '\\n'\n"," 'The Toolformer paper likely served as a foundational work or a reference '\n"," 'point in the exploration of how language models can self-teach themselves to '\n"," 'effectively utilize tools, which aligns with the objectives and themes '\n"," 'discussed in the mentioned papers. These papers build upon the concepts and '\n"," 'methodologies introduced in the Toolformer paper to further enhance the '\n"," 'tool-use capabilities of LLMs, ultimately aiming to improve their '\n"," 'adaptability, reliability, and applicability in real-world scenarios.\\n'\n"," '\\n'\n"," '**2. Brief Explanation of Abstracts:**\\n'\n"," '\\n'\n"," \"- **'Gorilla: Large Language Model Connected with Massive APIs':** The \"\n"," 'abstract introduces Gorilla, a finetuned large language model that excels in '\n"," 'writing API calls by surpassing the performance of GPT-4. Gorilla, when '\n"," 'combined with a document retriever, shows adaptability to document changes '\n"," 'and mitigates the issue of hallucination, leading to more accurate tool '\n"," 'usage by LLMs.\\n'\n"," '\\n'\n"," \"- **'Mind2Web: Towards a Generalist Agent for the Web':** This paper \"\n"," 'presents Mind2Web, a dataset for developing generalist agents for the web '\n"," 'that can follow language instructions to perform complex tasks on various '\n"," 'real-world websites. The dataset facilitates the exploration of using large '\n"," 'language models for building generalist web agents, showcasing promising '\n"," 'results in task completion across diverse websites.\\n'\n"," '\\n'\n"," \"- **'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world \"\n"," \"APIs':** The abstract highlights the introduction of ToolLLM, a framework \"\n"," 'aimed at enhancing the tool-use capabilities of LLMs by constructing a '\n"," 'dataset for instruction tuning in tool use scenarios. The paper demonstrates '\n"," 'the successful fine-tuning of LLaMA to execute complex instructions and '\n"," 'generalize to unseen APIs, showcasing strong zero-shot generalization '\n"," 'abilities.\\n'\n"," '\\n'\n"," \"- **'Qwen Technical Report':** This work introduces Qwen, a comprehensive \"\n"," 'language model series encompassing base language models and chat models '\n"," 'finetuned with human alignment techniques. Qwen models demonstrate superior '\n"," 'performance across various tasks, including coding and mathematics, '\n"," 'showcasing competitive capabilities compared to larger models.\\n'\n"," '\\n'\n"," \"- **'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging \"\n"," \"Face':** The abstract presents HuggingGPT, an LLM-powered agent that \"\n"," 'leverages ChatGPT to connect various AI models in Hugging Face for solving '\n"," 'complex AI tasks autonomously. By utilizing language as a generic interface, '\n"," 'HuggingGPT demonstrates impressive results across different modalities and '\n"," 'domains, contributing to the advancement of artificial general intelligence.')\n"]}]},{"cell_type":"code","source":["pprint(res.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4Ae0zz30RX8","executionInfo":{"status":"ok","timestamp":1716373236287,"user_tz":-540,"elapsed":355,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"dcdc093c-63fa-48a2-d2ee-7188e3d61c22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('Here is the answer to the query:\\n'\n"," '\\n'\n"," '**1. Why was Toolformer: Language Models Can Teach Themselves to Use Tools '\n"," 'mentioned in the given papers?**\\n'\n"," '\\n'\n"," 'The Toolformer: Language Models Can Teach Themselves to Use Tools was not '\n"," 'mentioned in the given papers. However, the papers do discuss the topic of '\n"," 'large language models (LLMs) using tools or APIs, which is a related '\n"," 'concept.\\n'\n"," '\\n'\n"," '* In \"Gorilla: Large Language Model Connected with Massive APIs\", the '\n"," 'authors release a finetuned LLaMA-based model that surpasses the performance '\n"," 'of GPT-4 on writing API calls, demonstrating the potential for LLMs to use '\n"," 'tools more accurately.\\n'\n"," '* In \"ToolLLM: Facilitating Large Language Models to Master 16000+ '\n"," 'Real-world APIs\", the authors introduce a general tool-use framework '\n"," 'encompassing data construction, model training, and evaluation, which '\n"," 'enables LLMs to master real-world APIs.\\n'\n"," '* In \"Qwen Technical Report\", the authors discuss the development of '\n"," 'coding-specialized models, Code-Qwen and Code-Qwen-Chat, which are built '\n"," 'upon base language models and demonstrate improved performance in comparison '\n"," 'with open-source models.\\n'\n"," '* In \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging '\n"," 'Face\", the authors propose an LLM-powered agent that leverages LLMs (e.g., '\n"," 'ChatGPT) to connect various AI models in machine learning communities (e.g., '\n"," 'Hugging Face) to solve AI tasks.\\n'\n"," '\\n'\n"," 'These papers focus on the capabilities of LLMs in using tools or APIs, '\n"," 'rather than the specific concept of \"Toolformer: Language Models Can Teach '\n"," 'Themselves to Use Tools\".\\n'\n"," '\\n'\n"," '**2. Briefly explain the abstract of each paper:**\\n'\n"," '\\n'\n"," '* \"Gorilla: Large Language Model Connected with Massive APIs\" introduces a '\n"," 'finetuned LLaMA-based model that surpasses the performance of GPT-4 on '\n"," 'writing API calls, enabling LLMs to use tools more accurately and adapt to '\n"," 'test-time document changes.\\n'\n"," '* \"Mind2Web: Towards a Generalist Agent for the Web\" introduces a dataset '\n"," 'for developing and evaluating generalist agents for the web that can follow '\n"," 'language instructions to complete complex tasks on any website, and explores '\n"," 'the use of large language models (LLMs) for building generalist web agents.\\n'\n"," '* \"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world '\n"," 'APIs\" introduces a general tool-use framework encompassing data '\n"," 'construction, model training, and evaluation, which enables LLM')\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qsfE7EmJ0STn"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"f8983ad513194fdcbd9cf6d606a37d04":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_199009d89041476f96cfb20341553f5e","IPY_MODEL_109f8e7ca2814221a7ff481c2e34626d","IPY_MODEL_3b75eb6fc754495994929d7f36e1945f","IPY_MODEL_2ca07f81f8864cc58e6545b4e25f1430"],"layout":"IPY_MODEL_94310722603f4988aee80475e8980ceb"}},"941d528db0fa45febe8a84056ed1cce7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd16de4b101e4e799ffe7fa9cfb94bfe","placeholder":"​","style":"IPY_MODEL_3ce607e886404ddab7a6571b5395eee3","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"6ffd0de542024e06b48de8dfd8b17133":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_890db37c6a1b4b698320900eaa267995","placeholder":"​","style":"IPY_MODEL_f04b23a3796c469790f09ccbf1f4b1c3","value":""}},"46ed5e36f5cd49838a7bda5b09bda633":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_3476724478054d5580404866d6f76f4d","style":"IPY_MODEL_eee2d9053bfc415798460a20bc04900c","value":true}},"2125304c2a624a9b9564e4826ad41065":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_8f528705a3cd445998b3a7053c9aa168","style":"IPY_MODEL_e0c05fa9d3e940aca9d53b0126ebf118","tooltip":""}},"550ae926768746d0ae3ddfb878e73271":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16bb04afd218493bb646a9a9e75a3a97","placeholder":"​","style":"IPY_MODEL_c42151a876f64653a679015cfa6101de","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"94310722603f4988aee80475e8980ceb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"dd16de4b101e4e799ffe7fa9cfb94bfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ce607e886404ddab7a6571b5395eee3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"890db37c6a1b4b698320900eaa267995":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f04b23a3796c469790f09ccbf1f4b1c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3476724478054d5580404866d6f76f4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eee2d9053bfc415798460a20bc04900c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f528705a3cd445998b3a7053c9aa168":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0c05fa9d3e940aca9d53b0126ebf118":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"16bb04afd218493bb646a9a9e75a3a97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c42151a876f64653a679015cfa6101de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99120cc07a7f451bac8e9bdd5360ff1f":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58cd571609fc4e4ab49f26eba170f6bd","placeholder":"​","style":"IPY_MODEL_f856133ed87d4af1bac229b6c02566ef","value":"Connecting..."}},"58cd571609fc4e4ab49f26eba170f6bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f856133ed87d4af1bac229b6c02566ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"199009d89041476f96cfb20341553f5e":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df06f66a8769406e930b645442652334","placeholder":"​","style":"IPY_MODEL_cb35b5db525d41aca552f0e7427a32af","value":"Token is valid (permission: write)."}},"109f8e7ca2814221a7ff481c2e34626d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0f2add67efd4ba9bb114729587c7116","placeholder":"​","style":"IPY_MODEL_37a70e851a794f6e801d2e61c54c3ef4","value":"Your token has been saved in your configured git credential helpers (store)."}},"3b75eb6fc754495994929d7f36e1945f":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36c5f74dab144cb6b49ff105c84fdaec","placeholder":"​","style":"IPY_MODEL_b849f4da354c44c5a4109be5855d3a45","value":"Your token has been saved to /root/.cache/huggingface/token"}},"2ca07f81f8864cc58e6545b4e25f1430":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b80a2776e62a448eafe6ea31ccef3981","placeholder":"​","style":"IPY_MODEL_e22563212265462a9a05a6675c0ee9b9","value":"Login successful"}},"df06f66a8769406e930b645442652334":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb35b5db525d41aca552f0e7427a32af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0f2add67efd4ba9bb114729587c7116":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37a70e851a794f6e801d2e61c54c3ef4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36c5f74dab144cb6b49ff105c84fdaec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b849f4da354c44c5a4109be5855d3a45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b80a2776e62a448eafe6ea31ccef3981":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e22563212265462a9a05a6675c0ee9b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa38c9636e854277a5cd1d41d97d2dce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b789400b0af41dfaaddd231ba586a95","IPY_MODEL_d6a24b1a1a6244569a52b6af9c3656f1","IPY_MODEL_971adde303f6483795b339c0115c7d94"],"layout":"IPY_MODEL_c70434340ff24d8f8d18b15f8eb64364"}},"0b789400b0af41dfaaddd231ba586a95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73ba67b286b34e0e8806f192908f7a2e","placeholder":"​","style":"IPY_MODEL_95a40de37ccf4f2eb9565cb005a68efe","value":"tokenizer_config.json: 100%"}},"d6a24b1a1a6244569a52b6af9c3656f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a451cc62ee14f3597ec0c787fdc1ce3","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5eb81579a10848018fb99f2ef2535e90","value":350}},"971adde303f6483795b339c0115c7d94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceccf734d0f44e6d8c8d33e2cf042cae","placeholder":"​","style":"IPY_MODEL_fac4ed733ac945fe903c789a20aa0acb","value":" 350/350 [00:00&lt;00:00, 16.8kB/s]"}},"c70434340ff24d8f8d18b15f8eb64364":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73ba67b286b34e0e8806f192908f7a2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95a40de37ccf4f2eb9565cb005a68efe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a451cc62ee14f3597ec0c787fdc1ce3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eb81579a10848018fb99f2ef2535e90":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ceccf734d0f44e6d8c8d33e2cf042cae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fac4ed733ac945fe903c789a20aa0acb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28d6e41c27314210910b957649a61433":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef2a465a3fdb42c09e334609a7b7c994","IPY_MODEL_fe118937612d487296807c15c84bbce7","IPY_MODEL_1e256a8d2b28418e8f20a95bb246dec6"],"layout":"IPY_MODEL_442865e15ce5408dbb3aeb457a139afd"}},"ef2a465a3fdb42c09e334609a7b7c994":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_201dae1b7fea4eeaae3a65649a9a8323","placeholder":"​","style":"IPY_MODEL_95e8500a72064e50bcfe4f4fdbcc66fa","value":"vocab.txt: 100%"}},"fe118937612d487296807c15c84bbce7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb70a981ba464115bd87c31e3ae4eb07","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c18ba66e9e9949bdbcda325297674d50","value":231508}},"1e256a8d2b28418e8f20a95bb246dec6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40935025d74a435dacbfd366dd1324ca","placeholder":"​","style":"IPY_MODEL_ac094f1fce444af09b3f3c4af6b275ec","value":" 232k/232k [00:00&lt;00:00, 5.06MB/s]"}},"442865e15ce5408dbb3aeb457a139afd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"201dae1b7fea4eeaae3a65649a9a8323":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95e8500a72064e50bcfe4f4fdbcc66fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb70a981ba464115bd87c31e3ae4eb07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c18ba66e9e9949bdbcda325297674d50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40935025d74a435dacbfd366dd1324ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac094f1fce444af09b3f3c4af6b275ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72769211827f4ea0b6847360665a577b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39acd3ccf7b14790883a542dabfb6075","IPY_MODEL_6c3da03c1341494c9f55d26be22ee580","IPY_MODEL_521214760aa74b95b4fd4fb048ca8c93"],"layout":"IPY_MODEL_6aa8376fda4a4ea7afb30bf23522cee5"}},"39acd3ccf7b14790883a542dabfb6075":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f6acf546c464cd2b155f9211dbc520c","placeholder":"​","style":"IPY_MODEL_5f5dc555d6dd4450aada9585d2251e22","value":"tokenizer.json: 100%"}},"6c3da03c1341494c9f55d26be22ee580":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e8902d5c630449cb5a1eaef8aa073ea","max":466247,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c7df3b71792442c6a8d2a2b2dd9e54e2","value":466247}},"521214760aa74b95b4fd4fb048ca8c93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de557eae54474d769a5828e4371c3018","placeholder":"​","style":"IPY_MODEL_3982b22fe18d427592be448bf91dce55","value":" 466k/466k [00:00&lt;00:00, 3.97MB/s]"}},"6aa8376fda4a4ea7afb30bf23522cee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f6acf546c464cd2b155f9211dbc520c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f5dc555d6dd4450aada9585d2251e22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e8902d5c630449cb5a1eaef8aa073ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7df3b71792442c6a8d2a2b2dd9e54e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de557eae54474d769a5828e4371c3018":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3982b22fe18d427592be448bf91dce55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"caf880807c584df89a3301f96e4c5a8b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f9d4b69e41c44cdb624034e8fb4b3e7","IPY_MODEL_06d332146e34467b9fe460d1ed05c66d","IPY_MODEL_1a266be8f24845eb98469bc4b7ef0e4b"],"layout":"IPY_MODEL_0d1136efdca840d28cd8ec993234b83c"}},"7f9d4b69e41c44cdb624034e8fb4b3e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41e7a22d0bf041c596ef74e32f4d8cfc","placeholder":"​","style":"IPY_MODEL_8be4ce3eb510405d850510f01009305d","value":"special_tokens_map.json: 100%"}},"06d332146e34467b9fe460d1ed05c66d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92fc195eed044ac3acf6a3fffa90c29a","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a91c23ddb4745fe98ba194553a961c1","value":112}},"1a266be8f24845eb98469bc4b7ef0e4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_781ed289bb2940108ce0091afa4a42c2","placeholder":"​","style":"IPY_MODEL_dcea85781d014bcea0ffaf5f1852e66c","value":" 112/112 [00:00&lt;00:00, 4.90kB/s]"}},"0d1136efdca840d28cd8ec993234b83c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41e7a22d0bf041c596ef74e32f4d8cfc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8be4ce3eb510405d850510f01009305d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92fc195eed044ac3acf6a3fffa90c29a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a91c23ddb4745fe98ba194553a961c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"781ed289bb2940108ce0091afa4a42c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcea85781d014bcea0ffaf5f1852e66c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f7b19e72feb481f97a51f899813d05d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3fe182f10a2445aab1509ba7b9d9540","IPY_MODEL_a10861c746304f3a814a78ceb08bd491","IPY_MODEL_6f7bc536b6ad4f01a8107dad8e12314b"],"layout":"IPY_MODEL_599f007c541f4f2faa3c03c769fc1881"}},"e3fe182f10a2445aab1509ba7b9d9540":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b96b8bd3f8148b8b019725c827f824a","placeholder":"​","style":"IPY_MODEL_8e74f605b7b048b08ecb37f86229d0af","value":"config.json: 100%"}},"a10861c746304f3a814a78ceb08bd491":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00dcbd29b45c49888f0eb80946aa2d09","max":612,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bb097a01a174b15af9717911c1992b8","value":612}},"6f7bc536b6ad4f01a8107dad8e12314b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_138983fb4ed34e76a216e08019053822","placeholder":"​","style":"IPY_MODEL_1be1e4e49dc742c988e5faaec5bb8a21","value":" 612/612 [00:00&lt;00:00, 19.9kB/s]"}},"599f007c541f4f2faa3c03c769fc1881":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b96b8bd3f8148b8b019725c827f824a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e74f605b7b048b08ecb37f86229d0af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00dcbd29b45c49888f0eb80946aa2d09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bb097a01a174b15af9717911c1992b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"138983fb4ed34e76a216e08019053822":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1be1e4e49dc742c988e5faaec5bb8a21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b566dd5da324dac8b835da73884e691":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_123fc8f3d4004c4da4fc8d91bd6236e3","IPY_MODEL_a2f588fc1a5c46a3936fb027aeff6508","IPY_MODEL_b91c6629fc4947ae956a415b5c283d63"],"layout":"IPY_MODEL_966905699ee341c6a44e130cce8dcdf8"}},"123fc8f3d4004c4da4fc8d91bd6236e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e484f61007e64dc4aae494a59c2c9ea9","placeholder":"​","style":"IPY_MODEL_8aed86cf700e44e68038439446348074","value":"model.safetensors: 100%"}},"a2f588fc1a5c46a3936fb027aeff6508":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3567bbd98a5487eb3377cd152bd1dd1","max":90868376,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ee86e2a1ca049a8a3cf4cfb6de1f35e","value":90868376}},"b91c6629fc4947ae956a415b5c283d63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b6dcc12f97a4c19ac24c42e16f395d7","placeholder":"​","style":"IPY_MODEL_f9b776dca6b64e0bb34746ce2de6dbfe","value":" 90.9M/90.9M [00:01&lt;00:00, 115MB/s]"}},"966905699ee341c6a44e130cce8dcdf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e484f61007e64dc4aae494a59c2c9ea9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8aed86cf700e44e68038439446348074":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3567bbd98a5487eb3377cd152bd1dd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ee86e2a1ca049a8a3cf4cfb6de1f35e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b6dcc12f97a4c19ac24c42e16f395d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9b776dca6b64e0bb34746ce2de6dbfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}