{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"id":"qrmwf0_3EoyA"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import torch\n","import os\n","from pprint import pprint\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","# from langchain_openai import ChatOpenAI\n","from dotenv import load_dotenv\n","# Import things that are needed generically\n","from langchain.pydantic_v1 import BaseModel, Field\n","from langchain.tools import BaseTool, StructuredTool, tool\n","\n","from langchain_core.utils.function_calling import convert_to_openai_function\n","from langchain_openai import ChatOpenAI\n","\n","from langchain import hub\n","from langchain.agents import AgentExecutor, create_openai_tools_agent"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /c1/yumin/.cache/huggingface/token\n","Login successful\n"]}],"source":["import semantic_scholoar_api as ss"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["dotenv_path = '../../../.env'\n","load_dotenv(dotenv_path)\n","# openai_api = os.getenv(\"OPENAI_API_KEY\")\n","api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Define Tools"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class load_paper_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","    start_page: int = Field(description='start page number')\n","    \n","loadpaper = StructuredTool.from_function(\n","    func=ss.query2content,\n","    name=\"loadpaper\",\n","    description=\"The `loadPaper` tool is designed to facilitate the process of retrieving and reading academic papers based on a given search query. The `query` parameter is a string representing the title of the paper. The `loadPaper` tool extracts and returns the text content from the PDF. If the paper does not contain the desired information, you can adjust the start_page argument to specify the starting page for reading. For instance, to extract the title, abstract, or detailed method sections, set start_page to 1. Conversely, for sections like the appendix that are typically found towards the end of the paper, set start_page to a higher number, such as 9.\",\n","    args_schema=load_paper_input\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class recommend_reference_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","\n","recommend_reference = StructuredTool.from_function(\n","    func=ss.reference_recommend,\n","    name=\"recommend_reference\",\n","    description=\"The reference_recommend function recommends relevant academic papers based on a given query, focusing on papers that the target paper's references. This tool is ideal for researchers and academics looking to find related literature that has been directly cited by the target paper.\",\n","    args_schema=recommend_reference_input\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class citation_recommend_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","\n","recommend_citation = StructuredTool.from_function(\n","    func=ss.citation_recommend,\n","    name=\"recommend_citation\",\n","    description=\"The recommend_citation function identifies and recommends subsequent papers **that have cited a given target paper**, providing valuable insights into the evolution and impact of the research. This tool helps researchers discover influential follow-up studies and stay updated with the latest developments in their field.\",\n","    args_schema=citation_recommend_input\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Make Agent with GPT3.5"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["model = ChatOpenAI(model=\"gpt-3.5-turbo\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["tools = [loadpaper, recommend_reference, recommend_citation]\n","# functions = [convert_to_openai_function(t) for t in tools]\n","# functions[1]"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# load Agent prompt\n","prompt = hub.pull(\"hwchase17/openai-tools-agent\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Choose the LLM that will drive the agent\n","# Only certain models support this\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","# Construct the OpenAI Tools agent\n","agent = create_openai_tools_agent(llm, tools, prompt)\n","# Create an agent executor by passing in the agent and tools\n","agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'query': 'Language Models Can Teach Themselves to Use Tools', 'start_page': 3}`\n","\n","\n","\u001b[0m### Searched Paper ###\n","Title: Toolformer: Language Models Can Teach Themselves to Use Tools\n","Authors: Thomas Scialom\n","Published: 2023-02-09T16:49:57Z\n","ID: 2302.04761v1\n","File ./papers_db/2302.04761v1.pdf already exists. Skipping download.\n","Page limit reached at 4\n","\u001b[36;1m\u001b[1;3mYour task is to add calls to a Question Answering API to a piece of text. The questions should help you get information required to complete the text. You can call the API by writing \"[QA(question)]\" where \"question\" is the question you want to ask. Here are some examples of API calls: Input: Joe Biden was born in Scranton, Pennsylvania. Output: Joe Biden was born in [QA(\"Where was Joe Biden born?\")] Scranton, [QA(\"In which state is Scranton?\")] Pennsylvania. Input: Coca-Cola, or Coke, is a carbonated soft drink manufactured by the Coca-Cola Company. Output: Coca-Cola, or [QA(\"What other name is Coca-Cola known by?\")] Coke, is a carbonated soft drink manufactured by [QA(\"Who manufactures Coca-Cola?\")] the Coca-Cola Company. Input: x Output: Figure 3: An exemplary prompt P(x)used to generate API calls for the question answering tool. Mitself on this dataset. Each of these steps is described in more detail below. Sampling API Calls For each API, we write a promptP(x)that encourages the LM to anno- tate an example x=x1;:::;x nwith API calls. An example of such a prompt for a question an- swering tool is shown in Figure 3; all prompts used are shown in Appendix A.2. Let pM(zn+1j z1;:::;z n)be the probability that Massigns to tokenzn+1as a continuation for the sequence z1;:::;z n. We ﬁrst sample up to kcandidate posi- tions for doing API calls by computing, for each i2f1;:::;ng, the probability pi=pM(<API>jP(x);x1:i\u00001) thatMassigns to starting an API call at position i. Given a sampling threshold s, we keep all po- sitionsI=fijpi> sg; if there are more than k such positions, we only keep the top k. For each position i2I, we then obtain up to m API callsc1 i;:::;cm iby sampling from Mgiven the sequence [P(x);x1;:::;x i\u00001;<API> ]as a preﬁx and</API> as an end-of-sequence token.2 2We discard all examples where Mdoes not generate the </API> token.Executing API Calls As a next step, we execute all API calls generated by Mto obtain the corre- sponding results. How this is done depends entirely on the API itself – for example, it can involve call- ing another neural network, executing a Python script or using a retrieval system to perform search over a large corpus. The response for each API call cineeds to be a single text sequence ri. Filtering API Calls Letibe the position of the API callciin the sequence x=x1;:::;x n, and let ribe the response from the API. Further, given a sequence (wiji2N)ofweights , let Li(z) =\u0000nX j=iwj\u0000i\u0001logpM(xjjz;x1:j\u00001) be the weighted cross entropy loss for Mover the tokensxi;:::;x nif the model is preﬁxed with z. We compare two different instantiations of this loss: L+ i=Li(e(ci;ri)) L\u0000 i= min (Li(\");Li(e(ci;\"))) where\"denotes an empty sequence. The former is the weighted loss over all tokens xi;:::;x nif the API call and its result are given to Mas a preﬁx;3 the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to Mif providing it with both the input andthe output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a ﬁltering threshold f, we thus only keep API calls for which L\u0000 i\u0000L+ i\u0015 f holds, i.e., adding the API call and its result reduces the loss by at least f, compared to not doing any API call or obtaining no result from it. Model Finetuning After sampling and ﬁltering calls for all APIs, we ﬁnally merge the remaining API calls and interleave them with the original inputs. That is, for an input text x=x1;:::;x n with a corresponding API call and result (ci;ri)at positioni, we construct the new sequence x\u0003= 3We provide e(ci;ri)as a preﬁx instead of inserting it at positionibecauseMis not yet ﬁnetuned on any examples containing API calls, so inserting it in the middle of xwould interrupt the ﬂow and not align with patterns in the pretraining corpus, thus hurting perplexity.\u001b[0m\u001b[32;1m\u001b[1;3mThe math expression about the filtering of API calls in the paper \"Language Models Can Teach Themselves to Use Tools\" on page 3 involves several key components:\n","\n","1. Sampling API Calls:\n","   - A prompt P(x) is used to encourage the language model (LM) to annotate an example x with API calls.\n","   - The probability pM(z_n+1 | z_1, ..., z_n) is computed to determine the likelihood of the LM assigning a token z_n+1 as a continuation for the sequence z_1, ..., z_n.\n","   - Up to k candidate positions for API calls are sampled based on the probability pi = pM(<API> | P(x); x_1:i-1).\n","   - Positions with probabilities above a sampling threshold s are kept, and for each position i in the set I, up to m API calls are obtained by sampling from M given the sequence [P(x); x_1; ...; x_i-1; <API>] as a prefix.\n","\n","2. Executing API Calls:\n","   - All API calls generated by the LM are executed to obtain the corresponding results, which can involve calling another neural network, executing a Python script, or using a retrieval system for search.\n","\n","3. Filtering API Calls:\n","   - Let i be the position of the API call ci in the sequence x = x_1, ..., x_n, and let ri be the response from the API.\n","   - Define Li(z) as the weighted cross-entropy loss for the LM over the tokens x_i, ..., x_n if the model is prefixed with z.\n","   - Two different instantiations of the loss are compared: L+ i = Li(e(ci, ri)) and L- i = min(Li(\"\"), Li(e(ci, \"\")).\n","   - API calls are filtered based on a filtering threshold f, where only calls for which L- i ≤ L+ i + f are kept. This criterion ensures that adding the API call and its result reduces the loss by at least f compared to not using any API call or not receiving a result.\n","\n","4. Model Finetuning:\n","   - After sampling and filtering calls for all APIs, the remaining API calls are merged and interleaved with the original inputs to create a new sequence x'.\n","\n","This process of filtering API calls based on the impact on the LM's loss function helps determine the relevance and usefulness of incorporating external information through API calls in the language model training.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# If you want the model to see the specific page or specific content in the paper, mention the page in the prompt.\n","\n","output = agent_executor.invoke({\"input\": \"explain about the math expression about the Filtering api calls in the 3 page of the paper 'Language Models Can Teach Themselves to Use Tools'\"})"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('The math expression about the filtering of API calls in the paper \"Language '\n"," 'Models Can Teach Themselves to Use Tools\" on page 3 involves several key '\n"," 'components:\\n'\n"," '\\n'\n"," '1. Sampling API Calls:\\n'\n"," '   - A prompt P(x) is used to encourage the language model (LM) to annotate '\n"," 'an example x with API calls.\\n'\n"," '   - The probability pM(z_n+1 | z_1, ..., z_n) is computed to determine the '\n"," 'likelihood of the LM assigning a token z_n+1 as a continuation for the '\n"," 'sequence z_1, ..., z_n.\\n'\n"," '   - Up to k candidate positions for API calls are sampled based on the '\n"," 'probability pi = pM(<API> | P(x); x_1:i-1).\\n'\n"," '   - Positions with probabilities above a sampling threshold s are kept, and '\n"," 'for each position i in the set I, up to m API calls are obtained by sampling '\n"," 'from M given the sequence [P(x); x_1; ...; x_i-1; <API>] as a prefix.\\n'\n"," '\\n'\n"," '2. Executing API Calls:\\n'\n"," '   - All API calls generated by the LM are executed to obtain the '\n"," 'corresponding results, which can involve calling another neural network, '\n"," 'executing a Python script, or using a retrieval system for search.\\n'\n"," '\\n'\n"," '3. Filtering API Calls:\\n'\n"," '   - Let i be the position of the API call ci in the sequence x = x_1, ..., '\n"," 'x_n, and let ri be the response from the API.\\n'\n"," '   - Define Li(z) as the weighted cross-entropy loss for the LM over the '\n"," 'tokens x_i, ..., x_n if the model is prefixed with z.\\n'\n"," '   - Two different instantiations of the loss are compared: L+ i = Li(e(ci, '\n"," 'ri)) and L- i = min(Li(\"\"), Li(e(ci, \"\")).\\n'\n"," '   - API calls are filtered based on a filtering threshold f, where only '\n"," 'calls for which L- i ≤ L+ i + f are kept. This criterion ensures that adding '\n"," 'the API call and its result reduces the loss by at least f compared to not '\n"," 'using any API call or not receiving a result.\\n'\n"," '\\n'\n"," '4. Model Finetuning:\\n'\n"," '   - After sampling and filtering calls for all APIs, the remaining API '\n"," 'calls are merged and interleaved with the original inputs to create a new '\n"," \"sequence x'.\\n\"\n"," '\\n'\n"," \"This process of filtering API calls based on the impact on the LM's loss \"\n"," 'function helps determine the relevance and usefulness of incorporating '\n"," 'external information through API calls in the language model training.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `recommend_citation` with `{'query': 'Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[38;5;200m\u001b[1;3m[{'paperId': '7d8905a1fd288068f12c8347caeabefd36d0dd6c', 'title': 'Gorilla: Large Language Model Connected with Massive APIs', 'abstract': \"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\", 'influentialCitationCount': 26, 'publicationDate': '2023-05-24'}, {'paperId': '58f8925a8b87054ad0635a6398a7fe24935b1604', 'title': 'Mind2Web: Towards a Generalist Agent for the Web', 'abstract': 'We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.', 'influentialCitationCount': 23, 'publicationDate': '2023-06-09'}, {'paperId': '0bfc804e31eecfd77f45e4ee7f4d629fffdcd628', 'title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'abstract': 'Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.', 'influentialCitationCount': 49, 'publicationDate': '2023-07-31'}, {'paperId': '5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0', 'title': 'Qwen Technical Report', 'abstract': 'Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.', 'influentialCitationCount': 60, 'publicationDate': '2023-09-28'}, {'paperId': 'd1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43', 'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face', 'abstract': 'Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.', 'influentialCitationCount': 50, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some recommended papers related to \"Language Models Can Teach Themselves to Use Tools\":\n","\n","1. **Gorilla: Large Language Model Connected with Massive APIs**\n","   - Abstract: Large Language Models (LLMs) have seen significant advancements recently, excelling in various tasks. Gorilla, a finetuned LLaMA-based model, surpasses the performance of GPT-4 in writing API calls. It demonstrates a strong capability to adapt to test-time document changes and mitigate hallucination issues.\n","   - Publication Date: May 24, 2023\n","\n","2. **Mind2Web: Towards a Generalist Agent for the Web**\n","   - Abstract: Introduces Mind2Web, a dataset for developing generalist agents for the web. It provides diverse tasks from real-world websites and explores using large language models for building generalist web agents.\n","   - Publication Date: June 9, 2023\n","\n","3. **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs**\n","   - Abstract: ToolLLM bridges the gap in tool-use capabilities of LLMs by introducing a general tool-use framework. It fine-tunes LLaMA to execute complex instructions and generalize to unseen APIs, demonstrating strong zero-shot generalization ability.\n","   - Publication Date: July 31, 2023\n","\n","4. **Qwen Technical Report**\n","   - Abstract: Introduces Qwen, a comprehensive language model series with distinct models showcasing superior performance across various tasks. The chat models possess advanced tool-use and planning capabilities, even on complex tasks like utilizing a code interpreter.\n","   - Publication Date: September 28, 2023\n","\n","5. **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face**\n","   - Abstract: Presents HuggingGPT, an LLM-powered agent that connects various AI models to solve complicated AI tasks. By leveraging ChatGPT's language capabilities and AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated tasks.\n","   - Publication Date: December 31, 9999\n","\n","These papers offer valuable insights into the advancements and applications of large language models in various domains.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["output = agent_executor.invoke({\"input\": \"recommend the future works of the paper 'Language Models Can Teach Themselves to Use Tools'\"})\n","# TODO : we do not need paperID in this tool."]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('Here are some recommended papers related to \"Language Models Can Teach '\n"," 'Themselves to Use Tools\":\\n'\n"," '\\n'\n"," '1. **Gorilla: Large Language Model Connected with Massive APIs**\\n'\n"," '   - Abstract: Large Language Models (LLMs) have seen significant '\n"," 'advancements recently, excelling in various tasks. Gorilla, a finetuned '\n"," 'LLaMA-based model, surpasses the performance of GPT-4 in writing API calls. '\n"," 'It demonstrates a strong capability to adapt to test-time document changes '\n"," 'and mitigate hallucination issues.\\n'\n"," '   - Publication Date: May 24, 2023\\n'\n"," '\\n'\n"," '2. **Mind2Web: Towards a Generalist Agent for the Web**\\n'\n"," '   - Abstract: Introduces Mind2Web, a dataset for developing generalist '\n"," 'agents for the web. It provides diverse tasks from real-world websites and '\n"," 'explores using large language models for building generalist web agents.\\n'\n"," '   - Publication Date: June 9, 2023\\n'\n"," '\\n'\n"," '3. **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world '\n"," 'APIs**\\n'\n"," '   - Abstract: ToolLLM bridges the gap in tool-use capabilities of LLMs by '\n"," 'introducing a general tool-use framework. It fine-tunes LLaMA to execute '\n"," 'complex instructions and generalize to unseen APIs, demonstrating strong '\n"," 'zero-shot generalization ability.\\n'\n"," '   - Publication Date: July 31, 2023\\n'\n"," '\\n'\n"," '4. **Qwen Technical Report**\\n'\n"," '   - Abstract: Introduces Qwen, a comprehensive language model series with '\n"," 'distinct models showcasing superior performance across various tasks. The '\n"," 'chat models possess advanced tool-use and planning capabilities, even on '\n"," 'complex tasks like utilizing a code interpreter.\\n'\n"," '   - Publication Date: September 28, 2023\\n'\n"," '\\n'\n"," '5. **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging '\n"," 'Face**\\n'\n"," '   - Abstract: Presents HuggingGPT, an LLM-powered agent that connects '\n"," \"various AI models to solve complicated AI tasks. By leveraging ChatGPT's \"\n"," 'language capabilities and AI models in Hugging Face, HuggingGPT can tackle a '\n"," 'wide range of sophisticated tasks.\\n'\n"," '   - Publication Date: December 31, 9999\\n'\n"," '\\n'\n"," 'These papers offer valuable insights into the advancements and applications '\n"," 'of large language models in various domains.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'query': 'Large Language Model Connected with Massive APIs', 'start_page': 1}`\n","\n","\n","\u001b[0m### Searched Paper ###\n","Title: Gorilla: Large Language Model Connected with Massive APIs\n","Authors: Joseph E. Gonzalez\n","Published: 2023-05-24T16:48:11Z\n","ID: 2305.15334v1\n","Page limit reached at 4\n","\u001b[36;1m\u001b[1;3mGorilla: Large Language Model Connected with Massive APIs Shishir G. Patil1∗Tianjun Zhang1,∗Xin Wang2Joseph E. Gonzalez1 1UC Berkeley2Microsoft Research sgp@berkeley.edu Abstract Large Language Models (LLMs) have seen an impressive wave of advances re- cently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of- the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model’s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla’s code, model, data, and demo are available at https://gorilla.cs.berkeley.edu 1 Introduction Recent advances in large language models (LLMs) [ 10,5,32,6,29,30] have enabled significant new capabilities including natural dialogue, mathematical reasoning, and program synthesis. However, despite these advances, LLMs are still fundamentally limited by the information they can store in a fixed set of weights and the things they can compute using a static computation graph and limited context. Furthermore, as the world changes, LLMs require retraining to update their knowledge and reasoning capabilities. By empowering LLMs to use tools [ 33], we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks. By providing access to search technologies and databases, [ 26,39,37] demonstrated that we can augment LLMs to address a significantly larger and more dynamic knowledge space. Similarly, by providing access to computational tools, [ 39,2] demonstrated that LLMs can accomplish complex computational tasks. Consequently, leading LLM providers[ 29], have started to integrate plugins to allow LLMs to invoke external tools through APIs. This transition from a small set of hand-coded tools, to the ability to invoke a vast space of changing cloud APIs could transform LLMs into the primary interface to computing infrastructure and the web. Tasks ranging from booking an entire vacation to hosting a conference, could become as simple as talking to an LLM that has access to the flight, car rental, hotel, catering, and entertainment web APIs. However, much of the prior work [ 35,24] integrating tools into LLMs considered a small well documented set of APIs that can be easily injected into the prompt. ∗Equal contribution. Preprint. Under review.arXiv:2305.15334v1 [cs.CL] 24 May 2023GPT-4<domain>:Speech-to-Text<api_provider>:TorchHub<code>:asr_model= torch.hub.load('snakers4/silero-models', 'asr', source='local')result = asr_model.transcribe(audio_path)Claude<domain>:Audio-Translation<api_provider>:Pytorch<code>:import torchaudiotranslation = Torchaudio.pipelines.WAV2VEC2_ASR_PIPELINE(\"audio.wav\")Gorilla<domain>:Speech-to-Text<api_provider>:TorchHub<code>:asr_model = torch.hub.load('snakers4/silero-models', 'silero_sst’)result = asr_model.transcribe(audio_path) Hallucinate!Wrong library!Good to go!Prompt: Help me find an API to convert the spoken language in a recorded audio to text using Torch Hub. Figure 1: Examples of API calls . Example API calls generated by GPT-4 [ 29], Claude [ 3], and Gorilla for the given prompt. In this example, GPT-4 presents a model that doesn’t exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call. Better Figure 2: Accuracy (vs) hallucination in four settings, that is, zero-shot (i.e., without any retriever), and with retrievers .BM25 andGPTare commonly used retrievers and the oracle retriever returns relevant documents at 100%, indicating an upper bound. Higher in the graph (higher accuracy) and to the left is better (lower hallucination). Across the entire dataset, our model, Gorilla, improves accuracy while reducing hallucination. Supporting a web scale collection of potentially millions of changing APIs requires rethinking our approach to how we integrate tools. It is not longer possible to describe the full set of APIs in a single context. Many of the APIs will have overlapping functionality with nuanced limitations and constraints. Simply evaluating LLMs in this new setting requires new benchmarks. In this paper, we explore the use of self-instruct fine-tuning and retrieval to enable LLMs to accu- rately select from a large, overlapping, and changing set tools expressed using their APIs and API documentation. We construct, APIBench, a large corpus of APIs with complex and often overlapping functionality by scraping ML APIs (models) from public model hubs. We choose three major model hubs for dataset construction: TorchHub, TensorHub and HuggingFace. We exhaustively include every API call in TorchHub (94 API calls) and TensorHub (696 API calls); For HuggingFace, since the models come in a large number and lots of the models don’t have a specification, we choose the most downloaded 20 models per task category (in a total of 925). We also generate 10 synthetic user question prompts per API using Self-Instruct [ 42]. Thus, each entry in the dataset becomes an instruction reference API pair. We adopt a common AST sub-tree matching technique to evaluate the functional correctness of the generated API. We first parse the generated code into an AST tree, then find a sub-tree whose root node is the API call that we care about (e.g., torch.hub.load ) and use it to index our dataset. We check the functional correctness and hallucination problem for the LLMs, reporting the corresponding accuracy. We then finetune Gorilla, a LLaMA-7B-based model with document retrieval using our dataset. We find that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy as well as reducing hallucination errors. We show an example output in Fig. 1. Further, our retrieval-aware training of Gorilla enables the model to adapt to changes in the API documentation. Finally, we demonstrate Gorilla’s ability to understand and reason about constraints. 22 Related Work Large Language Models Recent strides in the field of LLMs have renovated many downstream domains [ 10,40,48,47], not only in traditional natural language processing tasks but also in program synthesis. Many of these advances are achieved by augmenting pre-trained LLMs by prompting [ 44,14] and instruction fine-tuning [ 11,31,43,15]. Recent open-sourced models like LLaMa [ 40], Alpaca [ 38], and Vicuna [ 9] have furthered the understanding of LLMs and facilitated their experimentation. While our approach, Gorilla, incorporates techniques akin to those mentioned, its primary emphasis is on enhancing the LLMs’ ability to utilize millions of tools, as opposed to refining their conversational skills. Additionally, we pioneer the study of fine-tuning a base model by supplementing it with information retrieval - a first, to the best of our knowledge. Tool Usage The discussion of tool usage within LLMs has seen an upsurge, with models like Toolformer taking the lead [ 33,19,21,26]. Tools often incorporated include web-browsing [ 34], calculators [ 12,39], translation systems [ 39], and Python interpreters [ 14]. While these efforts can be seen as preliminary explorations of marrying LLMs with tool usage, they generally focus on specific tools. Our paper, in contrast, aims to explore a vast array of tools (i.e., API calls) in an open-ended fashion, potentially covering a wide range of applications. With the recent launch of Toolformer [ 33] and GPT-4 [ 29], the importance of API calls has been highlighted, encouraging many works in employing API calls as tooling [ 35,24]. Moreover, the application of API calls in robotics has been explored to some extent [ 41,1]. However, these works primarily aim at showcasing the potential of “prompting” LLMs rather than establishing a systematic method for evaluation and training (including fine-tuning). Our work, on the other hand, concentrates on systematic evaluation and building a pipeline for future use. LLMs for Program Synthesis Harnessing LLMs for program synthesis has historically been a challenging task [ 23,7,45,16,13,20]. Researchers have proposed an array of strategies to prompt LLMs to perform better in coding tasks, including in-context learning [ 44,18,7], task decomposition [ 17,46], and self-debugging [ 8,36]. Besides prompting, there have also been efforts to pretrain language models specifically for code generation [28, 22, 27]. However, these strategies focus on prompting large language models or pre-training them for general program synthesis. In our research, in contrast, we focus on a much restricted domain: the synthesis of linear programs using API calls. General program synthesis, not only is complex, but is also hard to verify and evaluate. API calls, on the other hand, function more like tool usage. This allows the LLM to significantly expand its capabilities without grappling with low-level implementation details. 3 Methodology In this section, we describe APIBench, a comprehensive benchmark constructed from TorchHub, TensorHub, and HuggingFace API Model Cards. We begin by outlining the process of collecting the API dataset and how we generated instruction-answer pairs. We then introduce Gorilla, a novel training paradigm with a information–retriever incorporated into the training and inference pipelines. Finally, we present our AST tree matching evaluation metric. 3.1 Dataset Collection To collect the dataset, we meticulously recorded all online model cards for HuggingFace’s “The Model Hub”, PyTorch Hub, and TensorFlow Hub Models. Throughout the rest of the paper, we call these HuggingFace, Torch Hub, and TensorFlow Hub respectively for brevity. API Documentation The HuggingFace platform hosts and servers about 203,681 models. However, many of them have poor documentation, lack dependencies, have no information in their model card, etc. To filter these out, we pick the top 20 models from each domain. We consider 7 domains in multimodal data, 8 in CV , 12 in NLP, 5 in Audio, 2 in tabular data, and 2 in reinforcement learning. Post filtering, we got a total of 925 models from HuggingFace. TensorFlow Hub is versioned into v1 and v2. The latest version (v2) has 801 models in total, and we process all of them. Post filtering out models, whose mode cards had little to no information, we are left with 626 models. Similar to 3\u001b[0m\u001b[32;1m\u001b[1;3mThe paper titled \"Large Language Model Connected with Massive APIs\" introduces Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 in writing API calls. Gorilla, when combined with a document retriever, demonstrates the ability to adapt to test-time document changes, reducing hallucination errors commonly encountered with LLMs. The paper introduces APIBench, a dataset consisting of HuggingFace, TorchHub, and TensorHub APIs, to evaluate the model's ability accurately. Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy and reduces hallucination errors. The paper emphasizes the importance of empowering LLMs to use tools via API calls to access vast knowledge bases and accomplish complex computational tasks effectively.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# TODO : Change Arxiv Search enging to Semantic Scholar \n","# TODO : error handling when the tool called.\n","# Because below code doesn't work. with the difference of 'Gorilla: '\n","# output = agent_executor.invoke({\"input\": \"summary the abstract of the paper 'Gorilla: Large Language Model Connected with Massive APIs'\"})\n","\n","output = agent_executor.invoke({\"input\": \"summary the abstract of the paper 'Large Language Model Connected with Massive APIs'\"})"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('The paper titled \"Large Language Model Connected with Massive APIs\" '\n"," 'introduces Gorilla, a finetuned LLaMA-based model that surpasses the '\n"," 'performance of GPT-4 in writing API calls. Gorilla, when combined with a '\n"," 'document retriever, demonstrates the ability to adapt to test-time document '\n"," 'changes, reducing hallucination errors commonly encountered with LLMs. The '\n"," 'paper introduces APIBench, a dataset consisting of HuggingFace, TorchHub, '\n"," \"and TensorHub APIs, to evaluate the model's ability accurately. Gorilla \"\n"," 'significantly outperforms GPT-4 in terms of API functionality accuracy and '\n"," 'reduces hallucination errors. The paper emphasizes the importance of '\n"," 'empowering LLMs to use tools via API calls to access vast knowledge bases '\n"," 'and accomplish complex computational tasks effectively.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `recommend_reference` with `{'query': 'Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[33;1m\u001b[1;3m[{'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0', 'title': 'Language Models are Few-Shot Learners', 'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\", 'influentialCitationCount': 3130, 'publicationDate': '2020-05-28', 'intent': 'methodology background result', 'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'}, {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb', 'title': 'PaLM: Scaling Language Modeling with Pathways', 'abstract': 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.', 'influentialCitationCount': 307, 'publicationDate': '2022-04-05', 'intent': 'background', 'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'}, {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221', 'title': 'OPT: Open Pre-trained Transformer Language Models', 'abstract': 'Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.', 'influentialCitationCount': 274, 'publicationDate': '2022-05-02', 'intent': 'result', 'context': 'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'}, {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', 'influentialCitationCount': 18170, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'background', 'context': 'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.'}, {'paperId': '9405cc0d6169988371b2755e573cc28650d14dfe', 'title': 'Language Models are Unsupervised Multitask Learners', 'abstract': 'Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.', 'influentialCitationCount': 2826, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'methodology', 'context': 'To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some recommended reference works related to the paper \"Language Models Can Teach Themselves to Use Tools\":\n","\n","1. **Paper Title:** Language Models are Few-Shot Learners\n","   - **Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. This paper shows that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\n","   - **Publication Date:** 2020-05-28\n","\n","2. **Paper Title:** PaLM: Scaling Language Modeling with Pathways\n","   - **Abstract:** This paper explores the impact of scale on few-shot learning by training a 540-billion parameter, densely activated, Transformer language model called Pathways Language Model PaLM. It achieves breakthrough performance on language understanding and generation benchmarks and demonstrates benefits of scaling in achieving state-of-the-art few-shot learning results.\n","   - **Publication Date:** 2022-04-05\n","\n","3. **Paper Title:** OPT: Open Pre-trained Transformer Language Models\n","   - **Abstract:** OPT presents a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, aiming to be fully and responsibly shared with interested researchers. The paper compares OPT-175B to GPT-3 and discusses the model's performance and carbon footprint.\n","   - **Publication Date:** 2022-05-02\n","\n","4. **Paper Title:** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n","   - **Abstract:** This paper introduces BERT, a language representation model designed for pre-training deep bidirectional representations from unlabeled text. BERT achieves state-of-the-art results on eleven natural language processing tasks and demonstrates the power of pre-training in creating models for various tasks.\n","   - **Publication Date:** Not specified\n","\n","5. **Paper Title:** Language Models are Unsupervised Multitask Learners\n","   - **Abstract:** This paper demonstrates that language models begin to learn natural language processing tasks without explicit supervision when trained on a new dataset of millions of webpages called WebText. The findings suggest a promising path towards building language processing systems that learn tasks from naturally occurring demonstrations.\n","   - **Publication Date:** Not specified\n","\n","These reference works provide valuable insights and background information related to language models and their capabilities in natural language processing tasks.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["output = agent_executor.invoke({\"input\": \"recommend the reference works of the paper 'Language Models Can Teach Themselves to Use Tools'\"})"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('Here are some recommended reference works related to the paper \"Language '\n"," 'Models Can Teach Themselves to Use Tools\":\\n'\n"," '\\n'\n"," '1. **Paper Title:** Language Models are Few-Shot Learners\\n'\n"," '   - **Abstract:** Recent work has demonstrated substantial gains on many '\n"," 'NLP tasks and benchmarks by pre-training on a large corpus of text followed '\n"," 'by fine-tuning on a specific task. This paper shows that scaling up language '\n"," 'models greatly improves task-agnostic, few-shot performance, sometimes even '\n"," 'reaching competitiveness with prior state-of-the-art fine-tuning '\n"," 'approaches.\\n'\n"," '   - **Publication Date:** 2020-05-28\\n'\n"," '\\n'\n"," '2. **Paper Title:** PaLM: Scaling Language Modeling with Pathways\\n'\n"," '   - **Abstract:** This paper explores the impact of scale on few-shot '\n"," 'learning by training a 540-billion parameter, densely activated, Transformer '\n"," 'language model called Pathways Language Model PaLM. It achieves breakthrough '\n"," 'performance on language understanding and generation benchmarks and '\n"," 'demonstrates benefits of scaling in achieving state-of-the-art few-shot '\n"," 'learning results.\\n'\n"," '   - **Publication Date:** 2022-04-05\\n'\n"," '\\n'\n"," '3. **Paper Title:** OPT: Open Pre-trained Transformer Language Models\\n'\n"," '   - **Abstract:** OPT presents a suite of decoder-only pre-trained '\n"," 'transformers ranging from 125M to 175B parameters, aiming to be fully and '\n"," 'responsibly shared with interested researchers. The paper compares OPT-175B '\n"," \"to GPT-3 and discusses the model's performance and carbon footprint.\\n\"\n"," '   - **Publication Date:** 2022-05-02\\n'\n"," '\\n'\n"," '4. **Paper Title:** BERT: Pre-training of Deep Bidirectional Transformers '\n"," 'for Language Understanding\\n'\n"," '   - **Abstract:** This paper introduces BERT, a language representation '\n"," 'model designed for pre-training deep bidirectional representations from '\n"," 'unlabeled text. BERT achieves state-of-the-art results on eleven natural '\n"," 'language processing tasks and demonstrates the power of pre-training in '\n"," 'creating models for various tasks.\\n'\n"," '   - **Publication Date:** Not specified\\n'\n"," '\\n'\n"," '5. **Paper Title:** Language Models are Unsupervised Multitask Learners\\n'\n"," '   - **Abstract:** This paper demonstrates that language models begin to '\n"," 'learn natural language processing tasks without explicit supervision when '\n"," 'trained on a new dataset of millions of webpages called WebText. The '\n"," 'findings suggest a promising path towards building language processing '\n"," 'systems that learn tasks from naturally occurring demonstrations.\\n'\n"," '   - **Publication Date:** Not specified\\n'\n"," '\\n'\n"," 'These reference works provide valuable insights and background information '\n"," 'related to language models and their capabilities in natural language '\n"," 'processing tasks.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"markdown","metadata":{},"source":["### Todo : Make Agent with Llama"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading shards:  25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]"]}],"source":["# we can use 262k version of Llama!\n","model_name = 'gradientai/Llama-3-8B-Instruct-262k'\n","\n","dtype = \"float16\"\n","if torch.cuda.is_bf16_supported():\n","    dtype = \"bfloat16\"\n","\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=getattr(torch, dtype), quantization_config=quantization_config, \n","                                             device_map=\"auto\",  cache_dir=os.getenv(\"HF_HOME\", \"~/.cache/huggingface\"))\n","model.eval()\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMCoSiIlUJj7T12v/N1YxQ2","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
