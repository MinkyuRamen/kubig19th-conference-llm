{"cells":[{"cell_type":"markdown","metadata":{"id":"79YmvumUw19_"},"source":["### Setting for Colab"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19303,"status":"ok","timestamp":1719243584697,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"P42nOWGUwpW_","outputId":"fd3dbbd3-87b5-4771-8717-1f925f8610dd"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","KeyboardInterrupt\n","\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":522,"status":"ok","timestamp":1719243585216,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"W73F_hiPwqoP","outputId":"cac64a15-4d69-4617-abd6-874e49ea24ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Projects/kubig19th-conference-llm/ym\n"]}],"source":["cd /content/drive/MyDrive/Projects/kubig19th-conference-llm/ym"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23951,"status":"ok","timestamp":1719243609165,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"OA7pz_enw1Wc","outputId":"e39c1669-504b-4a72-cab0-05b82c409214"},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.6 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"]}],"source":["!pip install python-dotenv semanticscholar langchain langchain_openai langchain_core langchainhub langchain-community"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28312,"status":"ok","timestamp":1719243637471,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"OhMB4hgnydgs","outputId":"e04b5f76-24ec-49d5-ef56-9f55ce8d3aba"},"outputs":[],"source":["!pip install feedparser PyPDF2 beautifulsoup4 requests scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"T57xxCX6w53q"},"source":["### Import Modules"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":14975,"status":"ok","timestamp":1719243658529,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"qrmwf0_3EoyA"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import torch\n","import os\n","from pprint import pprint\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from dotenv import load_dotenv\n","\n","from langchain.pydantic_v1 import BaseModel, Field\n","from langchain.tools import BaseTool, StructuredTool, tool\n","\n","from langchain_core.utils.function_calling import convert_to_openai_function\n","from langchain_openai import ChatOpenAI\n","\n","from langchain import hub\n","from langchain.agents import AgentExecutor, create_openai_tools_agent"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":352,"status":"ok","timestamp":1719243854067,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"6qbkQ7-qwg9g"},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /c1/yumin/.cache/huggingface/token\n","Login successful\n"]}],"source":["import semantic_scholoar_api as ss\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":331,"status":"ok","timestamp":1719243858386,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"z-6PP4lh61vk"},"outputs":[],"source":["from getpaper import GetPaper"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":422,"status":"ok","timestamp":1719243860897,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"3iekMnd4wg9k"},"outputs":[],"source":["dotenv_path = '.env'\n","load_dotenv(dotenv_path)\n","openai_api = os.getenv(\"OPENAI_API_KEY\")\n","ss_api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")"]},{"cell_type":"markdown","metadata":{"id":"rRoadQ6Xwg9m"},"source":["### Define Tools"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1719243861227,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"TNCsBS7u2Pfc"},"outputs":[],"source":["getpapermodule = GetPaper(ss_api_key, ar5iv_mode = True, path_db = './papers_db', page_limit = 5)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1719243862131,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"uvpUJHr4wg9r"},"outputs":[],"source":["class load_paper_input(BaseModel):\n","    title: str = Field(description=\"target paper title\")\n","    sections: list = Field(description='list of sections', default = None)\n","\n","loadpaper = StructuredTool.from_function(\n","    func=getpapermodule.load_paper,\n","    name=\"loadpaper\",\n","    description=\"The `loadPaper` tool is designed to facilitate the process of retrieving and reading academic papers based on a given search title. \\\n","    The `title` parameter is a string representing the title of the paper. The 'sections' parameter is a list representing the list of the sections in the paper. \\\n","    If the sections parameter is none, you can get the section list of the paper. If the sections parameter get the section list, you can load the paper's content. \\\n","    Use this tool several times to get the section first and then get the detail content of each section\",\n","    args_schema=load_paper_input\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1719243863090,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"-YXoYdfTwg9v"},"outputs":[],"source":["class recommend_reference_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","\n","recommend_reference = StructuredTool.from_function(\n","    func=ss.reference_recommend,\n","    name=\"recommend_reference\",\n","    description=\"The reference_recommend function recommends relevant academic papers based on a given query, focusing on papers that the target paper's references. This tool is ideal for researchers and academics looking to find related literature that has been directly cited by the target paper.\",\n","    args_schema=recommend_reference_input\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1719243863090,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"sjt5lcMEwg9y"},"outputs":[],"source":["class citation_recommend_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","\n","recommend_citation = StructuredTool.from_function(\n","    func=ss.citation_recommend,\n","    name=\"recommend_citation\",\n","    description=\"The recommend_citation function identifies and recommends subsequent papers **that have cited a given target paper**, providing valuable insights into the evolution and impact of the research. This tool helps researchers discover influential follow-up studies and stay updated with the latest developments in their field.\",\n","    args_schema=citation_recommend_input\n",")"]},{"cell_type":"markdown","metadata":{"id":"RCUtBt7Vwg92"},"source":["### Make Agent with GPT3.5"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":351,"status":"ok","timestamp":1719243863903,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"TRLoOwN6wg93"},"outputs":[],"source":["model = ChatOpenAI(model=\"gpt-3.5-turbo\")"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1719243863903,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"mY1iFRyQwg96"},"outputs":[],"source":["tools = [loadpaper, recommend_reference, recommend_citation]\n","# functions = [convert_to_openai_function(t) for t in tools]\n","# functions[1]"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":867,"status":"ok","timestamp":1719243864768,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"B65FMYzowg97"},"outputs":[],"source":["# load Agent prompt\n","# TODO : Write the better prompt for our agent\n","prompt = hub.pull(\"hwchase17/openai-tools-agent\")"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1719243864769,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"ZzNdKVQuwg99"},"outputs":[],"source":["# Choose the LLM that will drive the agent\n","# Only certain models support this\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","# Construct the OpenAI Tools agent\n","agent = create_openai_tools_agent(llm, tools, prompt)\n","# Create an agent executor by passing in the agent and tools\n","agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14164,"status":"ok","timestamp":1719243878930,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"g87scjN0wg9-","outputId":"51b3a809-0059-4ebd-ddca-55404c4c6349"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'ToolFormer : Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n","title\n","Toolformer: Language Models Can Teach Themselves to Use Tools\n","sections\n","          Abstract\n","  1 Introduction\n","  2 Approach\n","        Sampling API Calls\n","        Executing API Calls\n","        Filtering API Calls\n","        Model Finetuning\n","        Inference\n","  3 Tools\n","        Question Answering\n","        Calculator\n","        Wikipedia Search\n","        Machine Translation System\n","        Calendar\n","  4 Experiments\n","    4.1 Experimental Setup\n","        Dataset Generation\n","        Model Finetuning\n","        Baseline Models\n","    4.2 Downstream Tasks\n","      4.2.1 LAMA\n","      4.2.2 Math Datasets\n","      4.2.3 Question Answering\n","      4.2.4 Multilingual Question Answering\n","      4.2.5 Temporal Datasets\n","    4.3 Language Modeling\n","    4.4 Scaling Laws\n","  5 Analysis\n","        Decoding Strategy\n","        Data Quality\n","  6 Related Work\n","        Language Model Pretraining\n","        Tool Use\n","        Bootstrapping\n","  7 Limitations\n","  8 Conclusion\n","  References\n","  Appendix A API Details\n","    A.1 Implementation\n","        Question Answering\n","        Calculator\n","        Calendar\n","        Machine Translation\n","    A.2 Prompts\n","        Question Answering\n","        Calculator\n","        Wikipedia Search\n","        Machine Translation\n","        Calendar\n","  Appendix B Toolformer Training\n","  Appendix C Zero-Shot Prompts\n","    C.1 LAMA and TempLAMA\n","    C.2 Math Benchmarks\n","    C.3 Question Answering\n","    C.4 Multilingual Question Answering\n","  Appendix D Dateset\n","\n","\n"," Use the 'loadpaper' tool again, specifying the exact sections you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'ToolFormer : Language Models Can Teach Themselves to Use Tools', 'sections': ['Filtering API Calls']}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mFiltering API Calls\n","Let ið‘–i be the position of the API call cisubscriptð‘ð‘–c_{i} in the sequence ð±=x1,â€¦,xnð±subscriptð‘¥1â€¦subscriptð‘¥ð‘›\\mathbf{x}=x_{1},\\ldots,x_{n}, and let risubscriptð‘Ÿð‘–r_{i} be the response from the API. Further, given a sequence (wiâˆ£iâˆˆâ„•)conditionalsubscriptð‘¤ð‘–ð‘–â„•(w_{i}\\mid i\\in\\mathbb{N}) of weights, let\n","\n","Liâ€‹(ð³)=âˆ’âˆ‘j=inwjâˆ’iâ‹…logâ¡pMâ€‹(xjâˆ£ð³,x1:jâˆ’1)subscriptð¿ð‘–ð³superscriptsubscriptð‘—ð‘–ð‘›â‹…subscriptð‘¤ð‘—ð‘–subscriptð‘ð‘€conditionalsubscriptð‘¥ð‘—ð³subscriptð‘¥:1ð‘—1L_{i}(\\mathbf{z})=-\\sum_{j=i}^{n}w_{j-i}\\cdot\\log{p_{M}(x_{j}\\mid\\mathbf{z},x_{1:j-1})}\n","\n","be the weighted cross entropy loss for Mð‘€M over the tokens xi,â€¦,xnsubscriptð‘¥ð‘–â€¦subscriptð‘¥ð‘›x_{i},\\ldots,x_{n} if the model is prefixed with ð³ð³\\mathbf{z}.\n","We compare two different instantiations of this loss:\n","\n","Li+superscriptsubscriptð¿ð‘–\\displaystyle L_{i}^{+}\n","=Liâ€‹(eâ€‹(ci,ri))absentsubscriptð¿ð‘–esubscriptð‘ð‘–subscriptð‘Ÿð‘–\\displaystyle=L_{i}(\\text{e}(c_{i},r_{i}))\n","\n","Liâˆ’superscriptsubscriptð¿ð‘–\\displaystyle L_{i}^{-}\n","=minâ¡(Liâ€‹(Îµ),Liâ€‹(eâ€‹(ci,Îµ)))absentsubscriptð¿ð‘–ðœ€subscriptð¿ð‘–esubscriptð‘ð‘–ðœ€\\displaystyle=\\min\\left(L_{i}(\\varepsilon),L_{i}(\\text{e}(c_{i},\\varepsilon))\\right)\n","\n","where Îµðœ€\\varepsilon denotes an empty sequence. The former is the weighted loss over all tokens xi,â€¦,xnsubscriptð‘¥ð‘–â€¦subscriptð‘¥ð‘›x_{i},\\ldots,x_{n} if the API call and its result are given to Mð‘€M as a prefix;333We provide eâ€‹(ci,ri)esubscriptð‘ð‘–subscriptð‘Ÿð‘–\\text{e}(c_{i},r_{i}) as a prefix instead of inserting it at position ið‘–i because Mð‘€M is not yet finetuned on any examples containing API calls, so inserting it in the middle of ð±ð±\\mathbf{x} would interrupt the flow and not align with patterns in the pretraining corpus, thus hurting perplexity. the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to Mð‘€M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a filtering threshold Ï„fsubscriptðœð‘“\\tau_{f}, we thus only keep API calls for which\n","\n","Liâˆ’âˆ’Li+â‰¥Ï„fsuperscriptsubscriptð¿ð‘–superscriptsubscriptð¿ð‘–subscriptðœð‘“L_{i}^{-}-L_{i}^{+}\\geq\\tau_{f}\n","\n","holds, i.e., adding the API call and its result reduces the loss by at least Ï„fsubscriptðœð‘“\\tau_{f}, compared to not doing any API call or obtaining no result from it.\n","\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"ToolFormer: Language Models Can Teach Themselves to Use Tools\" discusses the filtering of API calls in the context of language models. Here is an explanation of the math expression related to filtering API calls as presented in the paper:\n","\n","The paper introduces a weighted cross-entropy loss function for a model M over a sequence of tokens xi to xn, where i represents the position of the API call ci in the sequence and ri represents the response from the API. The loss function is defined as:\n","\n","Li(ð³) = -âˆ‘j=i to n wj-i * log pM(xj | ð³, x1:j-1)\n","\n","In this expression:\n","- Li(ð³) is the weighted cross-entropy loss for model M over the tokens xi to xn if the model is prefixed with ð³.\n","- wj-i represents a sequence of weights.\n","- pM(xj | ð³, x1:j-1) represents the conditional probability of token xj given the prefix ð³ and the tokens x1 to xj-1.\n","\n","The paper compares two different instantiations of this loss:\n","1. Li+ = Li(e(ci, ri)): This represents the weighted loss over all tokens if the API call and its result are given to the model as a prefix.\n","2. Li- = min(Li(Îµ), Li(e(ci, Îµ)): This represents the minimum of the losses obtained from not doing any API call at all and doing an API call but not providing the response.\n","\n","The filtering process involves a filtering threshold Ï„f, where API calls are kept only if the difference between Li- and Li+ is greater than or equal to Ï„f. In other words, an API call is considered helpful if providing both the input and output of the call makes it easier for the model to predict future tokens compared to not receiving the API call at all or receiving only its input.\n","\n","This filtering mechanism ensures that only API calls that significantly reduce the loss and aid in prediction are retained for further processing by the language model.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# If you want the model to see the specific page or specific content in the paper, mention the page in the prompt.\n","\n","output = agent_executor.invoke({\"input\": \"explain about the math expression about the Filtering api calls in the paper 'ToolFormer : Language Models Can Teach Themselves to Use Tools'\"})"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1719243878931,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"5SQLHhC3wg9_","outputId":"42a11704-7d26-49f0-82ee-49da6d87e424"},"outputs":[{"name":"stdout","output_type":"stream","text":["('The paper \"Language Models Can Teach Themselves to Use Tools\" discusses the '\n"," 'filtering of API calls in the context of language models. Here is an '\n"," 'explanation of the math expression related to filtering API calls as '\n"," 'presented in the paper:\\n'\n"," '\\n'\n"," 'The paper introduces a weighted cross-entropy loss function for a language '\n"," 'model M over a sequence of tokens xi,â€¦,xn, where i represents the position '\n"," 'of the API call ci in the sequence and ri is the response from the API. The '\n"," 'loss function is defined as:\\n'\n"," '\\n'\n"," 'Li(ð³) = -âˆ‘j=iâ¿ wj-i * log pM(xjâˆ£ð³,x1:j-1)\\n'\n"," '\\n'\n"," 'In this expression:\\n'\n"," '- Li(ð³) is the weighted cross-entropy loss for model M over the tokens if '\n"," 'the model is prefixed with ð³.\\n'\n"," '- wj-i represents a sequence of weights.\\n'\n"," '- pM(xjâˆ£ð³,x1:j-1) is the conditional probability of token xj given the '\n"," 'prefix ð³ and the tokens x1:j-1.\\n'\n"," '\\n'\n"," 'The paper compares two instantiations of this loss:\\n'\n"," '1. Li+ = Li(e(ci,ri)): Weighted loss over all tokens if the API call and its '\n"," 'result are given as a prefix.\\n'\n"," '2. Li- = min(Li(Îµ), Li(e(ci,Îµ)): Minimum of the losses obtained from not '\n"," 'doing any API call and doing an API call without providing the response.\\n'\n"," '\\n'\n"," 'The filtering process involves a filtering threshold Ï„f, where API calls are '\n"," 'kept only if the difference between Li- and Li+ is greater than or equal to '\n"," 'Ï„f. This condition ensures that adding the API call and its result reduces '\n"," 'the loss by at least Ï„f compared to not using any API call or not receiving '\n"," 'its result.\\n'\n"," '\\n'\n"," 'This math expression and filtering mechanism help determine the relevance '\n"," \"and impact of API calls on the language model's performance and prediction \"\n"," 'accuracy.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21991,"status":"ok","timestamp":1719243916015,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"26_jBBViwg-A","outputId":"7db3611f-f111-4c13-d2ae-b8452e3ea654"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `recommend_citation` with `{'query': 'Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[38;5;200m\u001b[1;3m[{'paperId': '7d8905a1fd288068f12c8347caeabefd36d0dd6c', 'title': 'Gorilla: Large Language Model Connected with Massive APIs', 'abstract': \"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\", 'influentialCitationCount': 29, 'publicationDate': '2023-05-24'}, {'paperId': '58f8925a8b87054ad0635a6398a7fe24935b1604', 'title': 'Mind2Web: Towards a Generalist Agent for the Web', 'abstract': 'We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.', 'influentialCitationCount': 26, 'publicationDate': '2023-06-09'}, {'paperId': '0bfc804e31eecfd77f45e4ee7f4d629fffdcd628', 'title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'abstract': 'Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.', 'influentialCitationCount': 51, 'publicationDate': '2023-07-31'}, {'paperId': '5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0', 'title': 'Qwen Technical Report', 'abstract': 'Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.', 'influentialCitationCount': 73, 'publicationDate': '2023-09-28'}, {'paperId': 'd1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43', 'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face', 'abstract': 'Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.', 'influentialCitationCount': 50, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}]\u001b[0m\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'Gorilla: Large Language Model Connected with Massive APIs'}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n","title\n","('h1', 'Gorilla: Large Language Model Connected with Massive APIs')\n","sections\n","          Abstract\n","  1 Introduction\n","  2 Related Work\n","        Large Language Models\n","        Tool Usage\n","        LLMs for Program Synthesis\n","  3 Methodology\n","    3.1 Dataset Collection\n","        API Documentation\n","        Instruction Generation\n","    3.2 Gorilla\n","        API Call with Constraints\n","        Retriever-Aware training\n","        Gorilla Inference\n","    3.3 Verifying APIs\n","        AST Sub-Tree Matching\n","  4 Evaluation\n","        Baselines\n","        Retrievers\n","    4.1 AST Accuracy on API call\n","        Finetuning without Retrieval\n","        Finetuning with Retrieval\n","        Hallucination with LLM\n","    4.2 Test-Time Documentation Change\n","    4.3 API Call with Constraints\n","  5 Conclusion\n","  6 Limitations & Social Impacts\n","  7 Acknowledgement\n","  References\n","  8 Appendix\n","    8.1 Dataset Details\n","        Domain Classification\n","        API Call Task\n","        API Provider Component\n","        Explanation Element\n","        Code\n","    8.2 Gorilla Details\n","        Data\n","        Training\n","    8.3 Performance Comparison\n","      8.3.1 Evaluation\n","      8.3.2 Hallucination\n","\n","\n"," Use the 'loadpaper' tool again specifying the exact sections you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'Gorilla: Large Language Model Connected with Massive APIs', 'sections': ['Abstract']}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mAbstractLarge Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for todayâ€™s state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the modelâ€™s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs.\n","Gorillaâ€™s code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\u001b[0m\u001b[32;1m\u001b[1;3mThe recommended paper that is similar to \"Language Models Can Teach Themselves to Use Tools\" is titled \"Gorilla: Large Language Model Connected with Massive APIs.\" \n","\n","**Abstract:**\n","Large Language Models (LLMs) have made significant advancements, excelling in tasks like mathematical reasoning and program synthesis. However, their ability to effectively use tools through API calls remains a challenge. Gorilla, a finetuned LLaMA-based model, surpasses the performance of GPT-4 in writing API calls. By integrating with a document retriever, Gorilla adapts to test-time document changes, reducing hallucination issues. The model's evaluation includes APIBench, a dataset with HuggingFace, TorchHub, and TensorHub APIs. Gorilla demonstrates the potential for LLMs to use tools accurately, adapt to updated documentation, and enhance the reliability of their outputs.\n","\n","If you would like to explore more sections or details from this paper, feel free to let me know!\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["output = agent_executor.invoke({\"input\": \"ë…¼ë¬¸ 'Language Models Can Teach Themselves to Use Tools'ì™€ ë¹„ìŠ·í•œ í›„ì† ë…¼ë¬¸ì„ ì¶”ì²œí•´ì£¼ê³ , í•´ë‹¹ ë…¼ë¬¸ì˜ abstractê°€ ë‚´ê°€ ì¤€ ë…¼ë¬¸ê³¼ ì–´ë–¤ ì°¨ì´ê°€ ìžˆëŠ”ì§€ ì•Œë ¤ì¤˜\"})\n","# TODO : we do not need paperID in this tool."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHV5thnVwg-A","outputId":"ece5b6af-eadb-406f-fc51-df9518ec7bae"},"outputs":[{"name":"stdout","output_type":"stream","text":["('Here are some recommended papers related to \"Language Models Can Teach '\n"," 'Themselves to Use Tools\":\\n'\n"," '\\n'\n"," '1. **Gorilla: Large Language Model Connected with Massive APIs**\\n'\n"," '   - Abstract: Large Language Models (LLMs) have seen significant '\n"," 'advancements recently, excelling in various tasks. Gorilla, a finetuned '\n"," 'LLaMA-based model, surpasses the performance of GPT-4 in writing API calls. '\n"," 'It demonstrates a strong capability to adapt to test-time document changes '\n"," 'and mitigate hallucination issues.\\n'\n"," '   - Publication Date: May 24, 2023\\n'\n"," '\\n'\n"," '2. **Mind2Web: Towards a Generalist Agent for the Web**\\n'\n"," '   - Abstract: Introduces Mind2Web, a dataset for developing generalist '\n"," 'agents for the web. It provides diverse tasks from real-world websites and '\n"," 'explores using large language models for building generalist web agents.\\n'\n"," '   - Publication Date: June 9, 2023\\n'\n"," '\\n'\n"," '3. **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world '\n"," 'APIs**\\n'\n"," '   - Abstract: ToolLLM bridges the gap in tool-use capabilities of LLMs by '\n"," 'introducing a general tool-use framework. It fine-tunes LLaMA to execute '\n"," 'complex instructions and generalize to unseen APIs, demonstrating strong '\n"," 'zero-shot generalization ability.\\n'\n"," '   - Publication Date: July 31, 2023\\n'\n"," '\\n'\n"," '4. **Qwen Technical Report**\\n'\n"," '   - Abstract: Introduces Qwen, a comprehensive language model series with '\n"," 'distinct models showcasing superior performance across various tasks. The '\n"," 'chat models possess advanced tool-use and planning capabilities, even on '\n"," 'complex tasks like utilizing a code interpreter.\\n'\n"," '   - Publication Date: September 28, 2023\\n'\n"," '\\n'\n"," '5. **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging '\n"," 'Face**\\n'\n"," '   - Abstract: Presents HuggingGPT, an LLM-powered agent that connects '\n"," \"various AI models to solve complicated AI tasks. By leveraging ChatGPT's \"\n"," 'language capabilities and AI models in Hugging Face, HuggingGPT can tackle a '\n"," 'wide range of sophisticated tasks.\\n'\n"," '   - Publication Date: December 31, 9999\\n'\n"," '\\n'\n"," 'These papers offer valuable insights into the advancements and applications '\n"," 'of large language models in various domains.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5972,"status":"ok","timestamp":1719242855113,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"f1yIVnHKwg-B","outputId":"9f942f68-b88b-4baf-fdbd-4e64a9ef9571"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'Large Language Model Connected with Massive APIs'}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mGorilla: Large Language Model Connected with Massive APIs\n","          Abstract\n","  1 Introduction\n","  2 Related Work\n","        Large Language Models\n","        Tool Usage\n","        LLMs for Program Synthesis\n","  3 Methodology\n","    3.1 Dataset Collection\n","        API Documentation\n","        Instruction Generation\n","    3.2 Gorilla\n","        API Call with Constraints\n","        Retriever-Aware training\n","        Gorilla Inference\n","    3.3 Verifying APIs\n","        AST Sub-Tree Matching\n","  4 Evaluation\n","        Baselines\n","        Retrievers\n","    4.1 AST Accuracy on API call\n","        Finetuning without Retrieval\n","        Finetuning with Retrieval\n","        Hallucination with LLM\n","    4.2 Test-Time Documentation Change\n","    4.3 API Call with Constraints\n","  5 Conclusion\n","  6 Limitations & Social Impacts\n","  7 Acknowledgement\n","  References\n","  8 Appendix\n","    8.1 Dataset Details\n","        Domain Classification\n","        API Call Task\n","        API Provider Component\n","        Explanation Element\n","        Code\n","    8.2 Gorilla Details\n","        Data\n","        Training\n","    8.3 Performance Comparison\n","      8.3.1 Evaluation\n","      8.3.2 Hallucination\n","\u001b[0m\u001b[32;1m\u001b[1;3mThe abstract of the paper \"Large Language Model Connected with Massive APIs\" discusses the development of Gorilla, a large language model connected with massive APIs. The paper introduces the methodology used, including dataset collection, Gorilla model implementation, and API verification techniques. The evaluation section compares Gorilla with baselines and assesses its performance in various tasks such as AST accuracy on API calls, test-time documentation changes, and API calls with constraints. The paper concludes with insights on the limitations, social impacts, and acknowledgments.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# TODO : Change Arxiv Search enging to Semantic Scholar\n","# TODO : error handling when the tool called.\n","# Because below code doesn't work. with the difference of 'Gorilla: '\n","# output = agent_executor.invoke({\"input\": \"summary the abstract of the paper 'Gorilla: Large Language Model Connected with Massive APIs'\"})\n","\n","output = agent_executor.invoke({\"input\": \"summary the abstract of the paper 'Large Language Model Connected with Massive APIs'\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXyHypjmwg-C","outputId":"457f03a8-e5a2-4c16-fbd3-e1493f13a298"},"outputs":[{"name":"stdout","output_type":"stream","text":["('The paper titled \"Large Language Model Connected with Massive APIs\" '\n"," 'introduces Gorilla, a finetuned LLaMA-based model that surpasses the '\n"," 'performance of GPT-4 in writing API calls. Gorilla, when combined with a '\n"," 'document retriever, demonstrates the ability to adapt to test-time document '\n"," 'changes, reducing hallucination errors commonly encountered with LLMs. The '\n"," 'paper introduces APIBench, a dataset consisting of HuggingFace, TorchHub, '\n"," \"and TensorHub APIs, to evaluate the model's ability accurately. Gorilla \"\n"," 'significantly outperforms GPT-4 in terms of API functionality accuracy and '\n"," 'reduces hallucination errors. The paper emphasizes the importance of '\n"," 'empowering LLMs to use tools via API calls to access vast knowledge bases '\n"," 'and accomplish complex computational tasks effectively.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hX5aaMB6wg-D","outputId":"eab79a0c-23ce-4bf7-f2b1-00f46c25ea30"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `recommend_reference` with `{'query': 'Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[33;1m\u001b[1;3m[{'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0', 'title': 'Language Models are Few-Shot Learners', 'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\", 'influentialCitationCount': 3130, 'publicationDate': '2020-05-28', 'intent': 'methodology background result', 'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'}, {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb', 'title': 'PaLM: Scaling Language Modeling with Pathways', 'abstract': 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.', 'influentialCitationCount': 307, 'publicationDate': '2022-04-05', 'intent': 'background', 'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'}, {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221', 'title': 'OPT: Open Pre-trained Transformer Language Models', 'abstract': 'Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.', 'influentialCitationCount': 274, 'publicationDate': '2022-05-02', 'intent': 'result', 'context': 'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'}, {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', 'influentialCitationCount': 18170, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'background', 'context': 'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.'}, {'paperId': '9405cc0d6169988371b2755e573cc28650d14dfe', 'title': 'Language Models are Unsupervised Multitask Learners', 'abstract': 'Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.', 'influentialCitationCount': 2826, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'methodology', 'context': 'To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some recommended reference works related to the paper \"Language Models Can Teach Themselves to Use Tools\":\n","\n","1. **Paper Title:** Language Models are Few-Shot Learners\n","   - **Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. This paper shows that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\n","   - **Publication Date:** 2020-05-28\n","\n","2. **Paper Title:** PaLM: Scaling Language Modeling with Pathways\n","   - **Abstract:** This paper explores the impact of scale on few-shot learning by training a 540-billion parameter, densely activated, Transformer language model called Pathways Language Model PaLM. It achieves breakthrough performance on language understanding and generation benchmarks and demonstrates benefits of scaling in achieving state-of-the-art few-shot learning results.\n","   - **Publication Date:** 2022-04-05\n","\n","3. **Paper Title:** OPT: Open Pre-trained Transformer Language Models\n","   - **Abstract:** OPT presents a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, aiming to be fully and responsibly shared with interested researchers. The paper compares OPT-175B to GPT-3 and discusses the model's performance and carbon footprint.\n","   - **Publication Date:** 2022-05-02\n","\n","4. **Paper Title:** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n","   - **Abstract:** This paper introduces BERT, a language representation model designed for pre-training deep bidirectional representations from unlabeled text. BERT achieves state-of-the-art results on eleven natural language processing tasks and demonstrates the power of pre-training in creating models for various tasks.\n","   - **Publication Date:** Not specified\n","\n","5. **Paper Title:** Language Models are Unsupervised Multitask Learners\n","   - **Abstract:** This paper demonstrates that language models begin to learn natural language processing tasks without explicit supervision when trained on a new dataset of millions of webpages called WebText. The findings suggest a promising path towards building language processing systems that learn tasks from naturally occurring demonstrations.\n","   - **Publication Date:** Not specified\n","\n","These reference works provide valuable insights and background information related to language models and their capabilities in natural language processing tasks.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["output = agent_executor.invoke({\"input\": \"recommend the reference works of the paper 'Language Models Can Teach Themselves to Use Tools'\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEYY1iAWwg-D","outputId":"1260342a-d7d3-4486-94b3-8743f59f62d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["('Here are some recommended reference works related to the paper \"Language '\n"," 'Models Can Teach Themselves to Use Tools\":\\n'\n"," '\\n'\n"," '1. **Paper Title:** Language Models are Few-Shot Learners\\n'\n"," '   - **Abstract:** Recent work has demonstrated substantial gains on many '\n"," 'NLP tasks and benchmarks by pre-training on a large corpus of text followed '\n"," 'by fine-tuning on a specific task. This paper shows that scaling up language '\n"," 'models greatly improves task-agnostic, few-shot performance, sometimes even '\n"," 'reaching competitiveness with prior state-of-the-art fine-tuning '\n"," 'approaches.\\n'\n"," '   - **Publication Date:** 2020-05-28\\n'\n"," '\\n'\n"," '2. **Paper Title:** PaLM: Scaling Language Modeling with Pathways\\n'\n"," '   - **Abstract:** This paper explores the impact of scale on few-shot '\n"," 'learning by training a 540-billion parameter, densely activated, Transformer '\n"," 'language model called Pathways Language Model PaLM. It achieves breakthrough '\n"," 'performance on language understanding and generation benchmarks and '\n"," 'demonstrates benefits of scaling in achieving state-of-the-art few-shot '\n"," 'learning results.\\n'\n"," '   - **Publication Date:** 2022-04-05\\n'\n"," '\\n'\n"," '3. **Paper Title:** OPT: Open Pre-trained Transformer Language Models\\n'\n"," '   - **Abstract:** OPT presents a suite of decoder-only pre-trained '\n"," 'transformers ranging from 125M to 175B parameters, aiming to be fully and '\n"," 'responsibly shared with interested researchers. The paper compares OPT-175B '\n"," \"to GPT-3 and discusses the model's performance and carbon footprint.\\n\"\n"," '   - **Publication Date:** 2022-05-02\\n'\n"," '\\n'\n"," '4. **Paper Title:** BERT: Pre-training of Deep Bidirectional Transformers '\n"," 'for Language Understanding\\n'\n"," '   - **Abstract:** This paper introduces BERT, a language representation '\n"," 'model designed for pre-training deep bidirectional representations from '\n"," 'unlabeled text. BERT achieves state-of-the-art results on eleven natural '\n"," 'language processing tasks and demonstrates the power of pre-training in '\n"," 'creating models for various tasks.\\n'\n"," '   - **Publication Date:** Not specified\\n'\n"," '\\n'\n"," '5. **Paper Title:** Language Models are Unsupervised Multitask Learners\\n'\n"," '   - **Abstract:** This paper demonstrates that language models begin to '\n"," 'learn natural language processing tasks without explicit supervision when '\n"," 'trained on a new dataset of millions of webpages called WebText. The '\n"," 'findings suggest a promising path towards building language processing '\n"," 'systems that learn tasks from naturally occurring demonstrations.\\n'\n"," '   - **Publication Date:** Not specified\\n'\n"," '\\n'\n"," 'These reference works provide valuable insights and background information '\n"," 'related to language models and their capabilities in natural language '\n"," 'processing tasks.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"markdown","metadata":{},"source":["### Memory in Agent"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from langchain import hub\n","from langchain.agents import AgentExecutor, create_react_agent\n","from langchain.memory import ChatMessageHistory\n","\n","# prompt = hub.pull(\"hwchase17/react\")\n","prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n","memory = ChatMessageHistory(session_id=\"test-session\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["InMemoryChatMessageHistory(messages=[])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["memory"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from langchain_core.runnables.history import RunnableWithMessageHistory\n","from langchain_openai import OpenAI\n","\n","agent_with_chat_history = RunnableWithMessageHistory(\n","    agent_executor,\n","    # This is needed because in most real world scenarios, a session id is needed\n","    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n","    lambda session_id: memory,\n","    input_messages_key=\"input\",\n","    history_messages_key=\"chat_history\",\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'ToolFormer : Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n","title\n","Toolformer: Language Models Can Teach Themselves to Use Tools\n","sections\n","          Abstract\n","  1 Introduction\n","  2 Approach\n","        Sampling API Calls\n","        Executing API Calls\n","        Filtering API Calls\n","        Model Finetuning\n","        Inference\n","  3 Tools\n","        Question Answering\n","        Calculator\n","        Wikipedia Search\n","        Machine Translation System\n","        Calendar\n","  4 Experiments\n","    4.1 Experimental Setup\n","        Dataset Generation\n","        Model Finetuning\n","        Baseline Models\n","    4.2 Downstream Tasks\n","      4.2.1 LAMA\n","      4.2.2 Math Datasets\n","      4.2.3 Question Answering\n","      4.2.4 Multilingual Question Answering\n","      4.2.5 Temporal Datasets\n","    4.3 Language Modeling\n","    4.4 Scaling Laws\n","  5 Analysis\n","        Decoding Strategy\n","        Data Quality\n","  6 Related Work\n","        Language Model Pretraining\n","        Tool Use\n","        Bootstrapping\n","  7 Limitations\n","  8 Conclusion\n","  References\n","  Appendix A API Details\n","    A.1 Implementation\n","        Question Answering\n","        Calculator\n","        Calendar\n","        Machine Translation\n","    A.2 Prompts\n","        Question Answering\n","        Calculator\n","        Wikipedia Search\n","        Machine Translation\n","        Calendar\n","  Appendix B Toolformer Training\n","  Appendix C Zero-Shot Prompts\n","    C.1 LAMA and TempLAMA\n","    C.2 Math Benchmarks\n","    C.3 Question Answering\n","    C.4 Multilingual Question Answering\n","  Appendix D Dateset\n","\n","\n"," Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'ToolFormer : Language Models Can Teach Themselves to Use Tools', 'sections': ['Filtering API Calls']}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mFiltering API Calls\n","Let ið‘–i be the position of the API call cisubscriptð‘ð‘–c_{i} in the sequence ð±=x1,â€¦,xnð±subscriptð‘¥1â€¦subscriptð‘¥ð‘›\\mathbf{x}=x_{1},\\ldots,x_{n}, and let risubscriptð‘Ÿð‘–r_{i} be the response from the API. Further, given a sequence (wiâˆ£iâˆˆâ„•)conditionalsubscriptð‘¤ð‘–ð‘–â„•(w_{i}\\mid i\\in\\mathbb{N}) of weights, let\n","\n","Liâ€‹(ð³)=âˆ’âˆ‘j=inwjâˆ’iâ‹…logâ¡pMâ€‹(xjâˆ£ð³,x1:jâˆ’1)subscriptð¿ð‘–ð³superscriptsubscriptð‘—ð‘–ð‘›â‹…subscriptð‘¤ð‘—ð‘–subscriptð‘ð‘€conditionalsubscriptð‘¥ð‘—ð³subscriptð‘¥:1ð‘—1L_{i}(\\mathbf{z})=-\\sum_{j=i}^{n}w_{j-i}\\cdot\\log{p_{M}(x_{j}\\mid\\mathbf{z},x_{1:j-1})}\n","\n","be the weighted cross entropy loss for Mð‘€M over the tokens xi,â€¦,xnsubscriptð‘¥ð‘–â€¦subscriptð‘¥ð‘›x_{i},\\ldots,x_{n} if the model is prefixed with ð³ð³\\mathbf{z}.\n","We compare two different instantiations of this loss:\n","\n","Li+superscriptsubscriptð¿ð‘–\\displaystyle L_{i}^{+}\n","=Liâ€‹(eâ€‹(ci,ri))absentsubscriptð¿ð‘–esubscriptð‘ð‘–subscriptð‘Ÿð‘–\\displaystyle=L_{i}(\\text{e}(c_{i},r_{i}))\n","\n","Liâˆ’superscriptsubscriptð¿ð‘–\\displaystyle L_{i}^{-}\n","=minâ¡(Liâ€‹(Îµ),Liâ€‹(eâ€‹(ci,Îµ)))absentsubscriptð¿ð‘–ðœ€subscriptð¿ð‘–esubscriptð‘ð‘–ðœ€\\displaystyle=\\min\\left(L_{i}(\\varepsilon),L_{i}(\\text{e}(c_{i},\\varepsilon))\\right)\n","\n","where Îµðœ€\\varepsilon denotes an empty sequence. The former is the weighted loss over all tokens xi,â€¦,xnsubscriptð‘¥ð‘–â€¦subscriptð‘¥ð‘›x_{i},\\ldots,x_{n} if the API call and its result are given to Mð‘€M as a prefix;333We provide eâ€‹(ci,ri)esubscriptð‘ð‘–subscriptð‘Ÿð‘–\\text{e}(c_{i},r_{i}) as a prefix instead of inserting it at position ið‘–i because Mð‘€M is not yet finetuned on any examples containing API calls, so inserting it in the middle of ð±ð±\\mathbf{x} would interrupt the flow and not align with patterns in the pretraining corpus, thus hurting perplexity. the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to Mð‘€M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a filtering threshold Ï„fsubscriptðœð‘“\\tau_{f}, we thus only keep API calls for which\n","\n","Liâˆ’âˆ’Li+â‰¥Ï„fsuperscriptsubscriptð¿ð‘–superscriptsubscriptð¿ð‘–subscriptðœð‘“L_{i}^{-}-L_{i}^{+}\\geq\\tau_{f}\n","\n","holds, i.e., adding the API call and its result reduces the loss by at least Ï„fsubscriptðœð‘“\\tau_{f}, compared to not doing any API call or obtaining no result from it.\n","\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"ToolFormer: Language Models Can Teach Themselves to Use Tools\" discusses the filtering of API calls in the context of language models. Here is an explanation of the math expression related to filtering API calls as presented in the paper:\n","\n","The paper introduces a weighted cross-entropy loss function for a model M over a sequence of tokens xi,...,xn, where i represents the position of the API call ci in the sequence and ri represents the response from the API. The loss function is defined as:\n","\n","Li(z) = -âˆ‘j=i^n wj-i * log pM(xj | z, x1:j-1)\n","\n","In this expression:\n","- z represents the prefix for the model M.\n","- wj-i is a sequence of weights.\n","- pM(xj | z, x1:j-1) is the conditional probability of token xj given the prefix z and the tokens x1 to xj-1.\n","\n","The paper compares two variations of this loss function:\n","1. Li+ = Li(e(ci, ri)): This represents the weighted loss over all tokens if the model is given the API call and its result as a prefix.\n","2. Li- = min(Li(Îµ), Li(e(ci, Îµ)): This represents the minimum loss obtained when either no API call is made or when an API call is made without providing the response.\n","\n","The paper introduces a filtering threshold Ï„f, where API calls are kept only if the difference between Li- and Li+ is greater than or equal to Ï„f. In other words, an API call is considered helpful if providing both the input and output of the call reduces the loss by at least Ï„f compared to not making the call or not receiving the response.\n","\n","This approach aims to determine the relevance and impact of API calls on the model's prediction accuracy, ensuring that only beneficial API calls are incorporated into the model's learning process.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"data":{"text/plain":["{'input': \"explain about the math expression about the Filtering api calls in the paper 'ToolFormer : Language Models Can Teach Themselves to Use Tools'\",\n"," 'chat_history': [],\n"," 'output': 'The paper \"ToolFormer: Language Models Can Teach Themselves to Use Tools\" discusses the filtering of API calls in the context of language models. Here is an explanation of the math expression related to filtering API calls as presented in the paper:\\n\\nThe paper introduces a weighted cross-entropy loss function for a model M over a sequence of tokens xi,...,xn, where i represents the position of the API call ci in the sequence and ri represents the response from the API. The loss function is defined as:\\n\\nLi(z) = -âˆ‘j=i^n wj-i * log pM(xj | z, x1:j-1)\\n\\nIn this expression:\\n- z represents the prefix for the model M.\\n- wj-i is a sequence of weights.\\n- pM(xj | z, x1:j-1) is the conditional probability of token xj given the prefix z and the tokens x1 to xj-1.\\n\\nThe paper compares two variations of this loss function:\\n1. Li+ = Li(e(ci, ri)): This represents the weighted loss over all tokens if the model is given the API call and its result as a prefix.\\n2. Li- = min(Li(Îµ), Li(e(ci, Îµ)): This represents the minimum loss obtained when either no API call is made or when an API call is made without providing the response.\\n\\nThe paper introduces a filtering threshold Ï„f, where API calls are kept only if the difference between Li- and Li+ is greater than or equal to Ï„f. In other words, an API call is considered helpful if providing both the input and output of the call reduces the loss by at least Ï„f compared to not making the call or not receiving the response.\\n\\nThis approach aims to determine the relevance and impact of API calls on the model\\'s prediction accuracy, ensuring that only beneficial API calls are incorporated into the model\\'s learning process.'}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["agent_with_chat_history.invoke(\n","    {\"input\": \"explain about the math expression about the Filtering api calls in the paper 'ToolFormer : Language Models Can Teach Themselves to Use Tools'\"},\n","    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'ToolFormer : Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n","title\n","Toolformer: Language Models Can Teach Themselves to Use Tools\n","sections\n","          Abstract\n","  1 Introduction\n","  2 Approach\n","        Sampling API Calls\n","        Executing API Calls\n","        Filtering API Calls\n","        Model Finetuning\n","        Inference\n","  3 Tools\n","        Question Answering\n","        Calculator\n","        Wikipedia Search\n","        Machine Translation System\n","        Calendar\n","  4 Experiments\n","    4.1 Experimental Setup\n","        Dataset Generation\n","        Model Finetuning\n","        Baseline Models\n","    4.2 Downstream Tasks\n","      4.2.1 LAMA\n","      4.2.2 Math Datasets\n","      4.2.3 Question Answering\n","      4.2.4 Multilingual Question Answering\n","      4.2.5 Temporal Datasets\n","    4.3 Language Modeling\n","    4.4 Scaling Laws\n","  5 Analysis\n","        Decoding Strategy\n","        Data Quality\n","  6 Related Work\n","        Language Model Pretraining\n","        Tool Use\n","        Bootstrapping\n","  7 Limitations\n","  8 Conclusion\n","  References\n","  Appendix A API Details\n","    A.1 Implementation\n","        Question Answering\n","        Calculator\n","        Calendar\n","        Machine Translation\n","    A.2 Prompts\n","        Question Answering\n","        Calculator\n","        Wikipedia Search\n","        Machine Translation\n","        Calendar\n","  Appendix B Toolformer Training\n","  Appendix C Zero-Shot Prompts\n","    C.1 LAMA and TempLAMA\n","    C.2 Math Benchmarks\n","    C.3 Question Answering\n","    C.4 Multilingual Question Answering\n","  Appendix D Dateset\n","\n","\n"," Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'ToolFormer : Language Models Can Teach Themselves to Use Tools', 'sections': ['2 Approach']}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3m\n","2 Approach\n","Our aim is to equip a language model Mð‘€M with the ability to use different tools by means of API calls. We require that inputs and outputs for each API can be represented as text sequences.\n","This allows seamless insertion of API calls into any given text, using special tokens to mark the start and end of each such call.\n","\n","We represent each API call as a tuple c=(ac,ic)ð‘subscriptð‘Žð‘subscriptð‘–ð‘c=({a}_{c},{i}_{c}) where acsubscriptð‘Žð‘a_{c} is the name of the API and icsubscriptð‘–ð‘i_{c} is the corresponding input. Given an API call cð‘c with a corresponding result rð‘Ÿr, we denote the linearized sequences of the API call not including and including its result, respectively, as:\n","\n","eâ€‹(c)eð‘\\displaystyle\\text{e}(c)\n","=<API>â€‹acâ€‹(â€‹icâ€‹)â€‹</API>absent<API>subscriptð‘Žð‘(subscriptð‘–ð‘)</API>\\displaystyle=\\texttt{<API>}\\,a_{c}\\texttt{(}i_{c}\\texttt{)}\\,\\texttt{</API>}\n","\n","eâ€‹(c,r)eð‘ð‘Ÿ\\displaystyle\\text{e}(c,r)\n","=<API>â€‹acâ€‹(â€‹icâ€‹)â†’râ€‹</API>absent<API>subscriptð‘Žð‘(subscriptð‘–ð‘)â†’ð‘Ÿ</API>\\displaystyle=\\texttt{<API>}\\,a_{c}\\texttt{(}i_{c}\\texttt{)}\\rightarrow r\\,\\texttt{</API>}\n","\n","where â€œ<API>â€, â€œ</API>â€ and â€œâ†’â†’\\rightarrowâ€ are special tokens.111In practice, we use the token sequences â€œ [â€, â€œ]â€ and â€œ->â€ to represent â€œ<API>â€, â€œ</API>â€ and â€œâ†’â†’\\rightarrowâ€, respectively. This enables our approach to work without modifying the existing LMâ€™s vocabulary. For reasons of readability, we still refer to them as â€œ<API>â€, â€œ</API>â€ and â€œâ†’â†’\\rightarrowâ€ throughout this section. Some examples of linearized API calls inserted into text sequences are shown in FigureÂ 1.\n","\n","Given a dataset ð’ž={ð±1,â€¦,ð±|ð’ž|}ð’žsuperscriptð±1â€¦superscriptð±ð’ž\\mathcal{C}=\\{\\mathbf{x}^{1},\\ldots,\\mathbf{x}^{|\\mathcal{C}|}\\} of plain texts, we first convert this dataset into a dataset ð’žâˆ—superscriptð’ž\\mathcal{C}^{*} augmented with API calls. This is done in three steps, illustrated in FigureÂ 2: First, we exploit the in-context learning ability of Mð‘€M to sample a large number of potential API calls. We then execute these API calls and finally check whether the obtained responses are helpful for predicting future tokens; this is used as a filtering criterion. After filtering, we merge API calls for different tools, resulting in the augmented dataset ð’žâˆ—superscriptð’ž\\mathcal{C}^{*}, and finetune Mð‘€M itself on this dataset. Each of these steps is described in more detail below.\n","\n","Sampling API Calls\n","\n","For each API, we write a prompt Pâ€‹(ð±)ð‘ƒð±P(\\mathbf{x}) that encourages the LM to annotate an example ð±=x1,â€¦,xnð±subscriptð‘¥1â€¦subscriptð‘¥ð‘›\\mathbf{x}=x_{1},\\ldots,x_{n} with API calls. An example of such a prompt for a question answering tool is shown in FigureÂ 3; all prompts used are shown in AppendixÂ A.2. Let pMâ€‹(zn+1âˆ£z1,â€¦,zn)subscriptð‘ð‘€conditionalsubscriptð‘§ð‘›1subscriptð‘§1â€¦subscriptð‘§ð‘›p_{M}(z_{n+1}\\mid z_{1},\\ldots,z_{n}) be the probability that Mð‘€M assigns to token zn+1subscriptð‘§ð‘›1z_{n+1} as a continuation for the sequence z1,â€¦,znsubscriptð‘§1â€¦subscriptð‘§ð‘›z_{1},\\ldots,z_{n}. We first sample up to kð‘˜k candidate positions for doing API calls by computing, for each iâˆˆ{1,â€¦,n}ð‘–1â€¦ð‘›i\\in\\{1,\\ldots,n\\}, the probability\n","\n","pi=pMâ€‹(<API>âˆ£Pâ€‹(ð±),x1:iâˆ’1)subscriptð‘ð‘–subscriptð‘ð‘€conditional<API>ð‘ƒð±subscriptð‘¥:1ð‘–1p_{i}=p_{M}(\\texttt{<API>}\\mid P(\\mathbf{x}),x_{1:i-1})\n","\n","that Mð‘€M assigns to starting an API call at position ið‘–i.\n","Given a sampling threshold Ï„ssubscriptðœð‘ \\tau_{s}, we keep all positions I={iâˆ£pi>Ï„s}ð¼conditional-setð‘–subscriptð‘ð‘–subscriptðœð‘ I=\\{i\\mid p_{i}>\\tau_{s}\\}; if there are more than kð‘˜k such positions, we only keep the top kð‘˜k.\n","\n","For each position iâˆˆIð‘–ð¼i\\in I, we then obtain up to mð‘šm API calls ci1,â€¦,cimsuperscriptsubscriptð‘ð‘–1â€¦superscriptsubscriptð‘ð‘–ð‘šc_{i}^{1},\\ldots,c_{i}^{m} by sampling from Mð‘€M given the sequence [Pâ€‹(ð±),x1,â€¦,xiâˆ’1,<API>]ð‘ƒð±subscriptð‘¥1â€¦subscriptð‘¥ð‘–1<API>[P(\\mathbf{x}),x_{1},\\ldots,x_{i-1},\\texttt{<API>}] as a prefix and </API> as an end-of-sequence token.222We discard all examples where Mð‘€M does not generate the </API> token.\n","\n","Figure 3: An exemplary prompt Pâ€‹(ð±)ð‘ƒð±P(\\mathbf{x}) used to generate API calls for the question answering tool.\n","\n","Executing API Calls\n","\n","As a next step, we execute all API calls generated by Mð‘€M to obtain the corresponding results. How this is done depends entirely on the API itself â€“ for example, it can involve calling another neural network, executing a Python script or using a retrieval system to perform search over a large corpus. The response for each API call cisubscriptð‘ð‘–c_{i} needs to be a single text sequence risubscriptð‘Ÿð‘–r_{i}.\n","\n","Filtering API Calls\n","\n","Let ið‘–i be the position of the API call cisubscriptð‘ð‘–c_{i} in the sequence ð±=x1,â€¦,xnð±subscriptð‘¥1â€¦subscriptð‘¥ð‘›\\mathbf{x}=x_{1},\\ldots,x_{n}, and let risubscriptð‘Ÿð‘–r_{i} be the response from the API. Further, given a sequence (wiâˆ£iâˆˆâ„•)conditionalsubscriptð‘¤ð‘–ð‘–â„•(w_{i}\\mid i\\in\\mathbb{N}) of weights, let\n","\n","Liâ€‹(ð³)=âˆ’âˆ‘j=inwjâˆ’iâ‹…logâ¡pMâ€‹(xjâˆ£ð³,x1:jâˆ’1)subscriptð¿ð‘–ð³superscriptsubscriptð‘—ð‘–ð‘›â‹…subscriptð‘¤ð‘—ð‘–subscriptð‘ð‘€conditionalsubscriptð‘¥ð‘—ð³subscriptð‘¥:1ð‘—1L_{i}(\\mathbf{z})=-\\sum_{j=i}^{n}w_{j-i}\\cdot\\log{p_{M}(x_{j}\\mid\\mathbf{z},x_{1:j-1})}\n","\n","be the weighted cross entropy loss for Mð‘€M over the tokens xi,â€¦,xnsubscriptð‘¥ð‘–â€¦subscriptð‘¥ð‘›x_{i},\\ldots,x_{n} if the model is prefixed with ð³ð³\\mathbf{z}.\n","We compare two different instantiations of this loss:\n","\n","Li+superscriptsubscriptð¿ð‘–\\displaystyle L_{i}^{+}\n","=Liâ€‹(eâ€‹(ci,ri))absentsubscriptð¿ð‘–esubscriptð‘ð‘–subscriptð‘Ÿð‘–\\displaystyle=L_{i}(\\text{e}(c_{i},r_{i}))\n","\n","Liâˆ’superscriptsubscriptð¿ð‘–\\displaystyle L_{i}^{-}\n","=minâ¡(Liâ€‹(Îµ),Liâ€‹(eâ€‹(ci,Îµ)))absentsubscriptð¿ð‘–ðœ€subscriptð¿ð‘–esubscriptð‘ð‘–ðœ€\\displaystyle=\\min\\left(L_{i}(\\varepsilon),L_{i}(\\text{e}(c_{i},\\varepsilon))\\right)\n","\n","where Îµðœ€\\varepsilon denotes an empty sequence. The former is the weighted loss over all tokens xi,â€¦,xnsubscriptð‘¥ð‘–â€¦subscriptð‘¥ð‘›x_{i},\\ldots,x_{n} if the API call and its result are given to Mð‘€M as a prefix;333We provide eâ€‹(ci,ri)esubscriptð‘ð‘–subscriptð‘Ÿð‘–\\text{e}(c_{i},r_{i}) as a prefix instead of inserting it at position ið‘–i because Mð‘€M is not yet finetuned on any examples containing API calls, so inserting it in the middle of ð±ð±\\mathbf{x} would interrupt the flow and not align with patterns in the pretraining corpus, thus hurting perplexity. the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to Mð‘€M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a filtering threshold Ï„fsubscriptðœð‘“\\tau_{f}, we thus only keep API calls for which\n","\n","Liâˆ’âˆ’Li+â‰¥Ï„fsuperscriptsubscriptð¿ð‘–superscriptsubscriptð¿ð‘–subscriptðœð‘“L_{i}^{-}-L_{i}^{+}\\geq\\tau_{f}\n","\n","holds, i.e., adding the API call and its result reduces the loss by at least Ï„fsubscriptðœð‘“\\tau_{f}, compared to not doing any API call or obtaining no result from it.\n","\n","Model Finetuning\n","\n","After sampling and filtering calls for all APIs, we finally merge the remaining API calls and interleave them with the original inputs. That is, for an input text ð±=x1,â€¦,xnð±subscriptð‘¥1â€¦subscriptð‘¥ð‘›\\mathbf{x}=x_{1},\\ldots,x_{n} with a corresponding API call and result (ci,ri)subscriptð‘ð‘–subscriptð‘Ÿð‘–(c_{i},r_{i}) at position ið‘–i, we construct the new sequence ð±âˆ—=x1:iâˆ’1,eâ€‹(ci,ri),xi:nsuperscriptð±subscriptð‘¥:1ð‘–1esubscriptð‘ð‘–subscriptð‘Ÿð‘–subscriptð‘¥:ð‘–ð‘›\\mathbf{x}^{*}=x_{1:{i-1}},\\text{e}(c_{i},r_{i}),x_{i:n}; we proceed analogously for texts with multiple API calls. Doing this for all ð±âˆˆð’žð±ð’ž\\mathbf{x}\\in\\mathcal{C} results in the new dataset ð’žâˆ—superscriptð’ž\\mathcal{C}^{*} augmented with API calls. We use this new dataset to finetune Mð‘€M, using a standard language modeling objective. Crucially, apart from inserted API calls the augmented dataset ð’žâˆ—superscriptð’ž\\mathcal{C}^{*} contains the exact same texts as ð’žð’ž\\mathcal{C}, the original dataset. As a consequence, finetuning Mð‘€M on ð’žâˆ—superscriptð’ž\\mathcal{C}^{*} exposes it to the same content as finetuning on ð’žð’ž\\mathcal{C}. Moreover, as API calls are inserted in exactly those positions and with exactly those inputs that help Mð‘€M predict future tokens, finetuning on ð’žâˆ—superscriptð’ž\\mathcal{C}^{*} enables the language model to decide when and how to use which tool, based purely on its own feedback.\n","\n","Inference\n","\n","When generating text with Mð‘€M after finetuning with our approach, we perform regular decoding until Mð‘€M produces the â€œâ†’â†’\\rightarrowâ€ token, indicating that it next expects the response for an API call. At this point, we interrupt the decoding process, call the appropriate API to get a response, and continue the decoding process after inserting both the response and the </API> token.\n","\n","\u001b[0m\u001b[32;1m\u001b[1;3mThe method described in the paper \"ToolFormer: Language Models Can Teach Themselves to Use Tools\" focuses on equipping a language model (M) with the ability to utilize different tools through API calls. Here is a summary of the approach outlined in the paper:\n","\n","1. **Representation of API Calls**: Each API call is represented as a tuple (ac, ic), where ac is the API name and ic is the corresponding input. The API call without the result is denoted as e(c) and with the result as e(c, r), using special tokens to mark the start and end of each call.\n","\n","2. **Dataset Augmentation**: The dataset of plain texts is converted into an augmented dataset with API calls in three steps: sampling potential API calls using prompts, executing the API calls to obtain results, and filtering the API calls based on their impact on predicting future tokens.\n","\n","3. **Sampling API Calls**: The language model is prompted to annotate examples with API calls by sampling candidate positions for API calls based on the model's probabilities. API calls are generated by sampling from the model given specific prefixes and end-of-sequence tokens.\n","\n","4. **Executing API Calls**: The generated API calls are executed to obtain the corresponding results. The method of execution depends on the specific API, such as calling another neural network or using a retrieval system.\n","\n","5. **Filtering API Calls**: The paper introduces a weighted cross-entropy loss function to compare the impact of API calls on the model's prediction accuracy. API calls are filtered based on whether providing the input and output of the call reduces the loss by a certain threshold compared to not making the call or not receiving the response.\n","\n","6. **Model Finetuning**: After sampling, executing, and filtering API calls, the remaining calls are merged with the original inputs to create an augmented dataset. The model is finetuned on this dataset using a standard language modeling objective, enabling it to learn how and when to use different tools based on its own feedback.\n","\n","7. **Inference**: During text generation after finetuning, the model is interrupted when it expects an API call response, the appropriate API is called to get the response, and decoding continues after inserting the response and end-of-sequence token.\n","\n","This method allows the language model to learn to use tools through API calls and improve its performance in various tasks by leveraging external resources and information.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"data":{"text/plain":["{'input': 'explain about the Method',\n"," 'chat_history': [HumanMessage(content=\"explain about the math expression about the Filtering api calls in the paper 'ToolFormer : Language Models Can Teach Themselves to Use Tools'\"),\n","  AIMessage(content='The paper \"ToolFormer: Language Models Can Teach Themselves to Use Tools\" discusses the filtering of API calls in the context of language models. Here is an explanation of the math expression related to filtering API calls as presented in the paper:\\n\\nThe paper introduces a weighted cross-entropy loss function for a model M over a sequence of tokens xi,...,xn, where i represents the position of the API call ci in the sequence and ri represents the response from the API. The loss function is defined as:\\n\\nLi(z) = -âˆ‘j=i^n wj-i * log pM(xj | z, x1:j-1)\\n\\nIn this expression:\\n- z represents the prefix for the model M.\\n- wj-i is a sequence of weights.\\n- pM(xj | z, x1:j-1) is the conditional probability of token xj given the prefix z and the tokens x1 to xj-1.\\n\\nThe paper compares two variations of this loss function:\\n1. Li+ = Li(e(ci, ri)): This represents the weighted loss over all tokens if the model is given the API call and its result as a prefix.\\n2. Li- = min(Li(Îµ), Li(e(ci, Îµ)): This represents the minimum loss obtained when either no API call is made or when an API call is made without providing the response.\\n\\nThe paper introduces a filtering threshold Ï„f, where API calls are kept only if the difference between Li- and Li+ is greater than or equal to Ï„f. In other words, an API call is considered helpful if providing both the input and output of the call reduces the loss by at least Ï„f compared to not making the call or not receiving the response.\\n\\nThis approach aims to determine the relevance and impact of API calls on the model\\'s prediction accuracy, ensuring that only beneficial API calls are incorporated into the model\\'s learning process.'),\n","  HumanMessage(content='explain about the Dataset Generation Part'),\n","  AIMessage(content='The dataset generation process in the paper \"ToolFormer: Language Models Can Teach Themselves to Use Tools\" involves the following key steps and considerations:\\n\\n1. **Dataset Selection**: The paper uses a subset of CCNet as the language modeling dataset ð’ž and GPT-J as the language model M. This choice of datasets provides the foundation for training and evaluating the language model\\'s performance.\\n\\n2. **Heuristics for API Calls**: To reduce the computational cost of annotating the dataset with API calls, the paper defines heuristics for certain APIs. These heuristics help identify a subset of the dataset where API calls are more likely to be beneficial. For example, texts related to the calculator tool are considered only if they contain at least three numbers.\\n\\n3. **Dataset Filtering**: After defining the heuristics and selecting the subset of the dataset, the paper performs filtering steps to eliminate examples where all API calls are deemed unnecessary. This filtering process ensures that only relevant examples with potentially helpful API calls are retained for further analysis.\\n\\n4. **Weighting Function**: The paper introduces a weighting function to determine the importance of API calls in the dataset. The weighting function ensures that API calls occur close to where the information provided by the API is most beneficial for the model\\'s learning process.\\n\\n5. **Threshold Selection**: Individual thresholds Ï„s and Ï„f are chosen for each tool to ensure a sufficient number of examples for training. These thresholds are crucial in determining the relevance and impact of API calls on the dataset.\\n\\n6. **Dataset Statistics**: The paper provides relevant statistics of the final dataset augmented with API calls, showcasing the number of examples for different tools and filtering thresholds. The table presented in the paper displays the number of examples with API calls for various filtering threshold values.\\n\\nOverall, the dataset generation process in the paper involves careful selection, heuristic definition, filtering, weighting, and threshold setting to create a dataset that facilitates the training and evaluation of language models with integrated API calls.')],\n"," 'output': 'The method described in the paper \"ToolFormer: Language Models Can Teach Themselves to Use Tools\" focuses on equipping a language model (M) with the ability to utilize different tools through API calls. Here is a summary of the approach outlined in the paper:\\n\\n1. **Representation of API Calls**: Each API call is represented as a tuple (ac, ic), where ac is the API name and ic is the corresponding input. The API call without the result is denoted as e(c) and with the result as e(c, r), using special tokens to mark the start and end of each call.\\n\\n2. **Dataset Augmentation**: The dataset of plain texts is converted into an augmented dataset with API calls in three steps: sampling potential API calls using prompts, executing the API calls to obtain results, and filtering the API calls based on their impact on predicting future tokens.\\n\\n3. **Sampling API Calls**: The language model is prompted to annotate examples with API calls by sampling candidate positions for API calls based on the model\\'s probabilities. API calls are generated by sampling from the model given specific prefixes and end-of-sequence tokens.\\n\\n4. **Executing API Calls**: The generated API calls are executed to obtain the corresponding results. The method of execution depends on the specific API, such as calling another neural network or using a retrieval system.\\n\\n5. **Filtering API Calls**: The paper introduces a weighted cross-entropy loss function to compare the impact of API calls on the model\\'s prediction accuracy. API calls are filtered based on whether providing the input and output of the call reduces the loss by a certain threshold compared to not making the call or not receiving the response.\\n\\n6. **Model Finetuning**: After sampling, executing, and filtering API calls, the remaining calls are merged with the original inputs to create an augmented dataset. The model is finetuned on this dataset using a standard language modeling objective, enabling it to learn how and when to use different tools based on its own feedback.\\n\\n7. **Inference**: During text generation after finetuning, the model is interrupted when it expects an API call response, the appropriate API is called to get the response, and decoding continues after inserting the response and end-of-sequence token.\\n\\nThis method allows the language model to learn to use tools through API calls and improve its performance in various tasks by leveraging external resources and information.'}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["agent_with_chat_history.invoke(\n","    {\"input\": \"explain about the Method\"},\n","    config={\"configurable\": {\"session_id\": \"<test_session_id>\"}},\n",")"]},{"cell_type":"markdown","metadata":{"id":"bbxP_Psuwg-E"},"source":["### Todo : Make Agent with Llama"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wULDS5ozwg-F"},"outputs":[],"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AvoAOTmwg-F","outputId":"fc02781b-fbe0-47da-c521-c555fcc589bf"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:02,  1.20it/s]"]}],"source":["# we can use 262k version of Llama!\n","model_name = 'gradientai/Llama-3-8B-Instruct-262k'\n","\n","dtype = \"float16\"\n","if torch.cuda.is_bf16_supported():\n","    dtype = \"bfloat16\"\n","\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=getattr(torch, dtype), quantization_config=quantization_config,\n","                                             device_map=\"auto\",  cache_dir=os.getenv(\"HF_HOME\", \"~/.cache/huggingface\"))\n","model.eval()\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmpO15ehwg-G"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"zrfI9u3YHYxw"},"source":["### (deprecated) HTML "]},{"cell_type":"markdown","metadata":{},"source":["is moved to getpaper.py\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":365,"status":"ok","timestamp":1719243752174,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"wj5T-OOHHu32"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","from requests.exceptions import RequestException\n","import PyPDF2\n","import re\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1719243801997,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"DjVjHTaxSIL4"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","from requests.exceptions import RequestException\n","import PyPDF2\n","import re\n","import os\n","\n","class GetPaper:\n","    def __init__(self, ss_api_key, ar5iv_mode = True, path_db='./papers_db', page_limit = 5):\n","        self.ss_api_key = ss_api_key\n","        self.ar5iv_mode = ar5iv_mode\n","        self.path_db= path_db\n","        self.page_limit = page_limit\n","\n","    def get_paper_info_by_title(self, title):\n","        \"\"\"ë…¼ë¬¸ì˜ ì œëª©ìœ¼ë¡œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\"\"\"\n","        # Define the API endpoint URL\n","        url = 'https://api.semanticscholar.org/graph/v1/paper/search?query={}&fields=paperId,title,abstract,authors,citations,fieldsOfStudy,influentialCitationCount,isOpenAccess,openAccessPdf,publicationDate,publicationTypes,references,venue'\n","\n","        headers = {'x-api-key': self.ss_api_key}\n","        response = requests.get(url.format(title), headers=headers).json()\n","\n","        if response.get('data'):\n","            paper = response['data'][0]\n","            return paper\n","        else:\n","            return None\n","\n","    def get_ar5iv_url(self, paper):\n","        \"ë…¼ë¬¸ì˜ ar5iv ì£¼ì†Œë¥¼ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜\"\n","        external_ids = paper.get('openAccessPdf', {})\n","        arxiv_id = external_ids.get('url')\n","        if 'http' in arxiv_id:\n","            arxiv_id = arxiv_id.split('/')[-1]\n","            return f\"https://ar5iv.org/abs/{arxiv_id}\"\n","        else:\n","            return None\n","\n","    def get_soup_from_url(self, url):\n","        try:\n","            response = requests.get(url, timeout=10)\n","            response.raise_for_status()  # HTTP ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚´\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            return soup\n","        except RequestException as e:\n","            print(f\"Error fetching the URL: {e}\")\n","            return None\n","        except Exception as e:\n","            print(f\"An unexpected error occurred: {e}\")\n","            return None\n","\n","    def get_header_from_soup(self, soup):\n","        # h1ë¶€í„° h6ê¹Œì§€ íƒœê·¸ ì¶”ì¶œ\n","        headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n","\n","        # íƒœê·¸ì™€ ë‚´ìš©ì„ ê³„ì¸µ êµ¬ì¡°ë¡œ ì €ìž¥\n","        header_list = [(header.name, header.text.strip()) for header in headers]\n","        title = header_list[0][1]\n","        header_list = header_list[1:]\n","        return title, header_list\n","\n","\n","    def extract_text_under_headers(self, soup, text_list):\n","        # ê²°ê³¼ë¥¼ ì €ìž¥í•  ë³€ìˆ˜\n","        results = []\n","\n","        # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ê° í…ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” í—¤ë”ì™€ ê·¸ ì•„ëž˜ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ\n","        for text in text_list:\n","            header_tag = soup.find(lambda tag: tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6'] and text in tag.get_text())\n","            if header_tag:\n","                header_text = header_tag.get_text(strip=False)\n","                header_level = int(header_tag.name[1])\n","                current_header = {'tag': header_tag.name, 'text': header_text, 'subsections': []}\n","                results.append(current_header)\n","\n","                next_element = header_tag.find_next_sibling()\n","                while next_element:\n","                    if next_element.name and next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n","                        next_level = int(next_element.name[1])\n","                        if next_level <= header_level:\n","                            break\n","\n","                        # If it's a tag and within our header range\n","                        if next_element.name and next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n","                            next_text = next_element.get_text(strip=False)\n","                            next_subheader = {'tag': next_element.name, 'text': next_text, 'subsections': []}\n","                            current_header['subsections'].append(next_subheader)\n","                            current_header = next_subheader\n","                            header_level = next_level\n","                    else:\n","                        if 'subsections' not in current_header:\n","                            current_header['subsections'] = []\n","                        current_header['subsections'].append({'tag': 'p', 'text': next_element.get_text(strip=False)})\n","\n","                    next_element = next_element.find_next_sibling()\n","\n","        content = ''\n","        for x in results:\n","            content += x['text']\n","            for y in x['subsections']:\n","                content += y['text']\n","        content = re.sub(r'\\n{3,}', '\\n\\n', content) # 3ë²ˆ ì´ìƒ \\n ì´ ì—°ì†ë˜ë©´ 2ë²ˆìœ¼ë¡œ ì¤„ì´ê¸°\n","        return content\n","\n","    def list_section(self, header_list):\n","        section_list = ''\n","        for tag, text in header_list:\n","            level = int(tag[1])  # íƒœê·¸ì—ì„œ ë ˆë²¨ì„ ì¶”ì¶œ (h1 -> 1, h2 -> 2, ..)\n","            section_list += '  ' * (level - 1) + text +'\\n'\n","        return section_list\n","\n","\n","    def download_pdf(self, arxiv_id):\n","        \"\"\"\n","        Download the PDF of a paper given its arXiv ID, if it does not already exist.\n","        \"\"\"\n","        if not os.path.exists(self.path_db):\n","            os.makedirs(self.path_db)\n","\n","        file_path = os.path.join(self.path_db, f'{arxiv_id}.pdf')\n","\n","        if os.path.exists(file_path):\n","            print(f\"File {file_path} already exists. Skipping download.\")\n","            return file_path\n","\n","        pdf_url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n","        response = requests.get(pdf_url)\n","\n","        if response.status_code != 200:\n","            raise Exception('Error downloading PDF from arXiv')\n","\n","        with open(file_path, 'wb') as file:\n","            file.write(response.content)\n","        return file_path\n","\n","    def read_pdf(self, arxiv_id, end_page=None):\n","        pdf_content = \"\"\n","        file_path = f'{self.path_db}/{arxiv_id}.pdf'\n","\n","        try:\n","            with open(file_path, 'rb') as file:\n","                reader = PyPDF2.PdfReader(file)\n","                total_pages = len(reader.pages)\n","\n","                if end_page is None or end_page > total_pages:\n","                    end_page = total_pages\n","\n","                for page_num in range(1, end_page):\n","                    page = reader.pages[page_num-1]\n","                    pdf_content += page.extract_text()\n","                    if page_num == self.page_limit:\n","                        print('Page limit reached at', self.page_limit + 1)\n","                        break\n","\n","        except FileNotFoundError:\n","            return f\"Error: The file {file_path} does not exist.\"\n","        except Exception as e:\n","            return f\"An error occurred while reading the file: {e}\"\n","\n","        pdf_content = re.sub(r'\\s+', ' ', pdf_content).strip()\n","        return pdf_content\n","\n","\n","    def load_paper(self, title:str, sections:list=None):\n","        '''\n","        INPUT : title of paper,\n","                list of sections in paper\n","        OUTPUT : text of the paper\n","        '''\n","        paper = self.get_paper_info_by_title(title)\n","        url = self.get_ar5iv_url(paper)\n","        soup = self.get_soup_from_url(url) if self.ar5iv_mode else None\n","        if (soup):\n","            title, header_list = self.get_header_from_soup(soup)\n","            if sections == None:\n","                sections_list = self.list_section(header_list)\n","                instruction_for_agent = f'Here is the title and section of the paper\\ntitle\\n{title}\\nsections\\n{sections_list}\\n\\n Use the \\'loadpaper\\' tool again, specifying the exact sections you want to view in detail.'\n","                return instruction_for_agent\n","            else:\n","                return self.extract_text_under_headers(soup, sections)\n","        else: # case for ar5iv is not exist or request error\n","            arxiv_id = url.split('/')[-1]\n","            download_path = self.download_pdf(arxiv_id)\n","            pdf_content = self.read_pdf(arxiv_id)\n","            return pdf_content\n","\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1719243802397,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"3_YcqSB_SzuK"},"outputs":[],"source":["GetPaperModule = GetPaper(ss_api_key)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":1576,"status":"ok","timestamp":1719243804383,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"Un4HZzRAnn4v"},"outputs":[],"source":["title = 'Large Language Model Connected with Massive APIs'\n","text_list = ['Methodology']\n","text = GetPaperModule.load_paper(title)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1719243804383,"user":{"displayName":"ìµœìœ ë¯¼","userId":"14691279395225646105"},"user_tz":-540},"id":"FXssQdXKKN-p","outputId":"193ed167-3251-4893-fc16-c5123fcaa79e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Here is the title and section of the paper\n","title\n","Gorilla: Large Language Model Connected with Massive APIs\n","sections\n","          Abstract\n","  1 Introduction\n","  2 Related Work\n","        Large Language Models\n","        Tool Usage\n","        LLMs for Program Synthesis\n","  3 Methodology\n","    3.1 Dataset Collection\n","        API Documentation\n","        Instruction Generation\n","    3.2 Gorilla\n","        API Call with Constraints\n","        Retriever-Aware training\n","        Gorilla Inference\n","    3.3 Verifying APIs\n","        AST Sub-Tree Matching\n","  4 Evaluation\n","        Baselines\n","        Retrievers\n","    4.1 AST Accuracy on API call\n","        Finetuning without Retrieval\n","        Finetuning with Retrieval\n","        Hallucination with LLM\n","    4.2 Test-Time Documentation Change\n","    4.3 API Call with Constraints\n","  5 Conclusion\n","  6 Limitations & Social Impacts\n","  7 Acknowledgement\n","  References\n","  8 Appendix\n","    8.1 Dataset Details\n","        Domain Classification\n","        API Call Task\n","        API Provider Component\n","        Explanation Element\n","        Code\n","    8.2 Gorilla Details\n","        Data\n","        Training\n","    8.3 Performance Comparison\n","      8.3.1 Evaluation\n","      8.3.2 Hallucination\n","\n","\n"," Use the 'loadpaper' tool again specifying the exact sections you want to view in detail.\n"]}],"source":["print(text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHF_88yd6hNq"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["bbxP_Psuwg-E"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}
