{"cells":[{"cell_type":"markdown","source":["### Setting for Colab"],"metadata":{"id":"79YmvumUw19_"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P42nOWGUwpW_","executionInfo":{"status":"ok","timestamp":1719243584697,"user_tz":-540,"elapsed":19303,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"fd3dbbd3-87b5-4771-8717-1f925f8610dd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Projects/kubig19th-conference-llm/ym"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W73F_hiPwqoP","executionInfo":{"status":"ok","timestamp":1719243585216,"user_tz":-540,"elapsed":522,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"cac64a15-4d69-4617-abd6-874e49ea24ea"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Projects/kubig19th-conference-llm/ym\n"]}]},{"cell_type":"code","source":["!pip install python-dotenv semanticscholar langchain langchain_openai langchain_core langchainhub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OA7pz_enw1Wc","executionInfo":{"status":"ok","timestamp":1719243609165,"user_tz":-540,"elapsed":23951,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"e39c1669-504b-4a72-cab0-05b82c409214"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting semanticscholar\n","  Downloading semanticscholar-0.8.2-py3-none-any.whl (24 kB)\n","Collecting langchain\n","  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain_openai\n","  Downloading langchain_openai-0.1.9-py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain_core\n","  Downloading langchain_core-0.2.9-py3-none-any.whl (321 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchainhub\n","  Downloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n","Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (8.4.1)\n","Collecting httpx (from semanticscholar)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (1.6.0)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n","  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.81-py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Collecting openai<2.0.0,>=1.26.0 (from langchain_openai)\n","  Downloading openai-1.35.3-py3-none-any.whl (327 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain_openai)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain_core)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n","Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (2024.6.2)\n","Collecting httpcore==1.* (from httpx->semanticscholar)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (3.7)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->semanticscholar)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n","Installing collected packages: types-requests, python-dotenv, orjson, jsonpointer, h11, tiktoken, langchainhub, jsonpatch, httpcore, langsmith, httpx, semanticscholar, openai, langchain_core, langchain-text-splitters, langchain_openai, langchain\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.5 langchain-text-splitters-0.2.1 langchain_core-0.2.9 langchain_openai-0.1.9 langchainhub-0.1.20 langsmith-0.1.81 openai-1.35.3 orjson-3.10.5 python-dotenv-1.0.1 semanticscholar-0.8.2 tiktoken-0.7.0 types-requests-2.32.0.20240622\n"]}]},{"cell_type":"code","source":["!pip install feedparser PyPDF2 beautifulsoup4 requests"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhMB4hgnydgs","executionInfo":{"status":"ok","timestamp":1719243637471,"user_tz":-540,"elapsed":28312,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"e04b5f76-24ec-49d5-ef56-9f55ce8d3aba"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting feedparser\n","  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m925.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Collecting sgmllib3k (from feedparser)\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n","Building wheels for collected packages: sgmllib3k\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=c5ef43cd8e98ac976258aac8b2edef1a5384c6e3419b410468e2ba4fe970eda2\n","  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n","Successfully built sgmllib3k\n","Installing collected packages: sgmllib3k, PyPDF2, feedparser\n","Successfully installed PyPDF2-3.0.1 feedparser-6.0.11 sgmllib3k-1.0.0\n"]}]},{"cell_type":"markdown","source":["### Import Modules"],"metadata":{"id":"T57xxCX6w53q"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"qrmwf0_3EoyA","executionInfo":{"status":"ok","timestamp":1719243658529,"user_tz":-540,"elapsed":14975,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import torch\n","import os\n","from pprint import pprint\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from dotenv import load_dotenv\n","\n","from langchain.pydantic_v1 import BaseModel, Field\n","from langchain.tools import BaseTool, StructuredTool, tool\n","\n","from langchain_core.utils.function_calling import convert_to_openai_function\n","from langchain_openai import ChatOpenAI\n","\n","from langchain import hub\n","from langchain.agents import AgentExecutor, create_openai_tools_agent"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"6qbkQ7-qwg9g","executionInfo":{"status":"ok","timestamp":1719243854067,"user_tz":-540,"elapsed":352,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["import semantic_scholoar_api as ss\n"]},{"cell_type":"code","source":["from getpaper import GetPaper"],"metadata":{"id":"z-6PP4lh61vk","executionInfo":{"status":"ok","timestamp":1719243858386,"user_tz":-540,"elapsed":331,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","execution_count":26,"metadata":{"id":"3iekMnd4wg9k","executionInfo":{"status":"ok","timestamp":1719243860897,"user_tz":-540,"elapsed":422,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["dotenv_path = '.env'\n","load_dotenv(dotenv_path)\n","openai_api = os.getenv(\"OPENAI_API_KEY\")\n","ss_api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")"]},{"cell_type":"markdown","metadata":{"id":"rRoadQ6Xwg9m"},"source":["### Define Tools"]},{"cell_type":"code","source":["getpapermodule = GetPaper(ss_api_key, ar5iv_mode = True, path_db = './papers_db', page_limit = 5)"],"metadata":{"id":"TNCsBS7u2Pfc","executionInfo":{"status":"ok","timestamp":1719243861227,"user_tz":-540,"elapsed":1,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","execution_count":28,"metadata":{"id":"uvpUJHr4wg9r","executionInfo":{"status":"ok","timestamp":1719243862131,"user_tz":-540,"elapsed":2,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["class load_paper_input(BaseModel):\n","    title: str = Field(description=\"target paper title\")\n","    sections: list = Field(description='list of sections', default = None)\n","\n","loadpaper = StructuredTool.from_function(\n","    func=getpapermodule.load_paper,\n","    name=\"loadpaper\",\n","    description=\"The `loadPaper` tool is designed to facilitate the process of retrieving and reading academic papers based on a given search title. \\\n","    The `title` parameter is a string representing the title of the paper. The 'sections' parameter is a list representing the list of the sections in the paper. \\\n","    If the sections parameter is none, you can get the section list of the paper. If the sections parameter get the section list, you can load the paper's content. \\\n","    Use this tool several times to get the section first and then get the detail content of each section\",\n","    args_schema=load_paper_input\n",")"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"-YXoYdfTwg9v","executionInfo":{"status":"ok","timestamp":1719243863090,"user_tz":-540,"elapsed":3,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["class recommend_reference_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","\n","recommend_reference = StructuredTool.from_function(\n","    func=ss.reference_recommend,\n","    name=\"recommend_reference\",\n","    description=\"The reference_recommend function recommends relevant academic papers based on a given query, focusing on papers that the target paper's references. This tool is ideal for researchers and academics looking to find related literature that has been directly cited by the target paper.\",\n","    args_schema=recommend_reference_input\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"sjt5lcMEwg9y","executionInfo":{"status":"ok","timestamp":1719243863090,"user_tz":-540,"elapsed":3,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["class citation_recommend_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","\n","recommend_citation = StructuredTool.from_function(\n","    func=ss.citation_recommend,\n","    name=\"recommend_citation\",\n","    description=\"The recommend_citation function identifies and recommends subsequent papers **that have cited a given target paper**, providing valuable insights into the evolution and impact of the research. This tool helps researchers discover influential follow-up studies and stay updated with the latest developments in their field.\",\n","    args_schema=citation_recommend_input\n",")"]},{"cell_type":"markdown","metadata":{"id":"RCUtBt7Vwg92"},"source":["### Make Agent with GPT3.5"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"TRLoOwN6wg93","executionInfo":{"status":"ok","timestamp":1719243863903,"user_tz":-540,"elapsed":351,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["model = ChatOpenAI(model=\"gpt-3.5-turbo\")"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"mY1iFRyQwg96","executionInfo":{"status":"ok","timestamp":1719243863903,"user_tz":-540,"elapsed":3,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["tools = [loadpaper, recommend_reference, recommend_citation]\n","# functions = [convert_to_openai_function(t) for t in tools]\n","# functions[1]"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"B65FMYzowg97","executionInfo":{"status":"ok","timestamp":1719243864768,"user_tz":-540,"elapsed":867,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["# load Agent prompt\n","# TODO : Write the better prompt for our agent\n","prompt = hub.pull(\"hwchase17/openai-tools-agent\")"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"ZzNdKVQuwg99","executionInfo":{"status":"ok","timestamp":1719243864769,"user_tz":-540,"elapsed":3,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["# Choose the LLM that will drive the agent\n","# Only certain models support this\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","# Construct the OpenAI Tools agent\n","agent = create_openai_tools_agent(llm, tools, prompt)\n","# Create an agent executor by passing in the agent and tools\n","agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g87scjN0wg9-","executionInfo":{"status":"ok","timestamp":1719243878930,"user_tz":-540,"elapsed":14164,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"51b3a809-0059-4ebd-ddca-55404c4c6349"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n","title\n","('h1', 'Toolformer: Language Models Can Teach Themselves to Use Tools')\n","sections\n","          Abstract\n","  1 Introduction\n","  2 Approach\n","        Sampling API Calls\n","        Executing API Calls\n","        Filtering API Calls\n","        Model Finetuning\n","        Inference\n","  3 Tools\n","        Question Answering\n","        Calculator\n","        Wikipedia Search\n","        Machine Translation System\n","        Calendar\n","  4 Experiments\n","    4.1 Experimental Setup\n","        Dataset Generation\n","        Model Finetuning\n","        Baseline Models\n","    4.2 Downstream Tasks\n","      4.2.1 LAMA\n","      4.2.2 Math Datasets\n","      4.2.3 Question Answering\n","      4.2.4 Multilingual Question Answering\n","      4.2.5 Temporal Datasets\n","    4.3 Language Modeling\n","    4.4 Scaling Laws\n","  5 Analysis\n","        Decoding Strategy\n","        Data Quality\n","  6 Related Work\n","        Language Model Pretraining\n","        Tool Use\n","        Bootstrapping\n","  7 Limitations\n","  8 Conclusion\n","  References\n","  Appendix A API Details\n","    A.1 Implementation\n","        Question Answering\n","        Calculator\n","        Calendar\n","        Machine Translation\n","    A.2 Prompts\n","        Question Answering\n","        Calculator\n","        Wikipedia Search\n","        Machine Translation\n","        Calendar\n","  Appendix B Toolformer Training\n","  Appendix C Zero-Shot Prompts\n","    C.1 LAMA and TempLAMA\n","    C.2 Math Benchmarks\n","    C.3 Question Answering\n","    C.4 Multilingual Question Answering\n","  Appendix D Dateset\n","\n","\n"," Use the 'loadpaper' tool again specifying the exact sections you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'Language Models Can Teach Themselves to Use Tools', 'sections': ['Filtering API Calls']}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mFiltering API Calls\n","Let i𝑖i be the position of the API call cisubscript𝑐𝑖c_{i} in the sequence 𝐱=x1,…,xn𝐱subscript𝑥1…subscript𝑥𝑛\\mathbf{x}=x_{1},\\ldots,x_{n}, and let risubscript𝑟𝑖r_{i} be the response from the API. Further, given a sequence (wi∣i∈ℕ)conditionalsubscript𝑤𝑖𝑖ℕ(w_{i}\\mid i\\in\\mathbb{N}) of weights, let\n","\n","Li​(𝐳)=−∑j=inwj−i⋅log⁡pM​(xj∣𝐳,x1:j−1)subscript𝐿𝑖𝐳superscriptsubscript𝑗𝑖𝑛⋅subscript𝑤𝑗𝑖subscript𝑝𝑀conditionalsubscript𝑥𝑗𝐳subscript𝑥:1𝑗1L_{i}(\\mathbf{z})=-\\sum_{j=i}^{n}w_{j-i}\\cdot\\log{p_{M}(x_{j}\\mid\\mathbf{z},x_{1:j-1})}\n","\n","be the weighted cross entropy loss for M𝑀M over the tokens xi,…,xnsubscript𝑥𝑖…subscript𝑥𝑛x_{i},\\ldots,x_{n} if the model is prefixed with 𝐳𝐳\\mathbf{z}.\n","We compare two different instantiations of this loss:\n","\n","Li+superscriptsubscript𝐿𝑖\\displaystyle L_{i}^{+}\n","=Li​(e​(ci,ri))absentsubscript𝐿𝑖esubscript𝑐𝑖subscript𝑟𝑖\\displaystyle=L_{i}(\\text{e}(c_{i},r_{i}))\n","\n","Li−superscriptsubscript𝐿𝑖\\displaystyle L_{i}^{-}\n","=min⁡(Li​(ε),Li​(e​(ci,ε)))absentsubscript𝐿𝑖𝜀subscript𝐿𝑖esubscript𝑐𝑖𝜀\\displaystyle=\\min\\left(L_{i}(\\varepsilon),L_{i}(\\text{e}(c_{i},\\varepsilon))\\right)\n","\n","where ε𝜀\\varepsilon denotes an empty sequence. The former is the weighted loss over all tokens xi,…,xnsubscript𝑥𝑖…subscript𝑥𝑛x_{i},\\ldots,x_{n} if the API call and its result are given to M𝑀M as a prefix;333We provide e​(ci,ri)esubscript𝑐𝑖subscript𝑟𝑖\\text{e}(c_{i},r_{i}) as a prefix instead of inserting it at position i𝑖i because M𝑀M is not yet finetuned on any examples containing API calls, so inserting it in the middle of 𝐱𝐱\\mathbf{x} would interrupt the flow and not align with patterns in the pretraining corpus, thus hurting perplexity. the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to M𝑀M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a filtering threshold τfsubscript𝜏𝑓\\tau_{f}, we thus only keep API calls for which\n","\n","Li−−Li+≥τfsuperscriptsubscript𝐿𝑖superscriptsubscript𝐿𝑖subscript𝜏𝑓L_{i}^{-}-L_{i}^{+}\\geq\\tau_{f}\n","\n","holds, i.e., adding the API call and its result reduces the loss by at least τfsubscript𝜏𝑓\\tau_{f}, compared to not doing any API call or obtaining no result from it.\n","\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Language Models Can Teach Themselves to Use Tools\" discusses the filtering of API calls in the context of language models. Here is an explanation of the math expression related to filtering API calls as presented in the paper:\n","\n","The paper introduces a weighted cross-entropy loss function for a language model M over a sequence of tokens xi,…,xn, where i represents the position of the API call ci in the sequence and ri is the response from the API. The loss function is defined as:\n","\n","Li(𝐳) = -∑j=iⁿ wj-i * log pM(xj∣𝐳,x1:j-1)\n","\n","In this expression:\n","- Li(𝐳) is the weighted cross-entropy loss for model M over the tokens if the model is prefixed with 𝐳.\n","- wj-i represents a sequence of weights.\n","- pM(xj∣𝐳,x1:j-1) is the conditional probability of token xj given the prefix 𝐳 and the tokens x1:j-1.\n","\n","The paper compares two instantiations of this loss:\n","1. Li+ = Li(e(ci,ri)): Weighted loss over all tokens if the API call and its result are given as a prefix.\n","2. Li- = min(Li(ε), Li(e(ci,ε)): Minimum of the losses obtained from not doing any API call and doing an API call without providing the response.\n","\n","The filtering process involves a filtering threshold τf, where API calls are kept only if the difference between Li- and Li+ is greater than or equal to τf. This condition ensures that adding the API call and its result reduces the loss by at least τf compared to not using any API call or not receiving its result.\n","\n","This math expression and filtering mechanism help determine the relevance and impact of API calls on the language model's performance and prediction accuracy.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# If you want the model to see the specific page or specific content in the paper, mention the page in the prompt.\n","\n","output = agent_executor.invoke({\"input\": \"explain about the math expression about the Filtering api calls in the paper 'Language Models Can Teach Themselves to Use Tools'\"})"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5SQLHhC3wg9_","executionInfo":{"status":"ok","timestamp":1719243878931,"user_tz":-540,"elapsed":16,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"42a11704-7d26-49f0-82ee-49da6d87e424"},"outputs":[{"output_type":"stream","name":"stdout","text":["('The paper \"Language Models Can Teach Themselves to Use Tools\" discusses the '\n"," 'filtering of API calls in the context of language models. Here is an '\n"," 'explanation of the math expression related to filtering API calls as '\n"," 'presented in the paper:\\n'\n"," '\\n'\n"," 'The paper introduces a weighted cross-entropy loss function for a language '\n"," 'model M over a sequence of tokens xi,…,xn, where i represents the position '\n"," 'of the API call ci in the sequence and ri is the response from the API. The '\n"," 'loss function is defined as:\\n'\n"," '\\n'\n"," 'Li(𝐳) = -∑j=iⁿ wj-i * log pM(xj∣𝐳,x1:j-1)\\n'\n"," '\\n'\n"," 'In this expression:\\n'\n"," '- Li(𝐳) is the weighted cross-entropy loss for model M over the tokens if '\n"," 'the model is prefixed with 𝐳.\\n'\n"," '- wj-i represents a sequence of weights.\\n'\n"," '- pM(xj∣𝐳,x1:j-1) is the conditional probability of token xj given the '\n"," 'prefix 𝐳 and the tokens x1:j-1.\\n'\n"," '\\n'\n"," 'The paper compares two instantiations of this loss:\\n'\n"," '1. Li+ = Li(e(ci,ri)): Weighted loss over all tokens if the API call and its '\n"," 'result are given as a prefix.\\n'\n"," '2. Li- = min(Li(ε), Li(e(ci,ε)): Minimum of the losses obtained from not '\n"," 'doing any API call and doing an API call without providing the response.\\n'\n"," '\\n'\n"," 'The filtering process involves a filtering threshold τf, where API calls are '\n"," 'kept only if the difference between Li- and Li+ is greater than or equal to '\n"," 'τf. This condition ensures that adding the API call and its result reduces '\n"," 'the loss by at least τf compared to not using any API call or not receiving '\n"," 'its result.\\n'\n"," '\\n'\n"," 'This math expression and filtering mechanism help determine the relevance '\n"," \"and impact of API calls on the language model's performance and prediction \"\n"," 'accuracy.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"26_jBBViwg-A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719243916015,"user_tz":-540,"elapsed":21991,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"7db3611f-f111-4c13-d2ae-b8452e3ea654"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `recommend_citation` with `{'query': 'Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[38;5;200m\u001b[1;3m[{'paperId': '7d8905a1fd288068f12c8347caeabefd36d0dd6c', 'title': 'Gorilla: Large Language Model Connected with Massive APIs', 'abstract': \"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\", 'influentialCitationCount': 29, 'publicationDate': '2023-05-24'}, {'paperId': '58f8925a8b87054ad0635a6398a7fe24935b1604', 'title': 'Mind2Web: Towards a Generalist Agent for the Web', 'abstract': 'We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.', 'influentialCitationCount': 26, 'publicationDate': '2023-06-09'}, {'paperId': '0bfc804e31eecfd77f45e4ee7f4d629fffdcd628', 'title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'abstract': 'Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.', 'influentialCitationCount': 51, 'publicationDate': '2023-07-31'}, {'paperId': '5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0', 'title': 'Qwen Technical Report', 'abstract': 'Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.', 'influentialCitationCount': 73, 'publicationDate': '2023-09-28'}, {'paperId': 'd1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43', 'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face', 'abstract': 'Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.', 'influentialCitationCount': 50, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999)}]\u001b[0m\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'Gorilla: Large Language Model Connected with Massive APIs'}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper\n","title\n","('h1', 'Gorilla: Large Language Model Connected with Massive APIs')\n","sections\n","          Abstract\n","  1 Introduction\n","  2 Related Work\n","        Large Language Models\n","        Tool Usage\n","        LLMs for Program Synthesis\n","  3 Methodology\n","    3.1 Dataset Collection\n","        API Documentation\n","        Instruction Generation\n","    3.2 Gorilla\n","        API Call with Constraints\n","        Retriever-Aware training\n","        Gorilla Inference\n","    3.3 Verifying APIs\n","        AST Sub-Tree Matching\n","  4 Evaluation\n","        Baselines\n","        Retrievers\n","    4.1 AST Accuracy on API call\n","        Finetuning without Retrieval\n","        Finetuning with Retrieval\n","        Hallucination with LLM\n","    4.2 Test-Time Documentation Change\n","    4.3 API Call with Constraints\n","  5 Conclusion\n","  6 Limitations & Social Impacts\n","  7 Acknowledgement\n","  References\n","  8 Appendix\n","    8.1 Dataset Details\n","        Domain Classification\n","        API Call Task\n","        API Provider Component\n","        Explanation Element\n","        Code\n","    8.2 Gorilla Details\n","        Data\n","        Training\n","    8.3 Performance Comparison\n","      8.3.1 Evaluation\n","      8.3.2 Hallucination\n","\n","\n"," Use the 'loadpaper' tool again specifying the exact sections you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'Gorilla: Large Language Model Connected with Massive APIs', 'sections': ['Abstract']}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mAbstractLarge Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model’s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs.\n","Gorilla’s code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\u001b[0m\u001b[32;1m\u001b[1;3mThe recommended paper that is similar to \"Language Models Can Teach Themselves to Use Tools\" is titled \"Gorilla: Large Language Model Connected with Massive APIs.\" \n","\n","**Abstract:**\n","Large Language Models (LLMs) have made significant advancements, excelling in tasks like mathematical reasoning and program synthesis. However, their ability to effectively use tools through API calls remains a challenge. Gorilla, a finetuned LLaMA-based model, surpasses the performance of GPT-4 in writing API calls. By integrating with a document retriever, Gorilla adapts to test-time document changes, reducing hallucination issues. The model's evaluation includes APIBench, a dataset with HuggingFace, TorchHub, and TensorHub APIs. Gorilla demonstrates the potential for LLMs to use tools accurately, adapt to updated documentation, and enhance the reliability of their outputs.\n","\n","If you would like to explore more sections or details from this paper, feel free to let me know!\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["output = agent_executor.invoke({\"input\": \"논문 'Language Models Can Teach Themselves to Use Tools'와 비슷한 후속 논문을 추천해주고, 해당 논문의 abstract가 내가 준 논문과 어떤 차이가 있는지 알려줘\"})\n","# TODO : we do not need paperID in this tool."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHV5thnVwg-A","outputId":"ece5b6af-eadb-406f-fc51-df9518ec7bae"},"outputs":[{"name":"stdout","output_type":"stream","text":["('Here are some recommended papers related to \"Language Models Can Teach '\n"," 'Themselves to Use Tools\":\\n'\n"," '\\n'\n"," '1. **Gorilla: Large Language Model Connected with Massive APIs**\\n'\n"," '   - Abstract: Large Language Models (LLMs) have seen significant '\n"," 'advancements recently, excelling in various tasks. Gorilla, a finetuned '\n"," 'LLaMA-based model, surpasses the performance of GPT-4 in writing API calls. '\n"," 'It demonstrates a strong capability to adapt to test-time document changes '\n"," 'and mitigate hallucination issues.\\n'\n"," '   - Publication Date: May 24, 2023\\n'\n"," '\\n'\n"," '2. **Mind2Web: Towards a Generalist Agent for the Web**\\n'\n"," '   - Abstract: Introduces Mind2Web, a dataset for developing generalist '\n"," 'agents for the web. It provides diverse tasks from real-world websites and '\n"," 'explores using large language models for building generalist web agents.\\n'\n"," '   - Publication Date: June 9, 2023\\n'\n"," '\\n'\n"," '3. **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world '\n"," 'APIs**\\n'\n"," '   - Abstract: ToolLLM bridges the gap in tool-use capabilities of LLMs by '\n"," 'introducing a general tool-use framework. It fine-tunes LLaMA to execute '\n"," 'complex instructions and generalize to unseen APIs, demonstrating strong '\n"," 'zero-shot generalization ability.\\n'\n"," '   - Publication Date: July 31, 2023\\n'\n"," '\\n'\n"," '4. **Qwen Technical Report**\\n'\n"," '   - Abstract: Introduces Qwen, a comprehensive language model series with '\n"," 'distinct models showcasing superior performance across various tasks. The '\n"," 'chat models possess advanced tool-use and planning capabilities, even on '\n"," 'complex tasks like utilizing a code interpreter.\\n'\n"," '   - Publication Date: September 28, 2023\\n'\n"," '\\n'\n"," '5. **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging '\n"," 'Face**\\n'\n"," '   - Abstract: Presents HuggingGPT, an LLM-powered agent that connects '\n"," \"various AI models to solve complicated AI tasks. By leveraging ChatGPT's \"\n"," 'language capabilities and AI models in Hugging Face, HuggingGPT can tackle a '\n"," 'wide range of sophisticated tasks.\\n'\n"," '   - Publication Date: December 31, 9999\\n'\n"," '\\n'\n"," 'These papers offer valuable insights into the advancements and applications '\n"," 'of large language models in various domains.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"f1yIVnHKwg-B","outputId":"9f942f68-b88b-4baf-fdbd-4e64a9ef9571","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719242855113,"user_tz":-540,"elapsed":5972,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'title': 'Large Language Model Connected with Massive APIs'}`\n","\n","\n","\u001b[0m\u001b[36;1m\u001b[1;3mGorilla: Large Language Model Connected with Massive APIs\n","          Abstract\n","  1 Introduction\n","  2 Related Work\n","        Large Language Models\n","        Tool Usage\n","        LLMs for Program Synthesis\n","  3 Methodology\n","    3.1 Dataset Collection\n","        API Documentation\n","        Instruction Generation\n","    3.2 Gorilla\n","        API Call with Constraints\n","        Retriever-Aware training\n","        Gorilla Inference\n","    3.3 Verifying APIs\n","        AST Sub-Tree Matching\n","  4 Evaluation\n","        Baselines\n","        Retrievers\n","    4.1 AST Accuracy on API call\n","        Finetuning without Retrieval\n","        Finetuning with Retrieval\n","        Hallucination with LLM\n","    4.2 Test-Time Documentation Change\n","    4.3 API Call with Constraints\n","  5 Conclusion\n","  6 Limitations & Social Impacts\n","  7 Acknowledgement\n","  References\n","  8 Appendix\n","    8.1 Dataset Details\n","        Domain Classification\n","        API Call Task\n","        API Provider Component\n","        Explanation Element\n","        Code\n","    8.2 Gorilla Details\n","        Data\n","        Training\n","    8.3 Performance Comparison\n","      8.3.1 Evaluation\n","      8.3.2 Hallucination\n","\u001b[0m\u001b[32;1m\u001b[1;3mThe abstract of the paper \"Large Language Model Connected with Massive APIs\" discusses the development of Gorilla, a large language model connected with massive APIs. The paper introduces the methodology used, including dataset collection, Gorilla model implementation, and API verification techniques. The evaluation section compares Gorilla with baselines and assesses its performance in various tasks such as AST accuracy on API calls, test-time documentation changes, and API calls with constraints. The paper concludes with insights on the limitations, social impacts, and acknowledgments.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# TODO : Change Arxiv Search enging to Semantic Scholar\n","# TODO : error handling when the tool called.\n","# Because below code doesn't work. with the difference of 'Gorilla: '\n","# output = agent_executor.invoke({\"input\": \"summary the abstract of the paper 'Gorilla: Large Language Model Connected with Massive APIs'\"})\n","\n","output = agent_executor.invoke({\"input\": \"summary the abstract of the paper 'Large Language Model Connected with Massive APIs'\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXyHypjmwg-C","outputId":"457f03a8-e5a2-4c16-fbd3-e1493f13a298"},"outputs":[{"name":"stdout","output_type":"stream","text":["('The paper titled \"Large Language Model Connected with Massive APIs\" '\n"," 'introduces Gorilla, a finetuned LLaMA-based model that surpasses the '\n"," 'performance of GPT-4 in writing API calls. Gorilla, when combined with a '\n"," 'document retriever, demonstrates the ability to adapt to test-time document '\n"," 'changes, reducing hallucination errors commonly encountered with LLMs. The '\n"," 'paper introduces APIBench, a dataset consisting of HuggingFace, TorchHub, '\n"," \"and TensorHub APIs, to evaluate the model's ability accurately. Gorilla \"\n"," 'significantly outperforms GPT-4 in terms of API functionality accuracy and '\n"," 'reduces hallucination errors. The paper emphasizes the importance of '\n"," 'empowering LLMs to use tools via API calls to access vast knowledge bases '\n"," 'and accomplish complex computational tasks effectively.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hX5aaMB6wg-D","outputId":"eab79a0c-23ce-4bf7-f2b1-00f46c25ea30"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `recommend_reference` with `{'query': 'Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[33;1m\u001b[1;3m[{'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0', 'title': 'Language Models are Few-Shot Learners', 'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\", 'influentialCitationCount': 3130, 'publicationDate': '2020-05-28', 'intent': 'methodology background result', 'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'}, {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb', 'title': 'PaLM: Scaling Language Modeling with Pathways', 'abstract': 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.', 'influentialCitationCount': 307, 'publicationDate': '2022-04-05', 'intent': 'background', 'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'}, {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221', 'title': 'OPT: Open Pre-trained Transformer Language Models', 'abstract': 'Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.', 'influentialCitationCount': 274, 'publicationDate': '2022-05-02', 'intent': 'result', 'context': 'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'}, {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', 'influentialCitationCount': 18170, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'background', 'context': 'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.'}, {'paperId': '9405cc0d6169988371b2755e573cc28650d14dfe', 'title': 'Language Models are Unsupervised Multitask Learners', 'abstract': 'Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.', 'influentialCitationCount': 2826, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'methodology', 'context': 'To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some recommended reference works related to the paper \"Language Models Can Teach Themselves to Use Tools\":\n","\n","1. **Paper Title:** Language Models are Few-Shot Learners\n","   - **Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. This paper shows that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\n","   - **Publication Date:** 2020-05-28\n","\n","2. **Paper Title:** PaLM: Scaling Language Modeling with Pathways\n","   - **Abstract:** This paper explores the impact of scale on few-shot learning by training a 540-billion parameter, densely activated, Transformer language model called Pathways Language Model PaLM. It achieves breakthrough performance on language understanding and generation benchmarks and demonstrates benefits of scaling in achieving state-of-the-art few-shot learning results.\n","   - **Publication Date:** 2022-04-05\n","\n","3. **Paper Title:** OPT: Open Pre-trained Transformer Language Models\n","   - **Abstract:** OPT presents a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, aiming to be fully and responsibly shared with interested researchers. The paper compares OPT-175B to GPT-3 and discusses the model's performance and carbon footprint.\n","   - **Publication Date:** 2022-05-02\n","\n","4. **Paper Title:** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n","   - **Abstract:** This paper introduces BERT, a language representation model designed for pre-training deep bidirectional representations from unlabeled text. BERT achieves state-of-the-art results on eleven natural language processing tasks and demonstrates the power of pre-training in creating models for various tasks.\n","   - **Publication Date:** Not specified\n","\n","5. **Paper Title:** Language Models are Unsupervised Multitask Learners\n","   - **Abstract:** This paper demonstrates that language models begin to learn natural language processing tasks without explicit supervision when trained on a new dataset of millions of webpages called WebText. The findings suggest a promising path towards building language processing systems that learn tasks from naturally occurring demonstrations.\n","   - **Publication Date:** Not specified\n","\n","These reference works provide valuable insights and background information related to language models and their capabilities in natural language processing tasks.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["output = agent_executor.invoke({\"input\": \"recommend the reference works of the paper 'Language Models Can Teach Themselves to Use Tools'\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEYY1iAWwg-D","outputId":"1260342a-d7d3-4486-94b3-8743f59f62d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["('Here are some recommended reference works related to the paper \"Language '\n"," 'Models Can Teach Themselves to Use Tools\":\\n'\n"," '\\n'\n"," '1. **Paper Title:** Language Models are Few-Shot Learners\\n'\n"," '   - **Abstract:** Recent work has demonstrated substantial gains on many '\n"," 'NLP tasks and benchmarks by pre-training on a large corpus of text followed '\n"," 'by fine-tuning on a specific task. This paper shows that scaling up language '\n"," 'models greatly improves task-agnostic, few-shot performance, sometimes even '\n"," 'reaching competitiveness with prior state-of-the-art fine-tuning '\n"," 'approaches.\\n'\n"," '   - **Publication Date:** 2020-05-28\\n'\n"," '\\n'\n"," '2. **Paper Title:** PaLM: Scaling Language Modeling with Pathways\\n'\n"," '   - **Abstract:** This paper explores the impact of scale on few-shot '\n"," 'learning by training a 540-billion parameter, densely activated, Transformer '\n"," 'language model called Pathways Language Model PaLM. It achieves breakthrough '\n"," 'performance on language understanding and generation benchmarks and '\n"," 'demonstrates benefits of scaling in achieving state-of-the-art few-shot '\n"," 'learning results.\\n'\n"," '   - **Publication Date:** 2022-04-05\\n'\n"," '\\n'\n"," '3. **Paper Title:** OPT: Open Pre-trained Transformer Language Models\\n'\n"," '   - **Abstract:** OPT presents a suite of decoder-only pre-trained '\n"," 'transformers ranging from 125M to 175B parameters, aiming to be fully and '\n"," 'responsibly shared with interested researchers. The paper compares OPT-175B '\n"," \"to GPT-3 and discusses the model's performance and carbon footprint.\\n\"\n"," '   - **Publication Date:** 2022-05-02\\n'\n"," '\\n'\n"," '4. **Paper Title:** BERT: Pre-training of Deep Bidirectional Transformers '\n"," 'for Language Understanding\\n'\n"," '   - **Abstract:** This paper introduces BERT, a language representation '\n"," 'model designed for pre-training deep bidirectional representations from '\n"," 'unlabeled text. BERT achieves state-of-the-art results on eleven natural '\n"," 'language processing tasks and demonstrates the power of pre-training in '\n"," 'creating models for various tasks.\\n'\n"," '   - **Publication Date:** Not specified\\n'\n"," '\\n'\n"," '5. **Paper Title:** Language Models are Unsupervised Multitask Learners\\n'\n"," '   - **Abstract:** This paper demonstrates that language models begin to '\n"," 'learn natural language processing tasks without explicit supervision when '\n"," 'trained on a new dataset of millions of webpages called WebText. The '\n"," 'findings suggest a promising path towards building language processing '\n"," 'systems that learn tasks from naturally occurring demonstrations.\\n'\n"," '   - **Publication Date:** Not specified\\n'\n"," '\\n'\n"," 'These reference works provide valuable insights and background information '\n"," 'related to language models and their capabilities in natural language '\n"," 'processing tasks.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"markdown","metadata":{"id":"bbxP_Psuwg-E"},"source":["### Todo : Make Agent with Llama"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wULDS5ozwg-F"},"outputs":[],"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AvoAOTmwg-F","outputId":"fc02781b-fbe0-47da-c521-c555fcc589bf"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading shards:  25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]"]}],"source":["# we can use 262k version of Llama!\n","model_name = 'gradientai/Llama-3-8B-Instruct-262k'\n","\n","dtype = \"float16\"\n","if torch.cuda.is_bf16_supported():\n","    dtype = \"bfloat16\"\n","\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=getattr(torch, dtype), quantization_config=quantization_config,\n","                                             device_map=\"auto\",  cache_dir=os.getenv(\"HF_HOME\", \"~/.cache/huggingface\"))\n","model.eval()\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmpO15ehwg-G"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### HTML -> getpaper.py"],"metadata":{"id":"zrfI9u3YHYxw"}},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEMQu5lAHzeE","executionInfo":{"status":"ok","timestamp":1719235321700,"user_tz":-540,"elapsed":16683,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"0be8eb78-7bbb-4ec0-d186-cbb681a33b2b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from requests.exceptions import RequestException\n","import PyPDF2\n","import re\n"],"metadata":{"id":"wj5T-OOHHu32","executionInfo":{"status":"ok","timestamp":1719243752174,"user_tz":-540,"elapsed":365,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from requests.exceptions import RequestException\n","import PyPDF2\n","import re\n","import os\n","\n","class GetPaper:\n","    def __init__(self, ss_api_key, ar5iv_mode = True, path_db='./papers_db', page_limit = 5):\n","        self.ss_api_key = ss_api_key\n","        self.ar5iv_mode = ar5iv_mode\n","        self.path_db= path_db\n","        self.page_limit = page_limit\n","\n","    def get_paper_info_by_title(self, title):\n","        \"\"\"논문의 제목으로 정보를 가져오는 함수\"\"\"\n","        # Define the API endpoint URL\n","        url = 'https://api.semanticscholar.org/graph/v1/paper/search?query={}&fields=paperId,title,abstract,authors,citations,fieldsOfStudy,influentialCitationCount,isOpenAccess,openAccessPdf,publicationDate,publicationTypes,references,venue'\n","\n","        headers = {'x-api-key': self.ss_api_key}\n","        response = requests.get(url.format(title), headers=headers).json()\n","\n","        if response.get('data'):\n","            paper = response['data'][0]\n","            return paper\n","        else:\n","            return None\n","\n","    def get_ar5iv_url(self, paper):\n","        \"논문의 ar5iv 주소를 받아오는 함수\"\n","        external_ids = paper.get('openAccessPdf', {})\n","        arxiv_id = external_ids.get('url')\n","        if 'http' in arxiv_id:\n","            arxiv_id = arxiv_id.split('/')[-1]\n","            return f\"https://ar5iv.org/abs/{arxiv_id}\"\n","        else:\n","            return None\n","\n","    def get_soup_from_url(self, url):\n","        try:\n","            response = requests.get(url, timeout=10)\n","            response.raise_for_status()  # HTTP 에러가 발생하면 예외를 발생시킴\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            return soup\n","        except RequestException as e:\n","            print(f\"Error fetching the URL: {e}\")\n","            return None\n","        except Exception as e:\n","            print(f\"An unexpected error occurred: {e}\")\n","            return None\n","\n","    def get_header_from_soup(self, soup):\n","        # h1부터 h6까지 태그 추출\n","        headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n","\n","        # 태그와 내용을 계층 구조로 저장\n","        header_list = [(header.name, header.text.strip()) for header in headers]\n","        title = header_list[0][1]\n","        header_list = header_list[1:]\n","        return title, header_list\n","\n","\n","    def extract_text_under_headers(self, soup, text_list):\n","        # 결과를 저장할 변수\n","        results = []\n","\n","        # 텍스트 리스트를 순회하며 각 텍스트에 해당하는 헤더와 그 아래의 텍스트를 추출\n","        for text in text_list:\n","            header_tag = soup.find(lambda tag: tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6'] and text in tag.get_text())\n","            if header_tag:\n","                header_text = header_tag.get_text(strip=False)\n","                header_level = int(header_tag.name[1])\n","                current_header = {'tag': header_tag.name, 'text': header_text, 'subsections': []}\n","                results.append(current_header)\n","\n","                next_element = header_tag.find_next_sibling()\n","                while next_element:\n","                    if next_element.name and next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n","                        next_level = int(next_element.name[1])\n","                        if next_level <= header_level:\n","                            break\n","\n","                        # If it's a tag and within our header range\n","                        if next_element.name and next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n","                            next_text = next_element.get_text(strip=False)\n","                            next_subheader = {'tag': next_element.name, 'text': next_text, 'subsections': []}\n","                            current_header['subsections'].append(next_subheader)\n","                            current_header = next_subheader\n","                            header_level = next_level\n","                    else:\n","                        if 'subsections' not in current_header:\n","                            current_header['subsections'] = []\n","                        current_header['subsections'].append({'tag': 'p', 'text': next_element.get_text(strip=False)})\n","\n","                    next_element = next_element.find_next_sibling()\n","\n","        content = ''\n","        for x in results:\n","            content += x['text']\n","            for y in x['subsections']:\n","                content += y['text']\n","        content = re.sub(r'\\n{3,}', '\\n\\n', content) # 3번 이상 \\n 이 연속되면 2번으로 줄이기\n","        return content\n","\n","    def list_section(self, header_list):\n","        section_list = ''\n","        for tag, text in header_list:\n","            level = int(tag[1])  # 태그에서 레벨을 추출 (h1 -> 1, h2 -> 2, ..)\n","            section_list += '  ' * (level - 1) + text +'\\n'\n","        return section_list\n","\n","\n","    def download_pdf(self, arxiv_id):\n","        \"\"\"\n","        Download the PDF of a paper given its arXiv ID, if it does not already exist.\n","        \"\"\"\n","        if not os.path.exists(self.path_db):\n","            os.makedirs(self.path_db)\n","\n","        file_path = os.path.join(self.path_db, f'{arxiv_id}.pdf')\n","\n","        if os.path.exists(file_path):\n","            print(f\"File {file_path} already exists. Skipping download.\")\n","            return file_path\n","\n","        pdf_url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n","        response = requests.get(pdf_url)\n","\n","        if response.status_code != 200:\n","            raise Exception('Error downloading PDF from arXiv')\n","\n","        with open(file_path, 'wb') as file:\n","            file.write(response.content)\n","        return file_path\n","\n","    def read_pdf(self, arxiv_id, end_page=None):\n","        pdf_content = \"\"\n","        file_path = f'{self.path_db}/{arxiv_id}.pdf'\n","\n","        try:\n","            with open(file_path, 'rb') as file:\n","                reader = PyPDF2.PdfReader(file)\n","                total_pages = len(reader.pages)\n","\n","                if end_page is None or end_page > total_pages:\n","                    end_page = total_pages\n","\n","                for page_num in range(1, end_page):\n","                    page = reader.pages[page_num-1]\n","                    pdf_content += page.extract_text()\n","                    if page_num == self.page_limit:\n","                        print('Page limit reached at', self.page_limit + 1)\n","                        break\n","\n","        except FileNotFoundError:\n","            return f\"Error: The file {file_path} does not exist.\"\n","        except Exception as e:\n","            return f\"An error occurred while reading the file: {e}\"\n","\n","        pdf_content = re.sub(r'\\s+', ' ', pdf_content).strip()\n","        return pdf_content\n","\n","\n","    def load_paper(self, title:str, sections:list=None):\n","        '''\n","        INPUT : title of paper,\n","                list of sections in paper\n","        OUTPUT : text of the paper\n","        '''\n","        paper = self.get_paper_info_by_title(title)\n","        url = self.get_ar5iv_url(paper)\n","        soup = self.get_soup_from_url(url) if self.ar5iv_mode else None\n","        if (soup):\n","            title, header_list = self.get_header_from_soup(soup)\n","            if sections == None:\n","                sections_list = self.list_section(header_list)\n","                instruction_for_agent = f'Here is the title and section of the paper\\ntitle\\n{title}\\nsections\\n{sections_list}\\n\\n Use the \\'loadpaper\\' tool again, specifying the exact sections you want to view in detail.'\n","                return instruction_for_agent\n","            else:\n","                return self.extract_text_under_headers(soup, sections)\n","        else: # case for ar5iv is not exist or request error\n","            arxiv_id = url.split('/')[-1]\n","            download_path = self.download_pdf(arxiv_id)\n","            pdf_content = self.read_pdf(arxiv_id)\n","            return pdf_content\n","\n","\n"],"metadata":{"id":"DjVjHTaxSIL4","executionInfo":{"status":"ok","timestamp":1719243801997,"user_tz":-540,"elapsed":3,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["GetPaperModule = GetPaper(ss_api_key)"],"metadata":{"id":"3_YcqSB_SzuK","executionInfo":{"status":"ok","timestamp":1719243802397,"user_tz":-540,"elapsed":3,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["title = 'Large Language Model Connected with Massive APIs'\n","text_list = ['Methodology']\n","text = GetPaperModule.load_paper(title)"],"metadata":{"id":"Un4HZzRAnn4v","executionInfo":{"status":"ok","timestamp":1719243804383,"user_tz":-540,"elapsed":1576,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["print(text)\n"],"metadata":{"id":"FXssQdXKKN-p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719243804383,"user_tz":-540,"elapsed":3,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"193ed167-3251-4893-fc16-c5123fcaa79e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is the title and section of the paper\n","title\n","Gorilla: Large Language Model Connected with Massive APIs\n","sections\n","          Abstract\n","  1 Introduction\n","  2 Related Work\n","        Large Language Models\n","        Tool Usage\n","        LLMs for Program Synthesis\n","  3 Methodology\n","    3.1 Dataset Collection\n","        API Documentation\n","        Instruction Generation\n","    3.2 Gorilla\n","        API Call with Constraints\n","        Retriever-Aware training\n","        Gorilla Inference\n","    3.3 Verifying APIs\n","        AST Sub-Tree Matching\n","  4 Evaluation\n","        Baselines\n","        Retrievers\n","    4.1 AST Accuracy on API call\n","        Finetuning without Retrieval\n","        Finetuning with Retrieval\n","        Hallucination with LLM\n","    4.2 Test-Time Documentation Change\n","    4.3 API Call with Constraints\n","  5 Conclusion\n","  6 Limitations & Social Impacts\n","  7 Acknowledgement\n","  References\n","  8 Appendix\n","    8.1 Dataset Details\n","        Domain Classification\n","        API Call Task\n","        API Provider Component\n","        Explanation Element\n","        Code\n","    8.2 Gorilla Details\n","        Data\n","        Training\n","    8.3 Performance Comparison\n","      8.3.1 Evaluation\n","      8.3.2 Hallucination\n","\n","\n"," Use the 'loadpaper' tool again specifying the exact sections you want to view in detail.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pHF_88yd6hNq"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["bbxP_Psuwg-E"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}