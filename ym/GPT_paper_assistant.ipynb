{"cells":[{"cell_type":"markdown","source":["### Setting for Colab"],"metadata":{"id":"79YmvumUw19_"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P42nOWGUwpW_","executionInfo":{"status":"ok","timestamp":1719221740675,"user_tz":-540,"elapsed":24173,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"a9abc3e9-53fb-4657-ac4b-a1c1d1049354"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Projects/kubig19th-conference-llm/ym"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W73F_hiPwqoP","executionInfo":{"status":"ok","timestamp":1719221741220,"user_tz":-540,"elapsed":552,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"6f6c09e2-1768-4b66-f693-be1b4193c10a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Projects/kubig19th-conference-llm/ym\n"]}]},{"cell_type":"code","source":["!pip install python-dotenv semanticscholar langchain langchain_openai langchain_core langchainhub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OA7pz_enw1Wc","executionInfo":{"status":"ok","timestamp":1719221768504,"user_tz":-540,"elapsed":27289,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"da886405-ad28-4baa-c0dc-7cdc5163d563"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting semanticscholar\n","  Downloading semanticscholar-0.8.2-py3-none-any.whl (24 kB)\n","Collecting langchain\n","  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain_openai\n","  Downloading langchain_openai-0.1.9-py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain_core\n","  Downloading langchain_core-0.2.9-py3-none-any.whl (321 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchainhub\n","  Downloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n","Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (8.4.1)\n","Collecting httpx (from semanticscholar)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (1.6.0)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n","  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.81-py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Collecting openai<2.0.0,>=1.26.0 (from langchain_openai)\n","  Downloading openai-1.35.3-py3-none-any.whl (327 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain_openai)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain_core)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n","Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (2024.6.2)\n","Collecting httpcore==1.* (from httpx->semanticscholar)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (3.7)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->semanticscholar)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n","Installing collected packages: types-requests, python-dotenv, orjson, jsonpointer, h11, tiktoken, langchainhub, jsonpatch, httpcore, langsmith, httpx, semanticscholar, openai, langchain_core, langchain-text-splitters, langchain_openai, langchain\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.5 langchain-text-splitters-0.2.1 langchain_core-0.2.9 langchain_openai-0.1.9 langchainhub-0.1.20 langsmith-0.1.81 openai-1.35.3 orjson-3.10.5 python-dotenv-1.0.1 semanticscholar-0.8.2 tiktoken-0.7.0 types-requests-2.32.0.20240622\n"]}]},{"cell_type":"code","source":["!pip install feedparser PyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhMB4hgnydgs","executionInfo":{"status":"ok","timestamp":1719221788424,"user_tz":-540,"elapsed":19926,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"1dec0757-14d0-4160-b4dd-d0d8b14acc31"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting feedparser\n","  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sgmllib3k (from feedparser)\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: sgmllib3k\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=51c4531939e38d58e12e08223d7bf969556836dffacc5204c011171fef04de59\n","  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n","Successfully built sgmllib3k\n","Installing collected packages: sgmllib3k, PyPDF2, feedparser\n","Successfully installed PyPDF2-3.0.1 feedparser-6.0.11 sgmllib3k-1.0.0\n"]}]},{"cell_type":"markdown","source":["### Import Modules"],"metadata":{"id":"T57xxCX6w53q"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"qrmwf0_3EoyA","executionInfo":{"status":"ok","timestamp":1719221799748,"user_tz":-540,"elapsed":11329,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import torch\n","import os\n","from pprint import pprint\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from dotenv import load_dotenv\n","\n","from langchain.pydantic_v1 import BaseModel, Field\n","from langchain.tools import BaseTool, StructuredTool, tool\n","\n","from langchain_core.utils.function_calling import convert_to_openai_function\n","from langchain_openai import ChatOpenAI\n","\n","from langchain import hub\n","from langchain.agents import AgentExecutor, create_openai_tools_agent"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281,"referenced_widgets":["f98af812a74c42c1a10530a337bdc3fb","2f60128c644f4206a09b39d457fe70fd","43edb99773664e07a3a0763e2f6ed864","8fc11f16025e420baad59b007db15e62","23c9f7d665c348318aab143ab863c8f2","cbb679ba053044369e0d2ddbc6c0867b","24f0c57b5f2944689d029d90237f8a3b","21d3e57759574bf6b63a867bc6278392","c55df13de0ca47a5a9a7e9e8ca15805b","a400940465b8464aaf2bbdd6a226dc59","d5a5766fd81945a6a5e852053928feb3","b4845d9186a7467493c8b75d67eccf67","261e1f781362429ba269efa7241d58cd","68b0a1a1fd774fa7af55e83fbe1fd44b","2dfaf43f144d49f8823e7af85dab476b","6ff190f4acb545c0bc1c14e39e92ca3e","9be46bdfd8c9470da0b04cefcc1e91cb","a36696a63760461e8b3af98f513a4022","6f29907ce612441889c4c5793673a6c1","2e301e3d87f64adb8229e3fa0c57916a","a8c4865862c24a14a84f32959052e8b0","64111a4f28a0473b87b2bd6446245fc0","e923a5e5fa7f492bb6bd3b6af00e6587","a97b4c7e89d34ef985e9f7de92ff5da6","2d7abefb4ca04111a95f57cd8e89ab96","3ab119d4ab0b4fc8ba70cfd70c80788b","af5eb9e096ed41d7ae2b0ccc086b9c1b","34480d9ec3164cf984822fb3ed8fa404","faf8d5b569a8493db0a17f998ec5bd00","9cedc84729934fbdaa7d0137069a15fe","4501602c41724555b9fd019a060ddf89","63427e83e38e4543bad11ae029b553f0","ccc7dd51b69047d184490673de65443f","3e148d8c0866438b8f42d45c2a0d9598","6d5c723d7f0b4ab7adac8574101ca097","936f456008454583b1908976fd839060","fb2cf48ae46841e1b95e30ec19d7d0ba","56a567473b75462f98bf44c617622c05","5f4945e178cd4241aae5b88c24bba147","ea6e81b1b80441bd8fd52860efa2f667","971bc147bea1447485a41fe60848622b","cc28132d35e04c80a5f180d108dcaa8d","8eecb01f1b244a35982992e6756c5fc0","83edf745816e41f89c89bec2ba36f376","b0584db3d4f04dc2bfffac916352e8b8","84710db6182f4703aab99f1eea20e617","f92e488fd0d04e569ea98a9187917c1b","a0a05d734f9b4eb1a25f7a89b57cf2ad","4d2d43d42a354e468e6550478017b906","a418b0a198ef4b99a7caa4209a1d2c35","f5cb58a3bcae43d68ddf0a0c1a33adc1","3f99526247264dca950ac3406b8fd6eb","3e5c34a491c34babbf2f0f0ed7cf3e06","1fd85b87177f4bbca63f67ec76138b40","8eba2ca7215e4f918cd7e19ac57a8d3d","17fe43ef24374257980be07638e2b7a0","27eadaa37d0444cda7f62ebcb7cc8149","1e35c3cd1b184da4b2a22ad8ed301fa3","d25332b3eaa0490fa48d3f25f969cc2f","8d9e0cb594f04d5abf66289166fdb521","9c24912eee1f4454b67cd3e9aaf91256","df0696a1170243969c5dd9d54b067bfe","e1044f358c8c438181067bfd8edbe350","2f25874fd0bb4c07bff3814fbfc31b28","fc1b022db9634b0b98567089f8de1eb6","0646571ed687482691a148f4a2769c9d"]},"id":"6qbkQ7-qwg9g","executionInfo":{"status":"ok","timestamp":1719221811553,"user_tz":-540,"elapsed":11808,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"7902c2aa-a373-493f-eec7-e3edc9822a09"},"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f98af812a74c42c1a10530a337bdc3fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4845d9186a7467493c8b75d67eccf67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e923a5e5fa7f492bb6bd3b6af00e6587"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e148d8c0866438b8f42d45c2a0d9598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0584db3d4f04dc2bfffac916352e8b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17fe43ef24374257980be07638e2b7a0"}},"metadata":{}}],"source":["import semantic_scholoar_api as ss"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3iekMnd4wg9k","executionInfo":{"status":"ok","timestamp":1719221812575,"user_tz":-540,"elapsed":5,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"outputs":[],"source":["dotenv_path = '.env'\n","load_dotenv(dotenv_path)\n","openai_api = os.getenv(\"OPENAI_API_KEY\")\n","ss_api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")"]},{"cell_type":"markdown","metadata":{"id":"rRoadQ6Xwg9m"},"source":["### Define Tools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvpUJHr4wg9r"},"outputs":[],"source":["class load_paper_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","    start_page: int = Field(description='start page number')\n","\n","loadpaper = StructuredTool.from_function(\n","    func=ss.query2content,\n","    name=\"loadpaper\",\n","    description=\"The `loadPaper` tool is designed to facilitate the process of retrieving and reading academic papers based on a given search query. The `query` parameter is a string representing the title of the paper. The `loadPaper` tool extracts and returns the text content from the PDF. If the paper does not contain the desired information, you can adjust the start_page argument to specify the starting page for reading. For instance, to extract the title, abstract, or detailed method sections, set start_page to 1. Conversely, for sections like the appendix that are typically found towards the end of the paper, set start_page to a higher number, such as 9.\",\n","    args_schema=load_paper_input\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YXoYdfTwg9v"},"outputs":[],"source":["class recommend_reference_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","\n","recommend_reference = StructuredTool.from_function(\n","    func=ss.reference_recommend,\n","    name=\"recommend_reference\",\n","    description=\"The reference_recommend function recommends relevant academic papers based on a given query, focusing on papers that the target paper's references. This tool is ideal for researchers and academics looking to find related literature that has been directly cited by the target paper.\",\n","    args_schema=recommend_reference_input\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjt5lcMEwg9y"},"outputs":[],"source":["class citation_recommend_input(BaseModel):\n","    query: str = Field(description=\"target paper title\")\n","\n","recommend_citation = StructuredTool.from_function(\n","    func=ss.citation_recommend,\n","    name=\"recommend_citation\",\n","    description=\"The recommend_citation function identifies and recommends subsequent papers **that have cited a given target paper**, providing valuable insights into the evolution and impact of the research. This tool helps researchers discover influential follow-up studies and stay updated with the latest developments in their field.\",\n","    args_schema=citation_recommend_input\n",")"]},{"cell_type":"markdown","metadata":{"id":"RCUtBt7Vwg92"},"source":["### Make Agent with GPT3.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRLoOwN6wg93"},"outputs":[],"source":["model = ChatOpenAI(model=\"gpt-3.5-turbo\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mY1iFRyQwg96"},"outputs":[],"source":["tools = [loadpaper, recommend_reference, recommend_citation]\n","# functions = [convert_to_openai_function(t) for t in tools]\n","# functions[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B65FMYzowg97"},"outputs":[],"source":["# load Agent prompt\n","prompt = hub.pull(\"hwchase17/openai-tools-agent\")"]},{"cell_type":"code","source":["prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Y4QN7piy3hj","executionInfo":{"status":"ok","timestamp":1719208215363,"user_tz":-540,"elapsed":432,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"1beebd3b-bf43-4cc7-d700-a2bc9f96ef06"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzNdKVQuwg99"},"outputs":[],"source":["# Choose the LLM that will drive the agent\n","# Only certain models support this\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","# Construct the OpenAI Tools agent\n","agent = create_openai_tools_agent(llm, tools, prompt)\n","# Create an agent executor by passing in the agent and tools\n","agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g87scjN0wg9-","executionInfo":{"status":"ok","timestamp":1719210639821,"user_tz":-540,"elapsed":9752,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"8d20a2d8-a769-4c3d-f952-5fdd24d91f23"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'query': 'Language Models Can Teach Themselves to Use Tools', 'start_page': 3}`\n","\n","\n","\u001b[0m### Searched Paper ###\n","Title: Toolformer: Language Models Can Teach Themselves to Use Tools\n","Authors: Thomas Scialom\n","Published: 2023-02-09T16:49:57Z\n","ID: 2302.04761v1\n","File ./papers_db/2302.04761v1.pdf already exists. Skipping download.\n","Page limit reached at 4\n","\u001b[36;1m\u001b[1;3mYour task is to add calls to a Question Answering API to a piece of text. The questions should help you get information required to complete the text. You can call the API by writing \"[QA(question)]\" where \"question\" is the question you want to ask. Here are some examples of API calls: Input: Joe Biden was born in Scranton, Pennsylvania. Output: Joe Biden was born in [QA(\"Where was Joe Biden born?\")] Scranton, [QA(\"In which state is Scranton?\")] Pennsylvania. Input: Coca-Cola, or Coke, is a carbonated soft drink manufactured by the Coca-Cola Company. Output: Coca-Cola, or [QA(\"What other name is Coca-Cola known by?\")] Coke, is a carbonated soft drink manufactured by [QA(\"Who manufactures Coca-Cola?\")] the Coca-Cola Company. Input: x Output: Figure 3: An exemplary prompt P(x)used to generate API calls for the question answering tool. Mitself on this dataset. Each of these steps is described in more detail below. Sampling API Calls For each API, we write a promptP(x)that encourages the LM to anno- tate an example x=x1;:::;x nwith API calls. An example of such a prompt for a question an- swering tool is shown in Figure 3; all prompts used are shown in Appendix A.2. Let pM(zn+1j z1;:::;z n)be the probability that Massigns to tokenzn+1as a continuation for the sequence z1;:::;z n. We ﬁrst sample up to kcandidate posi- tions for doing API calls by computing, for each i2f1;:::;ng, the probability pi=pM(<API>jP(x);x1:i\u00001) thatMassigns to starting an API call at position i. Given a sampling threshold s, we keep all po- sitionsI=fijpi> sg; if there are more than k such positions, we only keep the top k. For each position i2I, we then obtain up to m API callsc1 i;:::;cm iby sampling from Mgiven the sequence [P(x);x1;:::;x i\u00001;<API> ]as a preﬁx and</API> as an end-of-sequence token.2 2We discard all examples where Mdoes not generate the </API> token.Executing API Calls As a next step, we execute all API calls generated by Mto obtain the corre- sponding results. How this is done depends entirely on the API itself – for example, it can involve call- ing another neural network, executing a Python script or using a retrieval system to perform search over a large corpus. The response for each API call cineeds to be a single text sequence ri. Filtering API Calls Letibe the position of the API callciin the sequence x=x1;:::;x n, and let ribe the response from the API. Further, given a sequence (wiji2N)ofweights , let Li(z) =\u0000nX j=iwj\u0000i\u0001logpM(xjjz;x1:j\u00001) be the weighted cross entropy loss for Mover the tokensxi;:::;x nif the model is preﬁxed with z. We compare two different instantiations of this loss: L+ i=Li(e(ci;ri)) L\u0000 i= min (Li(\");Li(e(ci;\"))) where\"denotes an empty sequence. The former is the weighted loss over all tokens xi;:::;x nif the API call and its result are given to Mas a preﬁx;3 the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to Mif providing it with both the input andthe output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a ﬁltering threshold f, we thus only keep API calls for which L\u0000 i\u0000L+ i\u0015 f holds, i.e., adding the API call and its result reduces the loss by at least f, compared to not doing any API call or obtaining no result from it. Model Finetuning After sampling and ﬁltering calls for all APIs, we ﬁnally merge the remaining API calls and interleave them with the original inputs. That is, for an input text x=x1;:::;x n with a corresponding API call and result (ci;ri)at positioni, we construct the new sequence x\u0003= 3We provide e(ci;ri)as a preﬁx instead of inserting it at positionibecauseMis not yet ﬁnetuned on any examples containing API calls, so inserting it in the middle of xwould interrupt the ﬂow and not align with patterns in the pretraining corpus, thus hurting perplexity.\u001b[0m\u001b[32;1m\u001b[1;3mThe math expression related to the Filtering API Calls in the 3rd page of the paper \"Language Models Can Teach Themselves to Use Tools\" involves a process of filtering API calls based on a weighted cross entropy loss calculation. Here is a summary of the filtering process:\n","\n","1. **Sampling API Calls**:\n","   - A prompt P(x) is used to encourage the language model (LM) to annotate an example x with API calls.\n","   - The probability p_i is computed for each position i to start an API call.\n","   - Positions with probabilities above a sampling threshold are selected, and up to m API calls are obtained for each selected position.\n","\n","2. **Executing API Calls**:\n","   - All API calls generated by the LM are executed to obtain the corresponding results using various methods depending on the API.\n","\n","3. **Filtering API Calls**:\n","   - The position of the API call c_i in the sequence x and the response r_i from the API are considered.\n","   - Weighted cross entropy loss L_i is calculated for the model M over the tokens x_i to x_n with a given prefix z.\n","   - Two different instantiations of the loss are compared: L^+_i and L^-_i, where L^+_i is the weighted loss over all tokens if the API call and its result are given as a prefix, and L^-_i is the minimum loss obtained from not doing any API call or doing an API call without providing the response.\n","   - API calls are filtered based on a filtering threshold f, where only calls that reduce the loss by at least f compared to not doing any API call or not receiving a result are kept.\n","\n","4. **Model Finetuning**:\n","   - After sampling and filtering calls for all APIs, the remaining API calls are merged and interleaved with the original inputs to create a new sequence.\n","\n","This process aims to optimize the use of API calls to improve the language model's predictions and performance.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# If you want the model to see the specific page or specific content in the paper, mention the page in the prompt.\n","\n","output = agent_executor.invoke({\"input\": \"explain about the math expression about the Filtering api calls in the 3 page of the paper 'Language Models Can Teach Themselves to Use Tools'\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5SQLHhC3wg9_","executionInfo":{"status":"ok","timestamp":1719210639822,"user_tz":-540,"elapsed":55,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"2e2a6650-2e3e-453e-afe4-63b1100275a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["('The math expression related to the Filtering API Calls in the 3rd page of '\n"," 'the paper \"Language Models Can Teach Themselves to Use Tools\" involves a '\n"," 'process of filtering API calls based on a weighted cross entropy loss '\n"," 'calculation. Here is a summary of the filtering process:\\n'\n"," '\\n'\n"," '1. **Sampling API Calls**:\\n'\n"," '   - A prompt P(x) is used to encourage the language model (LM) to annotate '\n"," 'an example x with API calls.\\n'\n"," '   - The probability p_i is computed for each position i to start an API '\n"," 'call.\\n'\n"," '   - Positions with probabilities above a sampling threshold are selected, '\n"," 'and up to m API calls are obtained for each selected position.\\n'\n"," '\\n'\n"," '2. **Executing API Calls**:\\n'\n"," '   - All API calls generated by the LM are executed to obtain the '\n"," 'corresponding results using various methods depending on the API.\\n'\n"," '\\n'\n"," '3. **Filtering API Calls**:\\n'\n"," '   - The position of the API call c_i in the sequence x and the response r_i '\n"," 'from the API are considered.\\n'\n"," '   - Weighted cross entropy loss L_i is calculated for the model M over the '\n"," 'tokens x_i to x_n with a given prefix z.\\n'\n"," '   - Two different instantiations of the loss are compared: L^+_i and L^-_i, '\n"," 'where L^+_i is the weighted loss over all tokens if the API call and its '\n"," 'result are given as a prefix, and L^-_i is the minimum loss obtained from '\n"," 'not doing any API call or doing an API call without providing the response.\\n'\n"," '   - API calls are filtered based on a filtering threshold f, where only '\n"," 'calls that reduce the loss by at least f compared to not doing any API call '\n"," 'or not receiving a result are kept.\\n'\n"," '\\n'\n"," '4. **Model Finetuning**:\\n'\n"," '   - After sampling and filtering calls for all APIs, the remaining API '\n"," 'calls are merged and interleaved with the original inputs to create a new '\n"," 'sequence.\\n'\n"," '\\n'\n"," 'This process aims to optimize the use of API calls to improve the language '\n"," \"model's predictions and performance.\")\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26_jBBViwg-A"},"outputs":[],"source":["output = agent_executor.invoke({\"input\": \"논문 'Language Models Can Teach Themselves to Use Tools'와 비슷한 후속 논문을 추천해주고, 해당 논문의 abstract가 내가 준 논문과 어떤 차이가 있는지 알려줘\"})\n","# TODO : we do not need paperID in this tool."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHV5thnVwg-A","outputId":"ece5b6af-eadb-406f-fc51-df9518ec7bae"},"outputs":[{"name":"stdout","output_type":"stream","text":["('Here are some recommended papers related to \"Language Models Can Teach '\n"," 'Themselves to Use Tools\":\\n'\n"," '\\n'\n"," '1. **Gorilla: Large Language Model Connected with Massive APIs**\\n'\n"," '   - Abstract: Large Language Models (LLMs) have seen significant '\n"," 'advancements recently, excelling in various tasks. Gorilla, a finetuned '\n"," 'LLaMA-based model, surpasses the performance of GPT-4 in writing API calls. '\n"," 'It demonstrates a strong capability to adapt to test-time document changes '\n"," 'and mitigate hallucination issues.\\n'\n"," '   - Publication Date: May 24, 2023\\n'\n"," '\\n'\n"," '2. **Mind2Web: Towards a Generalist Agent for the Web**\\n'\n"," '   - Abstract: Introduces Mind2Web, a dataset for developing generalist '\n"," 'agents for the web. It provides diverse tasks from real-world websites and '\n"," 'explores using large language models for building generalist web agents.\\n'\n"," '   - Publication Date: June 9, 2023\\n'\n"," '\\n'\n"," '3. **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world '\n"," 'APIs**\\n'\n"," '   - Abstract: ToolLLM bridges the gap in tool-use capabilities of LLMs by '\n"," 'introducing a general tool-use framework. It fine-tunes LLaMA to execute '\n"," 'complex instructions and generalize to unseen APIs, demonstrating strong '\n"," 'zero-shot generalization ability.\\n'\n"," '   - Publication Date: July 31, 2023\\n'\n"," '\\n'\n"," '4. **Qwen Technical Report**\\n'\n"," '   - Abstract: Introduces Qwen, a comprehensive language model series with '\n"," 'distinct models showcasing superior performance across various tasks. The '\n"," 'chat models possess advanced tool-use and planning capabilities, even on '\n"," 'complex tasks like utilizing a code interpreter.\\n'\n"," '   - Publication Date: September 28, 2023\\n'\n"," '\\n'\n"," '5. **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging '\n"," 'Face**\\n'\n"," '   - Abstract: Presents HuggingGPT, an LLM-powered agent that connects '\n"," \"various AI models to solve complicated AI tasks. By leveraging ChatGPT's \"\n"," 'language capabilities and AI models in Hugging Face, HuggingGPT can tackle a '\n"," 'wide range of sophisticated tasks.\\n'\n"," '   - Publication Date: December 31, 9999\\n'\n"," '\\n'\n"," 'These papers offer valuable insights into the advancements and applications '\n"," 'of large language models in various domains.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1yIVnHKwg-B","outputId":"d1c33db6-8053-4332-da5b-7f1f7b694bd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `loadpaper` with `{'query': 'Large Language Model Connected with Massive APIs', 'start_page': 1}`\n","\n","\n","\u001b[0m### Searched Paper ###\n","Title: Gorilla: Large Language Model Connected with Massive APIs\n","Authors: Joseph E. Gonzalez\n","Published: 2023-05-24T16:48:11Z\n","ID: 2305.15334v1\n","Page limit reached at 4\n","\u001b[36;1m\u001b[1;3mGorilla: Large Language Model Connected with Massive APIs Shishir G. Patil1∗Tianjun Zhang1,∗Xin Wang2Joseph E. Gonzalez1 1UC Berkeley2Microsoft Research sgp@berkeley.edu Abstract Large Language Models (LLMs) have seen an impressive wave of advances re- cently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of- the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model’s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla’s code, model, data, and demo are available at https://gorilla.cs.berkeley.edu 1 Introduction Recent advances in large language models (LLMs) [ 10,5,32,6,29,30] have enabled significant new capabilities including natural dialogue, mathematical reasoning, and program synthesis. However, despite these advances, LLMs are still fundamentally limited by the information they can store in a fixed set of weights and the things they can compute using a static computation graph and limited context. Furthermore, as the world changes, LLMs require retraining to update their knowledge and reasoning capabilities. By empowering LLMs to use tools [ 33], we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks. By providing access to search technologies and databases, [ 26,39,37] demonstrated that we can augment LLMs to address a significantly larger and more dynamic knowledge space. Similarly, by providing access to computational tools, [ 39,2] demonstrated that LLMs can accomplish complex computational tasks. Consequently, leading LLM providers[ 29], have started to integrate plugins to allow LLMs to invoke external tools through APIs. This transition from a small set of hand-coded tools, to the ability to invoke a vast space of changing cloud APIs could transform LLMs into the primary interface to computing infrastructure and the web. Tasks ranging from booking an entire vacation to hosting a conference, could become as simple as talking to an LLM that has access to the flight, car rental, hotel, catering, and entertainment web APIs. However, much of the prior work [ 35,24] integrating tools into LLMs considered a small well documented set of APIs that can be easily injected into the prompt. ∗Equal contribution. Preprint. Under review.arXiv:2305.15334v1 [cs.CL] 24 May 2023GPT-4<domain>:Speech-to-Text<api_provider>:TorchHub<code>:asr_model= torch.hub.load('snakers4/silero-models', 'asr', source='local')result = asr_model.transcribe(audio_path)Claude<domain>:Audio-Translation<api_provider>:Pytorch<code>:import torchaudiotranslation = Torchaudio.pipelines.WAV2VEC2_ASR_PIPELINE(\"audio.wav\")Gorilla<domain>:Speech-to-Text<api_provider>:TorchHub<code>:asr_model = torch.hub.load('snakers4/silero-models', 'silero_sst’)result = asr_model.transcribe(audio_path) Hallucinate!Wrong library!Good to go!Prompt: Help me find an API to convert the spoken language in a recorded audio to text using Torch Hub. Figure 1: Examples of API calls . Example API calls generated by GPT-4 [ 29], Claude [ 3], and Gorilla for the given prompt. In this example, GPT-4 presents a model that doesn’t exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call. Better Figure 2: Accuracy (vs) hallucination in four settings, that is, zero-shot (i.e., without any retriever), and with retrievers .BM25 andGPTare commonly used retrievers and the oracle retriever returns relevant documents at 100%, indicating an upper bound. Higher in the graph (higher accuracy) and to the left is better (lower hallucination). Across the entire dataset, our model, Gorilla, improves accuracy while reducing hallucination. Supporting a web scale collection of potentially millions of changing APIs requires rethinking our approach to how we integrate tools. It is not longer possible to describe the full set of APIs in a single context. Many of the APIs will have overlapping functionality with nuanced limitations and constraints. Simply evaluating LLMs in this new setting requires new benchmarks. In this paper, we explore the use of self-instruct fine-tuning and retrieval to enable LLMs to accu- rately select from a large, overlapping, and changing set tools expressed using their APIs and API documentation. We construct, APIBench, a large corpus of APIs with complex and often overlapping functionality by scraping ML APIs (models) from public model hubs. We choose three major model hubs for dataset construction: TorchHub, TensorHub and HuggingFace. We exhaustively include every API call in TorchHub (94 API calls) and TensorHub (696 API calls); For HuggingFace, since the models come in a large number and lots of the models don’t have a specification, we choose the most downloaded 20 models per task category (in a total of 925). We also generate 10 synthetic user question prompts per API using Self-Instruct [ 42]. Thus, each entry in the dataset becomes an instruction reference API pair. We adopt a common AST sub-tree matching technique to evaluate the functional correctness of the generated API. We first parse the generated code into an AST tree, then find a sub-tree whose root node is the API call that we care about (e.g., torch.hub.load ) and use it to index our dataset. We check the functional correctness and hallucination problem for the LLMs, reporting the corresponding accuracy. We then finetune Gorilla, a LLaMA-7B-based model with document retrieval using our dataset. We find that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy as well as reducing hallucination errors. We show an example output in Fig. 1. Further, our retrieval-aware training of Gorilla enables the model to adapt to changes in the API documentation. Finally, we demonstrate Gorilla’s ability to understand and reason about constraints. 22 Related Work Large Language Models Recent strides in the field of LLMs have renovated many downstream domains [ 10,40,48,47], not only in traditional natural language processing tasks but also in program synthesis. Many of these advances are achieved by augmenting pre-trained LLMs by prompting [ 44,14] and instruction fine-tuning [ 11,31,43,15]. Recent open-sourced models like LLaMa [ 40], Alpaca [ 38], and Vicuna [ 9] have furthered the understanding of LLMs and facilitated their experimentation. While our approach, Gorilla, incorporates techniques akin to those mentioned, its primary emphasis is on enhancing the LLMs’ ability to utilize millions of tools, as opposed to refining their conversational skills. Additionally, we pioneer the study of fine-tuning a base model by supplementing it with information retrieval - a first, to the best of our knowledge. Tool Usage The discussion of tool usage within LLMs has seen an upsurge, with models like Toolformer taking the lead [ 33,19,21,26]. Tools often incorporated include web-browsing [ 34], calculators [ 12,39], translation systems [ 39], and Python interpreters [ 14]. While these efforts can be seen as preliminary explorations of marrying LLMs with tool usage, they generally focus on specific tools. Our paper, in contrast, aims to explore a vast array of tools (i.e., API calls) in an open-ended fashion, potentially covering a wide range of applications. With the recent launch of Toolformer [ 33] and GPT-4 [ 29], the importance of API calls has been highlighted, encouraging many works in employing API calls as tooling [ 35,24]. Moreover, the application of API calls in robotics has been explored to some extent [ 41,1]. However, these works primarily aim at showcasing the potential of “prompting” LLMs rather than establishing a systematic method for evaluation and training (including fine-tuning). Our work, on the other hand, concentrates on systematic evaluation and building a pipeline for future use. LLMs for Program Synthesis Harnessing LLMs for program synthesis has historically been a challenging task [ 23,7,45,16,13,20]. Researchers have proposed an array of strategies to prompt LLMs to perform better in coding tasks, including in-context learning [ 44,18,7], task decomposition [ 17,46], and self-debugging [ 8,36]. Besides prompting, there have also been efforts to pretrain language models specifically for code generation [28, 22, 27]. However, these strategies focus on prompting large language models or pre-training them for general program synthesis. In our research, in contrast, we focus on a much restricted domain: the synthesis of linear programs using API calls. General program synthesis, not only is complex, but is also hard to verify and evaluate. API calls, on the other hand, function more like tool usage. This allows the LLM to significantly expand its capabilities without grappling with low-level implementation details. 3 Methodology In this section, we describe APIBench, a comprehensive benchmark constructed from TorchHub, TensorHub, and HuggingFace API Model Cards. We begin by outlining the process of collecting the API dataset and how we generated instruction-answer pairs. We then introduce Gorilla, a novel training paradigm with a information–retriever incorporated into the training and inference pipelines. Finally, we present our AST tree matching evaluation metric. 3.1 Dataset Collection To collect the dataset, we meticulously recorded all online model cards for HuggingFace’s “The Model Hub”, PyTorch Hub, and TensorFlow Hub Models. Throughout the rest of the paper, we call these HuggingFace, Torch Hub, and TensorFlow Hub respectively for brevity. API Documentation The HuggingFace platform hosts and servers about 203,681 models. However, many of them have poor documentation, lack dependencies, have no information in their model card, etc. To filter these out, we pick the top 20 models from each domain. We consider 7 domains in multimodal data, 8 in CV , 12 in NLP, 5 in Audio, 2 in tabular data, and 2 in reinforcement learning. Post filtering, we got a total of 925 models from HuggingFace. TensorFlow Hub is versioned into v1 and v2. The latest version (v2) has 801 models in total, and we process all of them. Post filtering out models, whose mode cards had little to no information, we are left with 626 models. Similar to 3\u001b[0m\u001b[32;1m\u001b[1;3mThe paper titled \"Large Language Model Connected with Massive APIs\" introduces Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 in writing API calls. Gorilla, when combined with a document retriever, demonstrates the ability to adapt to test-time document changes, reducing hallucination errors commonly encountered with LLMs. The paper introduces APIBench, a dataset consisting of HuggingFace, TorchHub, and TensorHub APIs, to evaluate the model's ability accurately. Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy and reduces hallucination errors. The paper emphasizes the importance of empowering LLMs to use tools via API calls to access vast knowledge bases and accomplish complex computational tasks effectively.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# TODO : Change Arxiv Search enging to Semantic Scholar\n","# TODO : error handling when the tool called.\n","# Because below code doesn't work. with the difference of 'Gorilla: '\n","# output = agent_executor.invoke({\"input\": \"summary the abstract of the paper 'Gorilla: Large Language Model Connected with Massive APIs'\"})\n","\n","output = agent_executor.invoke({\"input\": \"summary the abstract of the paper 'Large Language Model Connected with Massive APIs'\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXyHypjmwg-C","outputId":"457f03a8-e5a2-4c16-fbd3-e1493f13a298"},"outputs":[{"name":"stdout","output_type":"stream","text":["('The paper titled \"Large Language Model Connected with Massive APIs\" '\n"," 'introduces Gorilla, a finetuned LLaMA-based model that surpasses the '\n"," 'performance of GPT-4 in writing API calls. Gorilla, when combined with a '\n"," 'document retriever, demonstrates the ability to adapt to test-time document '\n"," 'changes, reducing hallucination errors commonly encountered with LLMs. The '\n"," 'paper introduces APIBench, a dataset consisting of HuggingFace, TorchHub, '\n"," \"and TensorHub APIs, to evaluate the model's ability accurately. Gorilla \"\n"," 'significantly outperforms GPT-4 in terms of API functionality accuracy and '\n"," 'reduces hallucination errors. The paper emphasizes the importance of '\n"," 'empowering LLMs to use tools via API calls to access vast knowledge bases '\n"," 'and accomplish complex computational tasks effectively.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hX5aaMB6wg-D","outputId":"eab79a0c-23ce-4bf7-f2b1-00f46c25ea30"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `recommend_reference` with `{'query': 'Language Models Can Teach Themselves to Use Tools'}`\n","\n","\n","\u001b[0m\u001b[33;1m\u001b[1;3m[{'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0', 'title': 'Language Models are Few-Shot Learners', 'abstract': \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\", 'influentialCitationCount': 3130, 'publicationDate': '2020-05-28', 'intent': 'methodology background result', 'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'}, {'paperId': '094ff971d6a8b8ff870946c9b3ce5aa173617bfb', 'title': 'PaLM: Scaling Language Modeling with Pathways', 'abstract': 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.', 'influentialCitationCount': 307, 'publicationDate': '2022-04-05', 'intent': 'background', 'context': 'Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.).'}, {'paperId': '13a0d8bb38f739990c8cd65a44061c6534f17221', 'title': 'OPT: Open Pre-trained Transformer Language Models', 'abstract': 'Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.', 'influentialCitationCount': 274, 'publicationDate': '2022-05-02', 'intent': 'result', 'context': 'We additionally compare to OPT (66B) (Zhang et al., 2022) and the original davinci variant of GPT-3 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than GPT-J.'}, {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', 'influentialCitationCount': 18170, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'background', 'context': 'As LAMA was originally designed to evaluate masked LMs (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that all examples can be processed in a left-to-right fashion.'}, {'paperId': '9405cc0d6169988371b2755e573cc28650d14dfe', 'title': 'Language Models are Unsupervised Multitask Learners', 'abstract': 'Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.', 'influentialCitationCount': 2826, 'publicationDate': datetime.datetime(9999, 12, 31, 23, 59, 59, 999999), 'intent': 'methodology', 'context': 'To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some recommended reference works related to the paper \"Language Models Can Teach Themselves to Use Tools\":\n","\n","1. **Paper Title:** Language Models are Few-Shot Learners\n","   - **Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. This paper shows that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\n","   - **Publication Date:** 2020-05-28\n","\n","2. **Paper Title:** PaLM: Scaling Language Modeling with Pathways\n","   - **Abstract:** This paper explores the impact of scale on few-shot learning by training a 540-billion parameter, densely activated, Transformer language model called Pathways Language Model PaLM. It achieves breakthrough performance on language understanding and generation benchmarks and demonstrates benefits of scaling in achieving state-of-the-art few-shot learning results.\n","   - **Publication Date:** 2022-04-05\n","\n","3. **Paper Title:** OPT: Open Pre-trained Transformer Language Models\n","   - **Abstract:** OPT presents a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, aiming to be fully and responsibly shared with interested researchers. The paper compares OPT-175B to GPT-3 and discusses the model's performance and carbon footprint.\n","   - **Publication Date:** 2022-05-02\n","\n","4. **Paper Title:** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n","   - **Abstract:** This paper introduces BERT, a language representation model designed for pre-training deep bidirectional representations from unlabeled text. BERT achieves state-of-the-art results on eleven natural language processing tasks and demonstrates the power of pre-training in creating models for various tasks.\n","   - **Publication Date:** Not specified\n","\n","5. **Paper Title:** Language Models are Unsupervised Multitask Learners\n","   - **Abstract:** This paper demonstrates that language models begin to learn natural language processing tasks without explicit supervision when trained on a new dataset of millions of webpages called WebText. The findings suggest a promising path towards building language processing systems that learn tasks from naturally occurring demonstrations.\n","   - **Publication Date:** Not specified\n","\n","These reference works provide valuable insights and background information related to language models and their capabilities in natural language processing tasks.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["output = agent_executor.invoke({\"input\": \"recommend the reference works of the paper 'Language Models Can Teach Themselves to Use Tools'\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEYY1iAWwg-D","outputId":"1260342a-d7d3-4486-94b3-8743f59f62d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["('Here are some recommended reference works related to the paper \"Language '\n"," 'Models Can Teach Themselves to Use Tools\":\\n'\n"," '\\n'\n"," '1. **Paper Title:** Language Models are Few-Shot Learners\\n'\n"," '   - **Abstract:** Recent work has demonstrated substantial gains on many '\n"," 'NLP tasks and benchmarks by pre-training on a large corpus of text followed '\n"," 'by fine-tuning on a specific task. This paper shows that scaling up language '\n"," 'models greatly improves task-agnostic, few-shot performance, sometimes even '\n"," 'reaching competitiveness with prior state-of-the-art fine-tuning '\n"," 'approaches.\\n'\n"," '   - **Publication Date:** 2020-05-28\\n'\n"," '\\n'\n"," '2. **Paper Title:** PaLM: Scaling Language Modeling with Pathways\\n'\n"," '   - **Abstract:** This paper explores the impact of scale on few-shot '\n"," 'learning by training a 540-billion parameter, densely activated, Transformer '\n"," 'language model called Pathways Language Model PaLM. It achieves breakthrough '\n"," 'performance on language understanding and generation benchmarks and '\n"," 'demonstrates benefits of scaling in achieving state-of-the-art few-shot '\n"," 'learning results.\\n'\n"," '   - **Publication Date:** 2022-04-05\\n'\n"," '\\n'\n"," '3. **Paper Title:** OPT: Open Pre-trained Transformer Language Models\\n'\n"," '   - **Abstract:** OPT presents a suite of decoder-only pre-trained '\n"," 'transformers ranging from 125M to 175B parameters, aiming to be fully and '\n"," 'responsibly shared with interested researchers. The paper compares OPT-175B '\n"," \"to GPT-3 and discusses the model's performance and carbon footprint.\\n\"\n"," '   - **Publication Date:** 2022-05-02\\n'\n"," '\\n'\n"," '4. **Paper Title:** BERT: Pre-training of Deep Bidirectional Transformers '\n"," 'for Language Understanding\\n'\n"," '   - **Abstract:** This paper introduces BERT, a language representation '\n"," 'model designed for pre-training deep bidirectional representations from '\n"," 'unlabeled text. BERT achieves state-of-the-art results on eleven natural '\n"," 'language processing tasks and demonstrates the power of pre-training in '\n"," 'creating models for various tasks.\\n'\n"," '   - **Publication Date:** Not specified\\n'\n"," '\\n'\n"," '5. **Paper Title:** Language Models are Unsupervised Multitask Learners\\n'\n"," '   - **Abstract:** This paper demonstrates that language models begin to '\n"," 'learn natural language processing tasks without explicit supervision when '\n"," 'trained on a new dataset of millions of webpages called WebText. The '\n"," 'findings suggest a promising path towards building language processing '\n"," 'systems that learn tasks from naturally occurring demonstrations.\\n'\n"," '   - **Publication Date:** Not specified\\n'\n"," '\\n'\n"," 'These reference works provide valuable insights and background information '\n"," 'related to language models and their capabilities in natural language '\n"," 'processing tasks.')\n"]}],"source":["pprint(output['output'])"]},{"cell_type":"markdown","metadata":{"id":"bbxP_Psuwg-E"},"source":["### Todo : Make Agent with Llama"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wULDS5ozwg-F"},"outputs":[],"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AvoAOTmwg-F","outputId":"fc02781b-fbe0-47da-c521-c555fcc589bf"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading shards:  25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]"]}],"source":["# we can use 262k version of Llama!\n","model_name = 'gradientai/Llama-3-8B-Instruct-262k'\n","\n","dtype = \"float16\"\n","if torch.cuda.is_bf16_supported():\n","    dtype = \"bfloat16\"\n","\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=getattr(torch, dtype), quantization_config=quantization_config,\n","                                             device_map=\"auto\",  cache_dir=os.getenv(\"HF_HOME\", \"~/.cache/huggingface\"))\n","model.eval()\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmpO15ehwg-G"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### HTML"],"metadata":{"id":"zrfI9u3YHYxw"}},{"cell_type":"code","source":["!pip install beautifulsoup4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEMQu5lAHzeE","executionInfo":{"status":"ok","timestamp":1719221826958,"user_tz":-540,"elapsed":14387,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"4b4fbbd7-0264-4615-95e5-7210b8f6627a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup"],"metadata":{"id":"wj5T-OOHHu32","executionInfo":{"status":"ok","timestamp":1719221826959,"user_tz":-540,"elapsed":7,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class GetPaper:\n","    def __init__(self, ss_api_key):\n","        self.ss_api_key = ss_api_key\n","\n","    def get_paper_info_by_title(self, title):\n","        \"\"\"논문의 제목으로 정보를 가져오는 함수\"\"\"\n","        # Define the API endpoint URL\n","        url = 'https://api.semanticscholar.org/graph/v1/paper/search?query={}&fields=paperId,title,abstract,authors,citations,fieldsOfStudy,influentialCitationCount,isOpenAccess,openAccessPdf,publicationDate,publicationTypes,references,venue'\n","\n","        headers = {'x-api-key': self.ss_api_key}\n","        response = requests.get(url.format(title), headers=headers).json()\n","\n","        if response.get('data'):\n","            paper = response['data'][0]\n","            return paper\n","        else:\n","            return None\n","\n","    def get_ar5iv_url(self, paper):\n","        \"논문의 ar5iv 주소를 받아오는 함수\"\n","        external_ids = paper.get('openAccessPdf', {})\n","        arxiv_id = external_ids.get('url')\n","        if 'http' in arxiv_id:\n","            arxiv_id = arxiv_id.split('/')[-1]\n","            return f\"https://ar5iv.org/abs/{arxiv_id}\"\n","        else:\n","            return None\n","\n","    def get_html_from_url(self, url, sections:list=None):\n","        response = requests.get(url)\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        all_sections = soup.find_all('section')\n","\n","        # h1부터 h6까지 태그 추출\n","        headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n","\n","        # 태그와 내용을 계층 구조로 저장\n","        header_list = [(header.name, header.text.strip()) for header in headers]\n","\n","        for tag, text in header_list:\n","            level = int(tag[1])  # 태그에서 레벨을 추출 (h1 -> 1, h2 -> 2, ...)\n","            print('  ' * (level - 1) + text)\n","\n","        return header_list\n","\n","    def extract_text_under_headers(url, text_list):\n","        # 페이지 요청 및 읽기\n","        response = requests.get(url)\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        # 결과를 저장할 변수\n","        results = []\n","\n","        # 텍스트 리스트를 순회하며 각 텍스트에 해당하는 헤더와 그 아래의 텍스트를 추출\n","        for text in text_list:\n","            header_tag = soup.find(lambda tag: tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6'] and text in tag.get_text(strip=True))\n","            if header_tag:\n","                header_text = header_tag.get_text(strip=True)\n","                header_level = int(header_tag.name[1])\n","                current_header = {'tag': header_tag.name, 'text': header_text, 'subsections': []}\n","                results.append(current_header)\n","\n","                next_element = header_tag.find_next_sibling()\n","                while next_element:\n","                    if next_element.name and next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n","                        next_level = int(next_element.name[1])\n","                        if next_level <= header_level:\n","                            break\n","\n","                        # If it's a tag and within our header range\n","                        if next_element.name and next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n","                            next_text = next_element.get_text(strip=True)\n","                            next_subheader = {'tag': next_element.name, 'text': next_text, 'subsections': []}\n","                            current_header['subsections'].append(next_subheader)\n","                            current_header = next_subheader\n","                            header_level = next_level\n","                    else:\n","                        if 'subsections' not in current_header:\n","                            current_header['subsections'] = []\n","                        current_header['subsections'].append({'tag': 'p', 'text': next_element.get_text(strip=True)})\n","\n","                    next_element = next_element.find_next_sibling()\n","\n","        return results\n","\n","    def print_hierarchy(self, hierarchy, level=0):\n","        for section in hierarchy:\n","            print('  ' * level + f\"{section['text']}\")\n","            if 'subsections' in section:\n","                self.print_hierarchy(section['subsections'], level + 1)\n","\n","\n","    def search_paper_by_title(self, title):\n","        '''\n","        INPUT : title of the paper\n","        OUTPUT : list of sections in paper\n","        '''\n","        paper = self.get_paper_info_by_title(title)\n","        url = self.get_ar5iv_url(paper)\n","        # todo : Error Handling when no ar5iv URL.\n","        # arxiv pdf를 다운받고, pdf읽는 식으로 handle가능.\n","        if (url == None):\n","            raise \"NO ar5iv URL is founded\"\n","        html_headers = self.get_html_from_url(url)\n","        return html_headers\n","\n","    def load_paper_by_sections(self, title, sections):\n","        '''\n","        INPUT : title of paper,\n","                list of sections in paper\n","        OUTPUT : text of the paper\n","        '''\n","        paper = self.get_paper_info_by_title(title)\n","        url = self.get_ar5iv_url(paper)\n","        text = self.extract_text_under_headers(url, sections)\n","        return text\n"],"metadata":{"id":"DjVjHTaxSIL4","executionInfo":{"status":"ok","timestamp":1719222107793,"user_tz":-540,"elapsed":512,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["paperModule = GetPaper(ss_api_key)"],"metadata":{"id":"3_YcqSB_SzuK","executionInfo":{"status":"ok","timestamp":1719222107793,"user_tz":-540,"elapsed":3,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["text = paperModule.load_paper_by_sections(title = 'Large Language Model Connected with Massive APIs', sections=['Methodology', 'Conclusion'])"],"metadata":{"id":"Un4HZzRAnn4v","executionInfo":{"status":"ok","timestamp":1719222112074,"user_tz":-540,"elapsed":1672,"user":{"displayName":"최유민","userId":"14691279395225646105"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5LhZc-_Jn44v","executionInfo":{"status":"ok","timestamp":1719222114957,"user_tz":-540,"elapsed":4,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"34a4bcdd-42c3-4899-a356-c6ed155b06e0"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'tag': 'h2',\n","  'text': '3Methodology',\n","  'subsections': [{'tag': 'p',\n","    'text': 'Figure 3:Gorilla: A system for enabling LLMs to interact with APIs.The upper half represents the training procedure as described in Sec3. This is the most exhaustive API data-set for ML to the best of our knowledge. During inference (lower half), Gorilla supports two modes - with retrieval, and zero-shot. In this example, it is able to suggest the right API call for generating the image from the user’s natural language query.'},\n","   {'tag': 'p',\n","    'text': 'In this section, we describe APIBench, a comprehensive benchmark constructed from TorchHub, TensorHub, and HuggingFace API Model Cards. We begin by outlining the process of collecting the API dataset and how we generated instruction-answer pairs. We then introduce Gorilla, a novel training paradigm with a information–retriever incorporated into the training and inference pipelines. Finally, we present our AST tree matching evaluation metric.'},\n","   {'tag': 'p',\n","    'text': '3.1Dataset CollectionTo collect the dataset, we meticulously recorded all online model cards for HuggingFace’s “The Model Hub”, PyTorch Hub, and TensorFlow Hub Models. Throughout the rest of the paper, we call these HuggingFace, Torch Hub, and TensorFlow Hub respectively for brevity.API DocumentationThe HuggingFace platform hosts and servers about 203,681 models. However, many of them have poor documentation, lack dependencies, have no information in their model card, etc. To filter these out, we pick the top 20 models from each domain.\\nWe consider 7 domains in multimodal data, 8 in CV, 12 in NLP, 5 in Audio, 2 in tabular data, and 2 in reinforcement learning. Post filtering, we got a total of 925 models from HuggingFace. TensorFlow Hub is versioned into v1 and v2. The latest version (v2) has 801 models in total, and we process all of them. Post filtering out models, whose mode cards had little to no information, we are left with 626 models. Similar to TensorFlow Hub, we get 95 models from Torch Hub.\\nWe then converted the model cards for each of these 1,645 API calls into a json object with the following fields: {domain, framework, functionality, api_name, api_call, api_arguments, environment_requirements, example_code, performance, and description.}. We provide more information in the Appendix. These fields were chose to generalize beyond the API calls within ML domain, to other domains, includin RESTful API calls.Instruction GenerationGuided by the self-instruct paradigm[42], we employed GPT-4 to generate synthetic instruction data.\\nWe provided three in-context examples, along with a reference API documentation, and tasked the model with generating real-world use cases that call upon the API. We specifically instructed the model to refrain from using any API names or hints when creating instructions. We constructed six examples (Instruction-API pairs) for each of the three model hubs. These 18 points, were the only hand-generated or modified data. For each of our 1,645 API datapoints, we sample 3 of 6 corresponding instruction examples to generate a total of 10 instruction-api pairs as demonstrated in Figure3. We would like to highlight that we only need to employ GPT-4 to generate the instructions and this can be swapped with open-source alternatives such as LLaMA, Alpaca, etc.'},\n","   {'tag': 'p',\n","    'text': '3.2GorillaOur model Gorilla, is retrieve-aware finetuned LLaMA-7B model, specifically for API calls. As shown in Fig3, we employ self-instruct to generate {instruction, API} pairs. To fine-tune LLaMA, we convert this to a user-agent chat-style conversation, where each data-point is a conversation with one round each for the user and the agent. We then perform standard instruction finetuning on the base LLaMA-7B model. For our experiments, we train Gorilla with and without the retriever.API Call with ConstraintsAPI calls often come with inherent constraints. These constraints necessitate that the LLM not only comprehend the functionality of the API call but also categorize the calls according to different constraint parameters.\\nThis requirement introduces an additional layer of complexity to the process, demanding a more nuanced understanding from the LLM.\\nSpecifically, for machine learning API calls, two common sets of constraints are: parameter size and a lower bound on accuracy. Consider, for instance, the following prompt:‘‘Invoke an image classification model that uses less than 10M parameters, but maintains an ImageNet accuracy of at least 70%’’. Such a prompt presents a substantial challenge for the LLM to accurately interpret and respond to. Not only must the LLM understand the user’s functional description, but it also needs to reason about the various constraints embedded within the request. This challenge underlines the intricate demands placed on LLMs in real-world API calls. It is not sufficient for the model to merely comprehend the basic functionality of an API call; it must also be capable of navigating the complex landscape of constraints that accompany such calls. These observations necessitate the need to fine-tune an LLM for APIs.Retriever-Aware trainingFor training with retriever, the instruction-tuned dataset, also has an additional\"Use this API documentation for reference: <retrieved_API_doc_JSON>\"appended to the user prompt. Through this, we aim to teach the LLM to parse the second half of the question to answer the first half. We demonstrate that this a) makes the LLM adapt to test-time changes in API documentation, and b) improves performance from in-context learning, and finally c) show that it reduces hallucination error.Surprisingly, we find that augmenting a LLM with retrieval, does not always lead to improved performance, and can at-times hurt performance. We share more insights along with details in Sec4.Gorilla InferenceDuring Inference, the user provides the prompt in natural language (Fig:3). This can be for a simple task (e.g,“I would like to identify the objects in an image”), or they can specify a vague goal, (.e.g,“I am going to the zoo, and would like to track animals”). Gorilla, similar to training, can be used for inference in two modes: zero-shot and with retrieval. In zero-shot, this prompt (with NO further prompt tuning) is fed to the Gorilla LLM model when then returns the API call that will help in accomplishing the task and/or goal. In retrieval mode, the retriever (either of BM25 or GPT-Index) first retrieves the most up-to-date API documentation stored in the API Database. This is then concatenated to the user prompt along with the messageUse this API documentation for reference:before feeding it to Gorilla. The output of Gorilla is an API to be invoked. Besides the concatenation as described, we doNOfurther prompt tuning in our system. While we do have a system to execute these APIs, that is not a focus of this paper.'},\n","   {'tag': 'p',\n","    'text': '3.3Verifying APIsInductive program synthesis, where a program is synthesized to satisfy test cases, has found success in several avenues[4,25].\\nHowever, test cases fall short when evaluating API calls, as it is often hard to verify the semantic correctness of the code. For example, consider the task of classifying an image.\\nThere are over 40 different models that can be used for the task. Even if we were to narrow down to a single family of Densenet, there are four different configurations possible. Hence, there exist multiple correct answers and it is hard to tell if the API being used is functionally equivalent to the reference API by unit tests. Thus, to evaluate the performance of our model, we compare their functional equivalence using the dataset we collected. To trace which API in the dataset is the LLM calling, we adopt the AST tree-matching strategy. Since we only consider one API call in this paper, checking if the AST of the candidate API call is a sub-tree of the reference API call reveals which API is being used in the dataset.Identifying and even defining hallucinations can be challenging.\\nWe use the AST matching process to directly identify the hallucinations.\\nWe define a hallucination as an API call that is not a sub-tree of any API in the database – invoking an entirely imagined tool.\\nThis form of hallucination is distinct from invoking an API incorrectly which we instead define as an error.AST Sub-Tree MatchingWe perform AST sub-tree matching to identify which API in our dataset is the LLM calling. Since each API call can have many arguments, we need to match on each of these arguments. Further, since, Python allows for default arguments, for each API, we define which arguments to match in our database. For example, we checkrepo_or_dirandmodelarguments in our function call. In this way, we can easily check if the argument matches the reference API or not. Please refer to Fig.4for more details. In this example, Gorilla returns a torch API call. We first build the tree, and verify that it matches a subtree in our dataset along nodestorch.hub.load,pytorch/vision, anddensenet121. But, we don’t check for match along leaf nodepretrained = Truesince that is an optional python argument.Figure 4:AST Sub-Tree Matching to evaluate API calls.On the left is an API call returned by Gorilla. We first build the associated API tree. We then compare this to our dataset, to see if the API dataset has a subtree match. In the above example, the matching subtree is highlighted in brown, signifying that the API call is indeed correct.Pretrained=Trueis an optional argument.'}]},\n"," {'tag': 'h2',\n","  'text': '5Conclusion',\n","  'subsections': [{'tag': 'p',\n","    'text': 'LLMs are swiftly gaining popularity across diverse domains. In our study, we spotlight techniques designed to enhance the LLM’s ability to accurately identify the appropriate API for a specific task—a significant but often overlooked aspect in the advancement of this technology. Since APIs function as a universal language enabling diverse systems to communicate effectively, their correct usage can boost the ability of LLMs to interact with tools in the wider world.\\nIn this paper, we propose Gorilla, a new novel pipeline for finetuning LLMs to call APIs. The finetuned model’s performance surpasses prompting the state-of-the-art LLM (GPT-4) in three massive datasets we collected. Gorilla generates reliable API calls to ML models without hallucination, demonstrates an impressive capability to adapt to test-time API usage changes, and can satisfy constraints while picking APIs.'}]}]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["header = paperModule.search_paper_by_title(title)\n","content = paperModule.extract_text_under_headers(url, text_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjA8_PAMHZno","executionInfo":{"status":"ok","timestamp":1719217130661,"user_tz":-540,"elapsed":4155,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"253f4430-f0b4-489f-96ca-7244e8bee1a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gorilla: Large Language Model Connected with Massive APIs\n","          Abstract\n","  1 Introduction\n","  2 Related Work\n","        Large Language Models\n","        Tool Usage\n","        LLMs for Program Synthesis\n","  3 Methodology\n","    3.1 Dataset Collection\n","        API Documentation\n","        Instruction Generation\n","    3.2 Gorilla\n","        API Call with Constraints\n","        Retriever-Aware training\n","        Gorilla Inference\n","    3.3 Verifying APIs\n","        AST Sub-Tree Matching\n","  4 Evaluation\n","        Baselines\n","        Retrievers\n","    4.1 AST Accuracy on API call\n","        Finetuning without Retrieval\n","        Finetuning with Retrieval\n","        Hallucination with LLM\n","    4.2 Test-Time Documentation Change\n","    4.3 API Call with Constraints\n","  5 Conclusion\n","  6 Limitations & Social Impacts\n","  7 Acknowledgement\n","  References\n","  8 Appendix\n","    8.1 Dataset Details\n","        Domain Classification\n","        API Call Task\n","        API Provider Component\n","        Explanation Element\n","        Code\n","    8.2 Gorilla Details\n","        Data\n","        Training\n","    8.3 Performance Comparison\n","      8.3.1 Evaluation\n","      8.3.2 Hallucination\n"]}]},{"cell_type":"code","source":["content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TvioYpFU4UR","executionInfo":{"status":"ok","timestamp":1719217133635,"user_tz":-540,"elapsed":711,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"b4b8de69-8328-4fed-a61a-d0d08be8de11"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'tag': 'h1',\n","  'text': 'Gorilla: Large Language Model Connected with Massive APIs',\n","  'subsections': [{'tag': 'p',\n","    'text': 'Shishir G. Patil1Tianjun Zhang1,∗Xin Wang2Joseph E. Gonzalez11UC Berkeley2Microsoft Researchsgp@berkeley.eduEqual contribution.'},\n","   {'tag': 'p',\n","    'text': 'AbstractLarge Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model’s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs.\\nGorilla’s code, model, data, and demo are available athttps://gorilla.cs.berkeley.edu'},\n","   {'tag': 'p',\n","    'text': '1IntroductionRecent advances in large language models (LLMs)[10,5,32,6,29,30]have enabled significant new capabilities including natural dialogue, mathematical reasoning, and program synthesis.\\nHowever, despite these advances, LLMs are still fundamentally limited by the information they can store in a fixed set of weights and the things they can compute using a static computation graph and limited context.\\nFurthermore, as the world changes, LLMs require retraining to update their knowledge and reasoning capabilities.By empowering LLMs to use tools[33], we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks.\\nBy providing access to search technologies and databases,[26,39,37]demonstrated that we can augment LLMs to address a significantly larger and more dynamic knowledge space.\\nSimilarly, by providing access to computational tools,[39,2]demonstrated that LLMs can accomplish complex computational tasks.\\nConsequently, leading LLM providers[29], have started to integrate plugins to allow LLMs to invoke external tools through APIs.This transition from a small set of hand-coded tools, to the ability to invoke a vast space of changing cloud APIs could transform LLMs into the primary interface to computing infrastructure and the web.\\nTasks ranging from booking an entire vacation to hosting a conference, could become as simple as talking to an LLM that has access to the flight, car rental, hotel, catering, and entertainment web APIs.\\nHowever, much of the prior work[35,24]integrating tools into LLMs considered a small well documented set of APIs that can be easily injected into the prompt.Supporting a web scale collection of potentially millions of changing APIs requires rethinking our approach to how we integrate tools.\\nIt is not longer possible to describe the full set of APIs in a single context.\\nMany of the APIs will have overlapping functionality with nuanced limitations and constraints.\\nSimply evaluating LLMs in this new setting requires new benchmarks.Figure 1:Examples of API calls. Example API calls generated by GPT-4[29], Claude[3], and Gorilla for the given prompt. In this example, GPT-4 presents a model that doesn’t exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call.In this paper, we explore the use of self-instruct fine-tuning and retrieval to enable LLMs to accurately select from a large, overlapping, and changing set tools expressed using their APIs and API documentation.\\nWe construct, APIBench, a large corpus of APIs with complex and often overlapping functionality by scraping ML APIs (models) from public model hubs.\\nWe choose three major model hubs for dataset construction: TorchHub, TensorHub and HuggingFace. We exhaustively include every API call in TorchHub (94 API calls) and TensorHub (696 API calls); For HuggingFace, since the models come in a large number and lots of the models don’t have a specification, we choose the most downloaded 20 models per task category (in a total of 925). We also generate 10 synthetic user question prompts per API using Self-Instruct[42]. Thus, each entry in the dataset becomes an instruction reference API pair. We adopt a common AST sub-tree matching technique to evaluate the functional correctness of the generated API. We first parse the generated code into an AST tree, then find a sub-tree whose root node is the API call that we care about (e.g.,torch.hub.load) and use it to index our dataset. We check the functional correctness and hallucination problem for the LLMs, reporting the corresponding accuracy.We then finetune Gorilla, a LLaMA-7B-based model with document retrieval using our dataset. We find that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy as well as reducing hallucination errors. We show an example output in Fig.1. Further, our retrieval-aware training of Gorilla enables the model to adapt to changes in the API documentation. Finally, we demonstrate Gorilla’s ability to understand and reason about constraints.Figure 2:Accuracy (vs) hallucinationin four settings, that is,zero-shot(i.e., without any retriever), andwith retrievers.BM25andGPTare commonly used retrievers and theoracleretriever returns relevant documents at 100%, indicating an upper bound. Higher in the graph (higher accuracy) and to the left is better (lower hallucination). Across the entire dataset, our model, Gorilla, improves accuracy while reducing hallucination.'},\n","   {'tag': 'p',\n","    'text': '2Related WorkLarge Language ModelsRecent strides in the field of LLMs have renovated many downstream domains[10,40,48,47], not only in traditional natural language processing tasks but also in program synthesis. Many of these advances are achieved by augmenting pre-trained LLMs by prompting[44,14]and instruction fine-tuning[11,31,43,15]. Recent open-sourced models like\\nLLaMa[40], Alpaca[38], and Vicuna[9]have furthered the understanding of LLMs and facilitated their experimentation. While our approach, Gorilla, incorporates techniques akin to those mentioned, its primary emphasis is on enhancing the LLMs’ ability to utilize millions of tools, as opposed to refining their conversational skills. Additionally, we pioneer the study of fine-tuning a base model by supplementing it with information retrieval - a first, to the best of our knowledge.Tool UsageThe discussion of tool usage within LLMs has seen an upsurge, with models like Toolformer taking the lead[33,19,21,26]. Tools often incorporated include web-browsing[34], calculators[12,39], translation systems[39], and Python interpreters[14].\\nWhile these efforts can be seen as preliminary explorations of marrying LLMs with tool usage, they generally focus on specific tools. Our paper, in contrast, aims to explore a vast array of tools (i.e., API calls) in an open-ended fashion, potentially covering a wide range of applications.With the recent launch of Toolformer[33]and GPT-4[29], the importance of API calls has been highlighted, encouraging many works in employing API calls as tooling[35,24]. Moreover, the application of API calls in robotics has been explored to some extent[41,1]. However, these works primarily aim at showcasing the potential of “prompting” LLMs rather than establishing a systematic method for evaluation and training (including fine-tuning). Our work, on the other hand, concentrates on systematic evaluation and building a pipeline for future use.LLMs for Program SynthesisHarnessing LLMs for program synthesis has historically been a challenging task[23,7,45,16,13,20]. Researchers have proposed an array of strategies to prompt LLMs to perform better in coding tasks, including in-context learning[44,18,7], task decomposition[17,46], and self-debugging[8,36]. Besides prompting, there have also been efforts to pretrain language models specifically for code generation[28,22,27].However, these strategies focus on prompting large language models or pre-training them for general program synthesis. In our research, in contrast, we focus on a much restricted domain: the synthesis of linear programs using API calls. General program synthesis, not only is complex, but is also hard to verify and evaluate. API calls, on the other hand, function more like tool usage. This allows the LLM to significantly expand its capabilities without grappling with low-level implementation details.'},\n","   {'tag': 'p',\n","    'text': '3MethodologyFigure 3:Gorilla: A system for enabling LLMs to interact with APIs.The upper half represents the training procedure as described in Sec3. This is the most exhaustive API data-set for ML to the best of our knowledge. During inference (lower half), Gorilla supports two modes - with retrieval, and zero-shot. In this example, it is able to suggest the right API call for generating the image from the user’s natural language query.In this section, we describe APIBench, a comprehensive benchmark constructed from TorchHub, TensorHub, and HuggingFace API Model Cards. We begin by outlining the process of collecting the API dataset and how we generated instruction-answer pairs. We then introduce Gorilla, a novel training paradigm with a information–retriever incorporated into the training and inference pipelines. Finally, we present our AST tree matching evaluation metric.3.1Dataset CollectionTo collect the dataset, we meticulously recorded all online model cards for HuggingFace’s “The Model Hub”, PyTorch Hub, and TensorFlow Hub Models. Throughout the rest of the paper, we call these HuggingFace, Torch Hub, and TensorFlow Hub respectively for brevity.API DocumentationThe HuggingFace platform hosts and servers about 203,681 models. However, many of them have poor documentation, lack dependencies, have no information in their model card, etc. To filter these out, we pick the top 20 models from each domain.\\nWe consider 7 domains in multimodal data, 8 in CV, 12 in NLP, 5 in Audio, 2 in tabular data, and 2 in reinforcement learning. Post filtering, we got a total of 925 models from HuggingFace. TensorFlow Hub is versioned into v1 and v2. The latest version (v2) has 801 models in total, and we process all of them. Post filtering out models, whose mode cards had little to no information, we are left with 626 models. Similar to TensorFlow Hub, we get 95 models from Torch Hub.\\nWe then converted the model cards for each of these 1,645 API calls into a json object with the following fields: {domain, framework, functionality, api_name, api_call, api_arguments, environment_requirements, example_code, performance, and description.}. We provide more information in the Appendix. These fields were chose to generalize beyond the API calls within ML domain, to other domains, includin RESTful API calls.Instruction GenerationGuided by the self-instruct paradigm[42], we employed GPT-4 to generate synthetic instruction data.\\nWe provided three in-context examples, along with a reference API documentation, and tasked the model with generating real-world use cases that call upon the API. We specifically instructed the model to refrain from using any API names or hints when creating instructions. We constructed six examples (Instruction-API pairs) for each of the three model hubs. These 18 points, were the only hand-generated or modified data. For each of our 1,645 API datapoints, we sample 3 of 6 corresponding instruction examples to generate a total of 10 instruction-api pairs as demonstrated in Figure3. We would like to highlight that we only need to employ GPT-4 to generate the instructions and this can be swapped with open-source alternatives such as LLaMA, Alpaca, etc.3.2GorillaOur model Gorilla, is retrieve-aware finetuned LLaMA-7B model, specifically for API calls. As shown in Fig3, we employ self-instruct to generate {instruction, API} pairs. To fine-tune LLaMA, we convert this to a user-agent chat-style conversation, where each data-point is a conversation with one round each for the user and the agent. We then perform standard instruction finetuning on the base LLaMA-7B model. For our experiments, we train Gorilla with and without the retriever.API Call with ConstraintsAPI calls often come with inherent constraints. These constraints necessitate that the LLM not only comprehend the functionality of the API call but also categorize the calls according to different constraint parameters.\\nThis requirement introduces an additional layer of complexity to the process, demanding a more nuanced understanding from the LLM.\\nSpecifically, for machine learning API calls, two common sets of constraints are: parameter size and a lower bound on accuracy. Consider, for instance, the following prompt:‘‘Invoke an image classification model that uses less than 10M parameters, but maintains an ImageNet accuracy of at least 70%’’. Such a prompt presents a substantial challenge for the LLM to accurately interpret and respond to. Not only must the LLM understand the user’s functional description, but it also needs to reason about the various constraints embedded within the request. This challenge underlines the intricate demands placed on LLMs in real-world API calls. It is not sufficient for the model to merely comprehend the basic functionality of an API call; it must also be capable of navigating the complex landscape of constraints that accompany such calls. These observations necessitate the need to fine-tune an LLM for APIs.Retriever-Aware trainingFor training with retriever, the instruction-tuned dataset, also has an additional\"Use this API documentation for reference: <retrieved_API_doc_JSON>\"appended to the user prompt. Through this, we aim to teach the LLM to parse the second half of the question to answer the first half. We demonstrate that this a) makes the LLM adapt to test-time changes in API documentation, and b) improves performance from in-context learning, and finally c) show that it reduces hallucination error.Surprisingly, we find that augmenting a LLM with retrieval, does not always lead to improved performance, and can at-times hurt performance. We share more insights along with details in Sec4.Gorilla InferenceDuring Inference, the user provides the prompt in natural language (Fig:3). This can be for a simple task (e.g,“I would like to identify the objects in an image”), or they can specify a vague goal, (.e.g,“I am going to the zoo, and would like to track animals”). Gorilla, similar to training, can be used for inference in two modes: zero-shot and with retrieval. In zero-shot, this prompt (with NO further prompt tuning) is fed to the Gorilla LLM model when then returns the API call that will help in accomplishing the task and/or goal. In retrieval mode, the retriever (either of BM25 or GPT-Index) first retrieves the most up-to-date API documentation stored in the API Database. This is then concatenated to the user prompt along with the messageUse this API documentation for reference:before feeding it to Gorilla. The output of Gorilla is an API to be invoked. Besides the concatenation as described, we doNOfurther prompt tuning in our system. While we do have a system to execute these APIs, that is not a focus of this paper.3.3Verifying APIsInductive program synthesis, where a program is synthesized to satisfy test cases, has found success in several avenues[4,25].\\nHowever, test cases fall short when evaluating API calls, as it is often hard to verify the semantic correctness of the code. For example, consider the task of classifying an image.\\nThere are over 40 different models that can be used for the task. Even if we were to narrow down to a single family of Densenet, there are four different configurations possible. Hence, there exist multiple correct answers and it is hard to tell if the API being used is functionally equivalent to the reference API by unit tests. Thus, to evaluate the performance of our model, we compare their functional equivalence using the dataset we collected. To trace which API in the dataset is the LLM calling, we adopt the AST tree-matching strategy. Since we only consider one API call in this paper, checking if the AST of the candidate API call is a sub-tree of the reference API call reveals which API is being used in the dataset.Identifying and even defining hallucinations can be challenging.\\nWe use the AST matching process to directly identify the hallucinations.\\nWe define a hallucination as an API call that is not a sub-tree of any API in the database – invoking an entirely imagined tool.\\nThis form of hallucination is distinct from invoking an API incorrectly which we instead define as an error.AST Sub-Tree MatchingWe perform AST sub-tree matching to identify which API in our dataset is the LLM calling. Since each API call can have many arguments, we need to match on each of these arguments. Further, since, Python allows for default arguments, for each API, we define which arguments to match in our database. For example, we checkrepo_or_dirandmodelarguments in our function call. In this way, we can easily check if the argument matches the reference API or not. Please refer to Fig.4for more details. In this example, Gorilla returns a torch API call. We first build the tree, and verify that it matches a subtree in our dataset along nodestorch.hub.load,pytorch/vision, anddensenet121. But, we don’t check for match along leaf nodepretrained = Truesince that is an optional python argument.Figure 4:AST Sub-Tree Matching to evaluate API calls.On the left is an API call returned by Gorilla. We first build the associated API tree. We then compare this to our dataset, to see if the API dataset has a subtree match. In the above example, the matching subtree is highlighted in brown, signifying that the API call is indeed correct.Pretrained=Trueis an optional argument.'},\n","   {'tag': 'p',\n","    'text': '4EvaluationWe carried out an array of experiments on our collected dataset, benchmarking our model Gorilla with other models, and exploring how different retrieval methods may impact the performance of the model in making API calls. We then demonstrate that Gorilla can easily adapt to test-time changes in API documentation. In addition, we assess Gorilla’s ability to reason about API calls under constraints. Lastly, we examined how integrating different retrieval methods during training influences the model’s final performance.BaselinesPrimarily, we compare Gorilla with state-of-the-art language models in a zero-shot setting. The models under consideration include: GPT-4 by OpenAI, we use thegpt-4-0314checkpoint; GPT-3.5-turbo with thegpt-3.5-turbo-0301checkpoint, both of which are RLHF-tuned model specifically designed for conversation; Claude withclaude-v1checkpoint, a language model by Anthropic, renowned for its lengthy context capabilities; LLaMA-7B, a large language model by Meta and the finest open-source model to date.RetrieversThe termZero-shot(abbreviated as 0-shot in tables) refers to scenarios where no retriever is used. The sole input to the model is the user’s natural language prompt. ForBM25, we consider each API as a separate document. During retrieval, we use the user’s query to search the index and fetch the most relevant (top-1) API.\\nThis API is concatenated with the user’s prompt to query the LLMs. Similarly, GPT-Index refers to the retrieval modeltext-davinci-003from OpenAI. Like BM25, each API call is indexed as an individual document, and the most relevant document, given a user query, is retrieved and appended to the user prompt. Lastly, we include an Oracle retriever, which serves two purposes: first, to identify the potential for performance improvement through more efficient retrievers, and second, to assist users who know which API to use but may need to help invoking it.\\nIn all cases, when a retriever is used, it is appended to the user’s prompt as follows:<user_prompt>Use this API documentation for reference:<retrieved_API_doc_JSON>.\\nThe dataset for these evaluations is detailed in Sec3. We emphasize that we have maintained a holdout test set on which we report our findings. The holdout test set was created by dividing the self-instruct dataset’s instruction, API pairs into training and testing sets.4.1AST Accuracy on API callFigure 5:Accuracy with GPT-retriever.Gorilla outperforms on Torch Hub and Hugging-Face while matching performance on Tensorflow Hub for all existing SoTA LLMs - closed source, and open source.We first demonstrate the results for the AST accuracy for different models. We present the results in Tab.1. We test each model for different retriever settings defined above. We report the overall accuracy, the error by hallucination and the error by selecting wrong API call. Note that for TorchHub and TensorHub, we evaluate all the models using AST tree accuracy score. However, for HuggingFace, since the dataset is not exhaustive, for all the models except Gorilla, we only check if they can provide the correct domain names. So this problem reduces to picking one of the multiple choices.Finetuning without RetrievalIn Tab.1we show that lightly fine-tuned Gorilla gets the state-of-the-art performance zero-shot over all the models, 20.43% better than GPT-4 and 10.75% better than ChatGPT. When compared to other open-source models LLAMA, the improvement is as big as 83%. his suggests quantitatively, that finetuning is better than retrieval, at-least in our scope.In addition, we found that finetuning without retriever and putting ground truth retriever in evaluation time rarely helps the performance: 0.88% worse in TensorHub and 0.97% better in HuggingFace. If we put BM25 or GPT-Index as retriever, results will be significantly dropped: 21.50% in Torch Hub and 47.57% in HuggingFace. The result illustrates that adding a non-optimal retriever at test time will sometime misguide the model and result in more errors. We will discuss an interesting ablation on how finetuning with the retriever will help the performance in the next paragraph.Table 1:Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIsLLM (retriever)TorchHubHuggingFaceTensorFlow Huboverall↑↑\\\\uparrowhallu↓↓\\\\downarrowerr↓↓\\\\downarrowoverall↑↑\\\\uparrowhallu↓↓\\\\downarrowerr↓↓\\\\downarrowoverall↑↑\\\\uparrowhallu↓↓\\\\downarrowerr↓↓\\\\downarrowLLAMA (0-shot)010000.0097.572.4301000GPT-3.5 (0-shot)48.3818.8132.7916.8135.7347.4641.7547.8810.36GPT-4 (0-shot)38.7036.5524.719.8037.1643.0318.2078.653.13Claude (0-shot)18.8165.5915.596.1977.6516.159.1988.462.33Gorilla (0-shot)59.136.9833.8771.6810.9517.3683.795.4010.80LLAMA (BM-25)8.6076.8814.513.0077.9919.028.9077.3713.72GPT-3.5 (BM-25)38.176.9854.8317.268.3074.4454.163.6442.18GPT-4 (BM-25)35.4811.2953.2216.4815.9367.5934.0137.0828.90Claude (BM-25)39.785.3754.8314.6015.8269.5835.1821.1643.64Gorilla (BM-25)40.324.3055.3717.036.4276.5541.892.7755.32LLAMA (GPT-Index)14.5175.89.6710.1875.6614.2015.6277.666.71GPT-3.5 (GPT-Index)60.211.6138.1729.087.8544.8065.593.7930.50GPT-4 (GPT-Index)59.131.0739.7844.5811.1844.2543.9431.5324.52Claude (GPT-Index)60.213.7636.0241.3718.8139.8255.6216.2028.17Gorilla (GPT-Index)61.82038.1747.468.1944.3664.962.3332.70LLAMA (Oracle)16.1279.034.8317.7077.105.2012.5587.000.43GPT-3.5 (Oracle)66.311.6032.0889.716.643.6595.030.294.67GPT-4 (Oracle)66.120.5333.3385.0710.624.3155.9137.956.13Claude (Oracle)63.443.7632.7977.2119.583.2174.7421.603.64Gorilla (Oracle)67.20032.7991.267.081.6694.161.893.94Finetuning with RetrievalWe now discuss an interesting experiment on how finetuning language with retriever incorporated is helping the performance. The settings for this experiment are finetuning the base LLAMA with the prompt (instruction generated), reference API document (from golden-truth oracle), and the example output generated by GPT-4. In Tab.2, we can see that incorporating ground truth retriever in the finetuning pipeline achieves significantly better results 12.37% better than training without retriever in Torch Hub and 23.46% better in HuggingFace. However, we found that at evaluation time, current retrievers still have a big gap between the ground truth retriever: using GPT-Index at evaluation results in 29.20% accuracy degradation, and using BM25 results in a 52.27% accuracy degradation. Nevertheless, we can still conclude that with a better retriever, finetuning with retriever is still a better method to adopt while in another scenario, when a good retriever is not available, zero-shot finetuning might be the preferred choice.Table 2:Comparison of retrieval techniquesGorilla without RetrieverGorilla with Oracle retrieverzero-shotBM25GPT-IndexOraclezero-shotBM25GPT-IndexOracleTorch Hub (overall)↑↑\\\\uparrow59.1337.6360.2154.83040.3261.8267.20HuggingFace (overall)↑↑\\\\uparrow71.6811.2828.1045.58017.0447.4691.26TensorHub (overall)↑↑\\\\uparrow83.7934.3052.4082.91041.8964.9694.16Torch Hub (Hallu)↓↓\\\\downarrow6.9811.294.3015.591004.3000HuggingFace (Hallu)↓↓\\\\downarrow10.9546.4641.4852.7799.676.428.197.08TensorHub (Hallu)↓↓\\\\downarrow5.4020.4319.7013.281002.772.331.89Hallucination with LLMOne phenomenon we observe is that zero-shot prompting with LLMs (GPT-4/GPT-3.5) to call APIs results in dire hallucination errors. These errors, while diverse, commonly manifest in erroneous behavior such as the model invoking the \"AutoModel.from_pretrained(dir_name)\" command with arbitrary GitHub repository names. Surprisingly, we also found that in TorchHub, HuggingFace and TensorFlow Hub, GPT-3.5 has less hallucination errors than GPT-4. This finding is also consistent for the settings when various retrieving methods are provided: 0-shot, BM25, GPT-Index and the oracle. This might suggest that RLHF plays a central role in turning the model to be truthful. Additional examples and discussion are in Appendix.4.2Test-Time Documentation ChangeFigure 6:Gorilla’s retriever–aware training enables it to react to changes in the APIs.The second column demonstrates changes in model  upgrading FCN’s ResNet–50 backbone to ResNet–101. The third column demonstrate changes in model registry frompytorch/visiontoNVIDIA/DeepLearningExamples:torchhubThe rapidly evolving nature of API documentation presents a significant challenge for the application of LLMs in this field. These documents are often updated at a frequency that outpaces the re-training or fine-tuning schedule of LLMs, making these models particularly brittle to changes in the information they are designed to process. This mismatch in update frequency can lead to a decline in the utility and reliability of LLMs over time.However, with the introduction of Gorilla’s retriever-aware training, we can readily adapt to changes in API documentation. This novel approach allows the model to remain updated and relevant, even as the API documentation it relies on undergoes modifications. This is a pivotal advancement in the field, as it ensures that the LLM maintains its efficacy and accuracy over time, providing reliable outputs irrespective of changes in the underlying documentation.For instance, consider the scenario illustrated in Figure 6, where the training of Gorilla has allowed it to react effectively to changes in APIs. This includes alterations such as upgrading the FCN’s ResNet-50 backbone to ResNet-101, as demonstrated in the second column of the figure. This capability ensures that the LLM remains relevant and accurate even as the underlying models and systems undergo upgrades and improvements.\\nFurthermore, the third column in Figure 6 shows how Gorilla adapts to changes in the model registry frompytorch/visiontoNVIDIA/DeepLearningExamples:torchhub. This reflects the model’s ability to adjust to shifts in API sources, which is vital as organizations may change their preferred model registries over time.In summary, Gorilla’s ability to adapt to test-time changes in API documentation offers numerous benefits. It maintains its accuracy and relevance over time, adapts to the rapid pace of updates in API documentation, and adjusts to modifications in underlying models and systems. This makes it a robust and reliable tool for API calls, significantly enhancing its practical utility.4.3API Call with ConstraintsWe now focus on the language model’s capability of understanding constraints. For any given task, which API call to invoke is typically a tradeoff between a multitude of factors. In the case of RESTFul APIs, it could be the cost of each invocation ($), and the latency of response (ms), among others. Similarly, within the scope of ML APIs, it is desirable for Gorilla to respect constraints such as accuracy, number of learnable parameters in the model, the size on disk, peak memory consumption, FLOPS, etc. We present the underlying ablation study evaluating the ability of different models in zero-shot and with retrievers settings to respect a given accuracy constraint. This setting is best understood with an example. If the user were to ask for an Image classification model that achieves at least 80% top-1 accuracy on the Imagenet dataset, then while both are classification models hosted by Torch Hub,ResNeXt-101 32x16dwith a top-1 accuracy of 84.2% would be the right model whose API to call and not, say,MobileNetV2which has a top-1 accuracy of 71.88%.Table 3:Evaluating LLMs on constraint-aware API invocationsGPT-3.5GPT-4Gorilla0-shotBM25GPT-IndexOracle0-shotBM25GPT-IndexOracle0-shotBM25GPT-IndexOracleTorch Hub (overall)73.9462.6781.6980.9862.6756.3371.1169.0171.8357.0471.8378.16Torch Hub (Hallu)19.0130.9814.7814.0815.4927.4614.089.1519.7139.4326.0516.90Torch Hub (err)7.046.333.524.9221.8316.1914.7821.838.453.522.114.92Accuracy const43.6633.8033.0969.0143.6629.5729.5759.1547.8830.2826.7667.60LLAMAClaude0-shotBM25GPT-IndexOracle0-shotBM25GPT-IndexOracleTorch Hub (overall)08.4511.9719.7129.9281.6982.3981.69Torch Hub (Hallu)10091.5488.0278.8767.2516.1915.4913.38Torch Hub (err)0001.42.812.112.114.92Accuracy const06.333.5217.6017.2529.5731.6969.71For Table3, we filtered a subset of the Torch Hub dataset that had accuracy defined for at least one-dataset in its model card (65.26% of TorchHub dataset in Table1). We notice that with constraints, understandably, the accuracy drops across all models, with and without a retriever. Gorilla is able to match performance with the best-performing model GPT-3.5 when using retrievals (BM25, GPT-Index) and has the highest accuracy in the Zero-shot case. This highlights Gorilla’s ability to navigate APIs while considering the trade-offs between different constraints.'},\n","   {'tag': 'p',\n","    'text': '5ConclusionLLMs are swiftly gaining popularity across diverse domains. In our study, we spotlight techniques designed to enhance the LLM’s ability to accurately identify the appropriate API for a specific task—a significant but often overlooked aspect in the advancement of this technology. Since APIs function as a universal language enabling diverse systems to communicate effectively, their correct usage can boost the ability of LLMs to interact with tools in the wider world.\\nIn this paper, we propose Gorilla, a new novel pipeline for finetuning LLMs to call APIs. The finetuned model’s performance surpasses prompting the state-of-the-art LLM (GPT-4) in three massive datasets we collected. Gorilla generates reliable API calls to ML models without hallucination, demonstrates an impressive capability to adapt to test-time API usage changes, and can satisfy constraints while picking APIs.'},\n","   {'tag': 'p',\n","    'text': '6Limitations & Social ImpactsWith the goal of wanting to have a challenging dataset, we chose ML APIs, given their functional similarity. The potential downside to APIs that focus on the ML domain, is their propensity to produce biased predictions if trained on skewed data, potentially disadvantaging certain sub-groups. To counter this concern and foster a deeper understanding of these APIs, we are releasing our extensive dataset, consisting of over 11,000 instruction-API pairs. This resource will serve the wider community as a valuable tool for studying and benchmarking existing APIs, contributing to a more fair and optimized usage of machine learning.'},\n","   {'tag': 'p',\n","    'text': '7AcknowledgementThis research is supported in part by gifts to UC Berkley Sky Computing Lab from Astronomer, Google, IBM, Intel, Lacework, Microsoft, Nexla, Samsung SDS, Uber, and VMware.'},\n","   {'tag': 'p',\n","    'text': 'ReferencesAhn et\\xa0al.,  [2022]Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C.,\\nGopalakrishnan, K., Hausman, K., Herzog, A., et\\xa0al. (2022).Do as i can, not as i say: Grounding language in robotic affordances.arXiv preprint arXiv:2204.01691.Andor et\\xa0al.,  [2019]Andor, D., He, L., Lee, K., and Pitler, E. (2019).Giving bert a calculator: Finding operations and arguments with\\nreading comprehension.arXiv preprint arXiv:1909.00109.Anthropic,  [2022]Anthropic, h.-c. (2022).Claude.Bavishi et\\xa0al.,  [2019]Bavishi, R., Lemieux, C., Fox, R., Sen, K., and Stoica, I. (2019).Autopandas: neural-backed generators for program synthesis.Proceedings of the ACM on Programming Languages,\\n3(OOPSLA):1–27.Brown et\\xa0al.,  [2020]Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.\\xa0D., Dhariwal, P.,\\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et\\xa0al. (2020).Language models are few-shot learners.Advances in neural information processing systems,\\n33:1877–1901.Bubeck et\\xa0al.,  [2023]Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E.,\\nLee, P., Lee, Y.\\xa0T., Li, Y., Lundberg, S., et\\xa0al. (2023).Sparks of artificial general intelligence: Early experiments with\\ngpt-4.arXiv preprint arXiv:2303.12712.Chen et\\xa0al.,  [2021]Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.\\xa0O., Kaplan, J.,\\nEdwards, H., Burda, Y., Joseph, N., Brockman, G., et\\xa0al. (2021).Evaluating large language models trained on code.arXiv preprint arXiv:2107.03374.Chen et\\xa0al.,  [2023]Chen, X., Lin, M., Schärli, N., and Zhou, D. (2023).Teaching large language models to self-debug.arXiv preprint arXiv:2304.05128.Chiang et\\xa0al.,  [2023]Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L.,\\nZhuang, S., Zhuang, Y., Gonzalez, J.\\xa0E., Stoica, I., and Xing, E.\\xa0P. (2023).Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt\\nquality.Chowdhery et\\xa0al.,  [2022]Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\\nBarham, P., Chung, H.\\xa0W., Sutton, C., Gehrmann, S., et\\xa0al. (2022).Palm: Scaling language modeling with pathways.arXiv preprint arXiv:2204.02311.Chung et\\xa0al.,  [2022]Chung, H.\\xa0W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang,\\nX., Dehghani, M., Brahma, S., et\\xa0al. (2022).Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416.Cobbe et\\xa0al.,  [2021]Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,\\nM., Tworek, J., Hilton, J., Nakano, R., et\\xa0al. (2021).Training verifiers to solve math word problems.arXiv preprint arXiv:2110.14168.Devlin et\\xa0al.,  [2017]Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A.-r., and Kohli,\\nP. (2017).Robustfill: Neural program learning under noisy i/o.InInternational conference on machine learning, pages\\n990–998. PMLR.Gao et\\xa0al.,  [2022]Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and\\nNeubig, G. (2022).Pal: Program-aided language models.arXiv preprint arXiv:2211.10435.Iyer et\\xa0al.,  [2022]Iyer, S., Lin, X.\\xa0V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster,\\nK., Wang, T., Liu, Q., Koura, P.\\xa0S., et\\xa0al. (2022).Opt-iml: Scaling language model instruction meta learning through the\\nlens of generalization.arXiv preprint arXiv:2212.12017.Jain et\\xa0al.,  [2022]Jain, N., Vaidyanath, S., Iyer, A., Natarajan, N., Parthasarathy, S., Rajamani,\\nS., and Sharma, R. (2022).Jigsaw: Large language models meet program synthesis.InProceedings of the 44th International Conference on Software\\nEngineering, pages 1219–1231.Kim et\\xa0al.,  [2023]Kim, G., Baldi, P., and McAleer, S. (2023).Language models can solve computer tasks.arXiv preprint arXiv:2303.17491.Kojima et\\xa0al.,  [2022]Kojima, T., Gu, S.\\xa0S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022).Large language models are zero-shot reasoners.arXiv preprint arXiv:2205.11916.Komeili et\\xa0al.,  [2021]Komeili, M., Shuster, K., and Weston, J. (2021).Internet-augmented dialogue generation.arXiv preprint arXiv:2107.07566.Lachaux et\\xa0al.,  [2020]Lachaux, M.-A., Roziere, B., Chanussot, L., and Lample, G. (2020).Unsupervised translation of programming languages.arXiv preprint arXiv:2006.03511.Lazaridou et\\xa0al.,  [2022]Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. (2022).Internet-augmented language models through few-shot prompting for\\nopen-domain question answering.arXiv preprint arXiv:2203.05115.Li et\\xa0al.,  [2023]Li, R., Allal, L.\\xa0B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone,\\nM., Akiki, C., Li, J., Chim, J., et\\xa0al. (2023).Starcoder: may the source be with you!arXiv preprint arXiv:2305.06161.Li et\\xa0al.,  [2022]Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R.,\\nEccles, T., Keeling, J., Gimeno, F., Dal\\xa0Lago, A., et\\xa0al. (2022).Competition-level code generation with alphacode.Science, 378(6624):1092–1097.Liang et\\xa0al.,  [2023]Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S., Ji, L.,\\nMao, S., et\\xa0al. (2023).Taskmatrix. ai: Completing tasks by connecting foundation models with\\nmillions of apis.arXiv preprint arXiv:2303.16434.Menon et\\xa0al.,  [2013]Menon, A., Tamuz, O., Gulwani, S., Lampson, B., and Kalai, A. (2013).A machine learning framework for programming by example.InInternational Conference on Machine Learning, pages\\n187–195. PMLR.Nakano et\\xa0al.,  [2021]Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C.,\\nJain, S., Kosaraju, V., Saunders, W., et\\xa0al. (2021).Webgpt: Browser-assisted question-answering with human feedback.arXiv preprint arXiv:2112.09332.Nijkamp et\\xa0al.,  [2023]Nijkamp, E., Hayashi, H., Xiong, C., Savarese, S., and Zhou, Y. (2023).Codegen2: Lessons for training llms on programming and natural\\nlanguages.arXiv preprint arXiv:2305.02309.Nijkamp et\\xa0al.,  [2022]Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S.,\\nand Xiong, C. (2022).Codegen: An open large language model for code with multi-turn\\nprogram synthesis.arXiv preprint arXiv:2203.13474.OpenAI,  [2023]OpenAI (2023).Gpt-4 technical report.OpenAI and https://openai.com/blog/chatgpt,  [2022]OpenAI and https://openai.com/blog/chatgpt (2022).Chatgpt.Sanh et\\xa0al.,  [2021]Sanh, V., Webson, A., Raffel, C., Bach, S.\\xa0H., Sutawika, L., Alyafeai, Z.,\\nChaffin, A., Stiegler, A., Scao, T.\\xa0L., Raja, A., et\\xa0al. (2021).Multitask prompted training enables zero-shot task generalization.arXiv preprint arXiv:2110.08207.Scao et\\xa0al.,  [2022]Scao, T.\\xa0L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D.,\\nCastagné, R., Luccioni, A.\\xa0S., Yvon, F., Gallé, M., et\\xa0al. (2022).Bloom: A 176b-parameter open-access multilingual language model.arXiv preprint arXiv:2211.05100.Schick et\\xa0al.,  [2023]Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M.,\\nZettlemoyer, L., Cancedda, N., and Scialom, T. (2023).Toolformer: Language models can teach themselves to use tools.arXiv preprint arXiv:2302.04761.Schick and Schütze,  [2020]Schick, T. and Schütze, H. (2020).Exploiting cloze questions for few shot text classification and\\nnatural language inference.arXiv preprint arXiv:2001.07676.Shen et\\xa0al.,  [2023]Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. (2023).Hugginggpt: Solving ai tasks with chatgpt and its friends in\\nhuggingface.arXiv preprint arXiv:2303.17580.Shinn et\\xa0al.,  [2023]Shinn, N., Labash, B., and Gopinath, A. (2023).Reflexion: an autonomous agent with dynamic memory and\\nself-reflection.arXiv preprint arXiv:2303.11366.Shuster et\\xa0al.,  [2022]Shuster, K., Xu, J., Komeili, M., Ju, D., Smith, E.\\xa0M., Roller, S., Ung, M.,\\nChen, M., Arora, K., Lane, J., et\\xa0al. (2022).Blenderbot 3: a deployed conversational agent that continually learns\\nto responsibly engage.arXiv preprint arXiv:2208.03188.Taori et\\xa0al.,  [2023]Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang,\\nP., and Hashimoto, T.\\xa0B. (2023).Stanford alpaca: An instruction-following llama model.https://github.com/tatsu-lab/stanford_alpaca.Thoppilan et\\xa0al.,  [2022]Thoppilan, R., De\\xa0Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,\\nH.-T., Jin, A., Bos, T., Baker, L., Du, Y., et\\xa0al. (2022).Lamda: Language models for dialog applications.arXiv preprint arXiv:2201.08239.Touvron et\\xa0al.,  [2023]Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,\\nT., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et\\xa0al. (2023).Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971.Vemprala et\\xa0al.,  [2023]Vemprala, S., Bonatti, R., Bucker, A., and Kapoor, A. (2023).Chatgpt for robotics: Design principles and model abilities.2023.[42]Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.\\xa0A., Khashabi, D., and\\nHajishirzi, H. (2022a).Self-instruct: Aligning language model with self generated\\ninstructions.arXiv preprint arXiv:2212.10560.[43]Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A.,\\nAshok, A., Dhanasekaran, A.\\xa0S., Arunkumar, A., Stap, D., et\\xa0al. (2022b).Super-naturalinstructions: Generalization via declarative\\ninstructions on 1600+ nlp tasks.InProceedings of the 2022 Conference on Empirical Methods in\\nNatural Language Processing, pages 5085–5109.Wei et\\xa0al.,  [2022]Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D.\\n(2022).Chain of thought prompting elicits reasoning in large language\\nmodels.arXiv preprint arXiv:2201.11903.Xu et\\xa0al.,  [2022]Xu, F.\\xa0F., Alon, U., Neubig, G., and Hellendoorn, V.\\xa0J. (2022).A systematic evaluation of large language models of code.InProceedings of the 6th ACM SIGPLAN International Symposium on\\nMachine Programming, pages 1–10.Yao et\\xa0al.,  [2022]Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y.\\n(2022).React: Synergizing reasoning and acting in language models.arXiv preprint arXiv:2210.03629.Zeng et\\xa0al.,  [2022]Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y.,\\nZheng, W., Xia, X., et\\xa0al. (2022).Glm-130b: An open bilingual pre-trained model.arXiv preprint arXiv:2210.02414.Zhang et\\xa0al.,  [2022]Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,\\nDiab, M., Li, X., Lin, X.\\xa0V., et\\xa0al. (2022).Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068.'},\n","   {'tag': 'p', 'text': ''},\n","   {'tag': 'p', 'text': ''},\n","   {'tag': 'p',\n","    'text': '8Appendix8.1Dataset DetailsOur dataset is multi-faceted, comprising three distinct domains: Torch Hub, Tensor Hub, and HuggingFace. Each entry within this dataset is rich in detail, carrying critical pieces of information that further illuminate the nature of the data. Delving deeper into the specifics of each domain, Torch Hub provides 95 APIs. The second domain, Tensor Hub, is more expansive with a total of 696 APIs. Finally, the most extensive of them all, HuggingFace, comprises 925 APIs.To enhance the value and utility of our dataset, we’ve undertaken an additional initiative. With each API, we have generated a set of 10 unique instructions. These instructions, carefully crafted and meticulously tailored, serve as a guide for both training and evaluation. This initiative ensures that every API is not just represented in our dataset, but is also comprehensively understood and effectively utilizable.In essence, our dataset is more than just a collection of APIs across three domains. It is a comprehensive resource, carefully structured and enriched with added layers of guidance and evaluation parameters.Domain ClassificationThe unique domain names encompassed within our dataset are illustrated in Figure7. The dataset consists of three sources with a diverse range of domains: Torch Hub houses 6 domains, Tensor Hub accommodates a much broader selection with 57 domains, while HuggingFace incorporates 37 domains. To exemplify the structure and nature of our dataset, we invite you to refer to the domain names represented in Figure8.API Call TaskIn this task, we test the model’s capability to generate a single line of code, either in a zero-shot fashion or by leveraging an API reference. Primarily designed for evaluation purposes, this task effectively gauges the model’s proficiency in identifying and utilizing the appropriate API call.API Provider ComponentThis facet relates to the provision of the programming language. In this context, the API provider plays a vital role as it serves as a foundation upon which APIs are built and executed.Explanation ElementThis component offers valuable insights into the rationale behind the usage of a particular API, detailing how it aligns with the prescribed requirements. Furthermore, when certain constraints are imposed, this segment also incorporates those limitations. Thus, the explanation element serves a dual purpose, offering a deep understanding of API selection, as well as the constraints that might influence such a selection. This balanced approach ensures a comprehensive understanding of the API usage within the given context.CodeExample code for accomplishing the task. We de-prioritize this as we haven’t tested the execution result of the code. We leave this for future works, but make this data available in-case others want to build on it.Torch Hub domain names: Classification, Semantic Segmentation, Object Detection, Audio Separation, Video Classification, Text-to-SpeechTensor Hub domain names: text-sequence-alignment, text-embedding, text-language-model, text-preprocessing, text-classification, text-generation, text-question-answering, text-retrieval-question-answering, text-segmentation, text-to-mel, image-classification, image-feature-vector, image-object-detection, image-segmentation, image-generator, image-pose-detection, image-rnn-agent, image-augmentation, image-classifier, image-style-transfer, image-aesthetic-quality, image-depth-estimation, image-super-resolution, image-deblurring, image-extrapolation, image-text-recognition, image-dehazing, image-deraining, image-enhancemenmt, image-classification-logits, image-frame-interpolation, image-text-detection, image-denoising, image-others, video-classification, video-feature-extraction, video-generation, video-audio-text, video-text, audio-embedding, audio-event-classification, audio-command-detection, audio-paralinguists-classification, audio-speech-to-text, audio-speech-synthesis, audio-synthesis, audio-pitch-extractionHuggingFace domain names: Multimodal Feature Extraction, Multimodal Text-to-Image, Multimodal Image-to-Text, Multimodal Text-to-Video, Multimodal Visual Question Answering, Multimodal Document Question Answer, Multimodal Graph Machine Learning, Computer Vision Depth Estimation, Computer Vision Image Classification, Computer Vision Object Detection, Computer Vision Image Segmentation, Computer Vision Image-to-Image, Computer Vision Unconditional Image Generation, Computer Vision Video Classification, Computer Vision Zero-Shor Image Classification, Natural Language Processing Text Classification, Natural Language Processing Token Classification, Natural Language Processing Table Question Answering, Natural Language Processing Question Answering, Natural Language Processing Zero-Shot Classification, Natural Language Processing Translation, Natural Language Processing Summarization, Natural Language Processing Conversational, Natural Language Processing Text Generation, Natural Language Processing Fill-Mask, Natural Language Processing Text2Text Generation, Natural Language Processing Sentence Similarity, Audio Text-to-Speech, Audio Automatic Speech Recognition, Audio Audio-to-Audio, Audio Audio Classification, Audio Voice Activity Detection, Tabular Tabular Classification, Tabular Tabular Regression, Reinforcement Learning Reinforcement Learning, Reinforcement Learning RoboticsFigure 7:Domain names: Domain names with the three dataset. Tensor Hub is the smallest dataset while the other two hubs contain many more models.### User: Our customer is a zoo and we want to help them detect movement of different animals. Write a python program in 1 to 2 lines to call API in TensorFlowHub.### Assistant:<domain>: image-object-detection,<api_call>: detector = hub.load(’https://tfhub.dev/google/openimages_v4/ssd_mobilenet_v2/1’),<api_provider>: TensorFlow Hub,<explanation>: This API call loads the pre-trained Single Shot Detector (SSD) MobileNet V2 model from TensorFlow Hub, which has been trained on the Open Images v4 dataset. This model is used for object detection in images. Once loaded, you can pass an image (and an optional score threshold for detection confidence) to the detector to get back the detected objects along with their bounding boxes, class labels, and associated confidence scores.<code>: import tensorflow as tf; import tensorflow_hub as hub; # Load the SSD MobileNet V2 model; detector = hub.load(’https://tfhub.dev/google/openimages_v4/ssd_mobilenet_v2/1’); # Use the model to detect objects in an image (assuming ’image’ is a TensorFlow tensor representing your image); result = detector(image, score_threshold=0.5)### User: I am an engineer at Uber and I need to find a API that can classify pedestrians, cars, etc. from an image of the scene. Write a python program in 1 to 2 lines to call API in TorchHub.<Reference API>: {domain: Object Detection, framework: PyTorch, functionality: Traffic Object Detection, Drivable Area Segmentation, Lane Detection, api_name: HybridNets, api_call: torch.hub.load(repo_or_dir=’datvuthanh/hybridnets’, model=’hybridnets’, pretrained=True), api_arguments: repo_or_dir, model, pretrained, python_environment_requirements: Python>=3.7, PyTorch>=1.10, example_code: ’import torch; model = torch.hub.load(’datvuthanh/hybridnets’, ’hybridnets’, pretrained=True); img = torch.randn(1,3,640,384); features, regression, classification, anchors, segmentation = model(img), performance: dataset: [name: BDD100K, accuracy: Traffic Object Detection: Recall (%): 92.8, mAP@0.5 (%): 77.3, Drivable Area Segmentation: Drivable mIoU (%): 90.5, Lane Line Detection: Accuracy (%): 85.4, Lane Line IoU (%): 31.6], description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.}### Assistant:<domain>: Object Detection,<api_call>: model = torch.hub.load(’datvuthanh/hybridnets’, ’hybridnets’, pretrained=True),<api_provider>: PyTorch,<explanation>: Load the pretrained HybridNets model for object detection from PyTorch Hub to classify pedestrians, cars, etc. in an image of the scene,<code>: import torch; model = torch.hub.load(’datvuthanh/hybridnets’, ’hybridnets’, pretrained=True)Figure 8:Example of the Dataset: Two examples of the dataset, the above one is zero-shot (without information retrievers) and the bottom one is with information retriever.8.2Gorilla DetailsWe provide all the training details for Gorilla in this section. This includes how we divide up the training, evaluation dataset, training hyperparameters for Gorilla.DataFor HuggingFace, we devise the entire dataset into 90% training and 10% evaluation. For Torch Hub and Tensor Hub, we devise the data in to 80% training and 20% testing.TrainingWe train Gorillafor 5 epochs with the 2e-5 learning rate with cosine decay. The details are provide in Tab.4. We finetune it on 8xA100 with 40G memory each.Table 4:Hyperparameters for training GorillaHyperparameter NameValuelearning rate2e-5batch size64epochs5warmup ratio0.03weight decay0max seq length20488.3Performance ComparisonWe provide a full comparison of each model’s performance in this section. In Fig10and Fig.11, the full set of comparisons is provided. We see that especially in zero-shot case, Gorilla surpasses the GPT-4 and GPT-3.5 by a large margin. The GPT-4 and GPT-3.5 gets around 40% accuracy in Torch Hub and Tensor Hub, which are two structured API calls. Compared to that, HuggingFace is a more flexible and diverse Hub, as a result, the performance on HuggingFace is not as competitive.8.3.1EvaluationFor ease of evaluation, we manually cleaned up the dataset to make sure each API call domain only contains the valid call in the form of:API_name(API_arg1subscriptarg1\\\\mathrm{arg_{1}}, API_arg2subscriptarg2\\\\mathrm{arg_{2}}, …, API_argksubscriptargk\\\\mathrm{arg_{k}})Our framework allows the user to define any combination of the arguments to check. For Torch Hub, we check for the API nametorch.hub.loadwith argumentsrepo_or_dirandmodel. For Tensor Hub, we check API namehub.KerasLayerandhub.loadwith argumenthandle. For HuggingFace, since there are many API function names, we don’t list all of them here. One specific note is that we require thepretrained_model_name_or_pathargument for all the calls except forpipeline. Forpipeline, we don’t require thepretrained_model_name_or_pathargument since it automatically select a model for you oncetaskis specified.8.3.2HallucinationWe found especially in HuggingFace, the GPT-4 model incurs serious hallucination problems. It would sometimes put a GitHub name that is not associated with the HuggingFace repository in to the domain ofpretrained_model_name_or_path. Fig.9demonstrates some examples and we also observe that GPT-4 sometimes assumes the user have a local path to the model likeyour_model_name. This is greatly reduced by Gorilla as we see the hallucination error comparison in Tab.1.generate_video = pipeline(\"text-to-video\", model=\"your_model_name\")vqa = pipeline(\"visual-question-answering\", model=\"microsoft/clip-vqa-base\", tokenizer=\"microsoft/clip-vqa-base\")depth_estimator = pipeline(\"depth-estimation\", model=\"intel-isl/MiDaS\", tokenizer=\"intel-isl/MiDaS\")Figure 9:Hallucination Examples: GPT-4 incurs serious hallucination errors in HuggingFace. We show a couple of examples in the figure.Figure 10:Performance: We plot each model’s performance on different configurations. We see that Gorilla performs extremely well in the zero-shot setting. While even when the oracle answer is given, Gorilla is still the best.Figure 11:Accuracy vs Hallucination: We plot each model’s performance on different configurations. We found that in the zero-shot setting, Gorilla has the most accuracy gain while maintaining good factual capability. When prompting with different retrievers, Gorilla is still capable to avoid the hallucination errors.'}]},\n"," {'tag': 'h2',\n","  'text': '1Introduction',\n","  'subsections': [{'tag': 'p',\n","    'text': 'Recent advances in large language models (LLMs)[10,5,32,6,29,30]have enabled significant new capabilities including natural dialogue, mathematical reasoning, and program synthesis.\\nHowever, despite these advances, LLMs are still fundamentally limited by the information they can store in a fixed set of weights and the things they can compute using a static computation graph and limited context.\\nFurthermore, as the world changes, LLMs require retraining to update their knowledge and reasoning capabilities.'},\n","   {'tag': 'p',\n","    'text': 'By empowering LLMs to use tools[33], we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks.\\nBy providing access to search technologies and databases,[26,39,37]demonstrated that we can augment LLMs to address a significantly larger and more dynamic knowledge space.\\nSimilarly, by providing access to computational tools,[39,2]demonstrated that LLMs can accomplish complex computational tasks.\\nConsequently, leading LLM providers[29], have started to integrate plugins to allow LLMs to invoke external tools through APIs.'},\n","   {'tag': 'p',\n","    'text': 'This transition from a small set of hand-coded tools, to the ability to invoke a vast space of changing cloud APIs could transform LLMs into the primary interface to computing infrastructure and the web.\\nTasks ranging from booking an entire vacation to hosting a conference, could become as simple as talking to an LLM that has access to the flight, car rental, hotel, catering, and entertainment web APIs.\\nHowever, much of the prior work[35,24]integrating tools into LLMs considered a small well documented set of APIs that can be easily injected into the prompt.'},\n","   {'tag': 'p',\n","    'text': 'Supporting a web scale collection of potentially millions of changing APIs requires rethinking our approach to how we integrate tools.\\nIt is not longer possible to describe the full set of APIs in a single context.\\nMany of the APIs will have overlapping functionality with nuanced limitations and constraints.\\nSimply evaluating LLMs in this new setting requires new benchmarks.'},\n","   {'tag': 'p',\n","    'text': 'Figure 1:Examples of API calls. Example API calls generated by GPT-4[29], Claude[3], and Gorilla for the given prompt. In this example, GPT-4 presents a model that doesn’t exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call.'},\n","   {'tag': 'p',\n","    'text': 'In this paper, we explore the use of self-instruct fine-tuning and retrieval to enable LLMs to accurately select from a large, overlapping, and changing set tools expressed using their APIs and API documentation.\\nWe construct, APIBench, a large corpus of APIs with complex and often overlapping functionality by scraping ML APIs (models) from public model hubs.\\nWe choose three major model hubs for dataset construction: TorchHub, TensorHub and HuggingFace. We exhaustively include every API call in TorchHub (94 API calls) and TensorHub (696 API calls); For HuggingFace, since the models come in a large number and lots of the models don’t have a specification, we choose the most downloaded 20 models per task category (in a total of 925). We also generate 10 synthetic user question prompts per API using Self-Instruct[42]. Thus, each entry in the dataset becomes an instruction reference API pair. We adopt a common AST sub-tree matching technique to evaluate the functional correctness of the generated API. We first parse the generated code into an AST tree, then find a sub-tree whose root node is the API call that we care about (e.g.,torch.hub.load) and use it to index our dataset. We check the functional correctness and hallucination problem for the LLMs, reporting the corresponding accuracy.'},\n","   {'tag': 'p',\n","    'text': 'We then finetune Gorilla, a LLaMA-7B-based model with document retrieval using our dataset. We find that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy as well as reducing hallucination errors. We show an example output in Fig.1. Further, our retrieval-aware training of Gorilla enables the model to adapt to changes in the API documentation. Finally, we demonstrate Gorilla’s ability to understand and reason about constraints.'},\n","   {'tag': 'p',\n","    'text': 'Figure 2:Accuracy (vs) hallucinationin four settings, that is,zero-shot(i.e., without any retriever), andwith retrievers.BM25andGPTare commonly used retrievers and theoracleretriever returns relevant documents at 100%, indicating an upper bound. Higher in the graph (higher accuracy) and to the left is better (lower hallucination). Across the entire dataset, our model, Gorilla, improves accuracy while reducing hallucination.'}]},\n"," {'tag': 'h3',\n","  'text': '3.1Dataset Collection',\n","  'subsections': [{'tag': 'p',\n","    'text': 'To collect the dataset, we meticulously recorded all online model cards for HuggingFace’s “The Model Hub”, PyTorch Hub, and TensorFlow Hub Models. Throughout the rest of the paper, we call these HuggingFace, Torch Hub, and TensorFlow Hub respectively for brevity.'},\n","   {'tag': 'p',\n","    'text': 'API DocumentationThe HuggingFace platform hosts and servers about 203,681 models. However, many of them have poor documentation, lack dependencies, have no information in their model card, etc. To filter these out, we pick the top 20 models from each domain.\\nWe consider 7 domains in multimodal data, 8 in CV, 12 in NLP, 5 in Audio, 2 in tabular data, and 2 in reinforcement learning. Post filtering, we got a total of 925 models from HuggingFace. TensorFlow Hub is versioned into v1 and v2. The latest version (v2) has 801 models in total, and we process all of them. Post filtering out models, whose mode cards had little to no information, we are left with 626 models. Similar to TensorFlow Hub, we get 95 models from Torch Hub.\\nWe then converted the model cards for each of these 1,645 API calls into a json object with the following fields: {domain, framework, functionality, api_name, api_call, api_arguments, environment_requirements, example_code, performance, and description.}. We provide more information in the Appendix. These fields were chose to generalize beyond the API calls within ML domain, to other domains, includin RESTful API calls.'},\n","   {'tag': 'p',\n","    'text': 'Instruction GenerationGuided by the self-instruct paradigm[42], we employed GPT-4 to generate synthetic instruction data.\\nWe provided three in-context examples, along with a reference API documentation, and tasked the model with generating real-world use cases that call upon the API. We specifically instructed the model to refrain from using any API names or hints when creating instructions. We constructed six examples (Instruction-API pairs) for each of the three model hubs. These 18 points, were the only hand-generated or modified data. For each of our 1,645 API datapoints, we sample 3 of 6 corresponding instruction examples to generate a total of 10 instruction-api pairs as demonstrated in Figure3. We would like to highlight that we only need to employ GPT-4 to generate the instructions and this can be swapped with open-source alternatives such as LLaMA, Alpaca, etc.'}]},\n"," {'tag': 'h2',\n","  'text': '2Related Work',\n","  'subsections': [{'tag': 'p',\n","    'text': 'Large Language ModelsRecent strides in the field of LLMs have renovated many downstream domains[10,40,48,47], not only in traditional natural language processing tasks but also in program synthesis. Many of these advances are achieved by augmenting pre-trained LLMs by prompting[44,14]and instruction fine-tuning[11,31,43,15]. Recent open-sourced models like\\nLLaMa[40], Alpaca[38], and Vicuna[9]have furthered the understanding of LLMs and facilitated their experimentation. While our approach, Gorilla, incorporates techniques akin to those mentioned, its primary emphasis is on enhancing the LLMs’ ability to utilize millions of tools, as opposed to refining their conversational skills. Additionally, we pioneer the study of fine-tuning a base model by supplementing it with information retrieval - a first, to the best of our knowledge.'},\n","   {'tag': 'p',\n","    'text': 'Tool UsageThe discussion of tool usage within LLMs has seen an upsurge, with models like Toolformer taking the lead[33,19,21,26]. Tools often incorporated include web-browsing[34], calculators[12,39], translation systems[39], and Python interpreters[14].\\nWhile these efforts can be seen as preliminary explorations of marrying LLMs with tool usage, they generally focus on specific tools. Our paper, in contrast, aims to explore a vast array of tools (i.e., API calls) in an open-ended fashion, potentially covering a wide range of applications.With the recent launch of Toolformer[33]and GPT-4[29], the importance of API calls has been highlighted, encouraging many works in employing API calls as tooling[35,24]. Moreover, the application of API calls in robotics has been explored to some extent[41,1]. However, these works primarily aim at showcasing the potential of “prompting” LLMs rather than establishing a systematic method for evaluation and training (including fine-tuning). Our work, on the other hand, concentrates on systematic evaluation and building a pipeline for future use.'},\n","   {'tag': 'p',\n","    'text': 'LLMs for Program SynthesisHarnessing LLMs for program synthesis has historically been a challenging task[23,7,45,16,13,20]. Researchers have proposed an array of strategies to prompt LLMs to perform better in coding tasks, including in-context learning[44,18,7], task decomposition[17,46], and self-debugging[8,36]. Besides prompting, there have also been efforts to pretrain language models specifically for code generation[28,22,27].However, these strategies focus on prompting large language models or pre-training them for general program synthesis. In our research, in contrast, we focus on a much restricted domain: the synthesis of linear programs using API calls. General program synthesis, not only is complex, but is also hard to verify and evaluate. API calls, on the other hand, function more like tool usage. This allows the LLM to significantly expand its capabilities without grappling with low-level implementation details.'}]}]"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","source":["# Example usage\n","title = \"Gorilla: Large Language Model Connected with Massive APIs\"\n","paper = get_paper_info_by_title(title, api_key)\n","\n","if paper:\n","    abstract = paper.get('abstract', '')\n","    print(f\"Title: {paper['title']}\")\n","    print(f\"Abstract: {abstract}\")\n","\n","    ar5iv_url = get_ar5iv_url(paper)\n","    if ar5iv_url:\n","        print(f\"ar5iv URL: {ar5iv_url}\")\n","\n","x = get_html_from_url(ar5iv_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5xYEGY4Ha4b","executionInfo":{"status":"ok","timestamp":1719214236779,"user_tz":-540,"elapsed":3568,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"00850a35-22bc-4f74-a5c9-44348b2bb4a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Title: Gorilla: Large Language Model Connected with Massive APIs\n","Abstract: Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\n","ar5iv URL: https://ar5iv.org/abs/2305.15334\n"]}]},{"cell_type":"code","source":["\n","# 주어진 텍스트 리스트 예시\n","text_list = [\n","    'Gorilla: Large Language Model Connected with Massive APIs',\n","    'Introduction',\n","    'Dataset Collection',\n","    'Related Work'\n","]\n","\n","# 함수 호출\n","extracted_texts = extract_text_under_headers(ar5iv_url, text_list)\n","\n","# 결과 출력\n","for tag, header, text in extracted_texts:\n","    print(f\"{tag}: {header}\\n{text}\\n\\n\\n\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcuC-OnTIrLP","executionInfo":{"status":"ok","timestamp":1719216245378,"user_tz":-540,"elapsed":2052,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"85836f5e-8ddc-4f96-fce9-46c748305b38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tag: text\n","subsections\n","\n","\n","\n","\n","tag: text\n","subsections\n","\n","\n","\n","\n","tag: text\n","subsections\n","\n","\n","\n","\n","tag: text\n","subsections\n","\n","\n","\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"knApLMXMQuA3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extracted_texts = extract_text_under_headers(ar5iv_url, text_list)\n","\n","# 결과 출력\n","print_hierarchy(extracted_texts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRPw4rgPJOic","executionInfo":{"status":"ok","timestamp":1719216249797,"user_tz":-540,"elapsed":1314,"user":{"displayName":"최유민","userId":"14691279395225646105"}},"outputId":"a9875f44-bff7-45d1-ff97-db80725b1914"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gorilla: Large Language Model Connected with Massive APIs\n","  Shishir G. Patil1Tianjun Zhang1,∗Xin Wang2Joseph E. Gonzalez11UC Berkeley2Microsoft Researchsgp@berkeley.eduEqual contribution.\n","  AbstractLarge Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model’s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs.\n","Gorilla’s code, model, data, and demo are available athttps://gorilla.cs.berkeley.edu\n","  1IntroductionRecent advances in large language models (LLMs)[10,5,32,6,29,30]have enabled significant new capabilities including natural dialogue, mathematical reasoning, and program synthesis.\n","However, despite these advances, LLMs are still fundamentally limited by the information they can store in a fixed set of weights and the things they can compute using a static computation graph and limited context.\n","Furthermore, as the world changes, LLMs require retraining to update their knowledge and reasoning capabilities.By empowering LLMs to use tools[33], we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks.\n","By providing access to search technologies and databases,[26,39,37]demonstrated that we can augment LLMs to address a significantly larger and more dynamic knowledge space.\n","Similarly, by providing access to computational tools,[39,2]demonstrated that LLMs can accomplish complex computational tasks.\n","Consequently, leading LLM providers[29], have started to integrate plugins to allow LLMs to invoke external tools through APIs.This transition from a small set of hand-coded tools, to the ability to invoke a vast space of changing cloud APIs could transform LLMs into the primary interface to computing infrastructure and the web.\n","Tasks ranging from booking an entire vacation to hosting a conference, could become as simple as talking to an LLM that has access to the flight, car rental, hotel, catering, and entertainment web APIs.\n","However, much of the prior work[35,24]integrating tools into LLMs considered a small well documented set of APIs that can be easily injected into the prompt.Supporting a web scale collection of potentially millions of changing APIs requires rethinking our approach to how we integrate tools.\n","It is not longer possible to describe the full set of APIs in a single context.\n","Many of the APIs will have overlapping functionality with nuanced limitations and constraints.\n","Simply evaluating LLMs in this new setting requires new benchmarks.Figure 1:Examples of API calls. Example API calls generated by GPT-4[29], Claude[3], and Gorilla for the given prompt. In this example, GPT-4 presents a model that doesn’t exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call.In this paper, we explore the use of self-instruct fine-tuning and retrieval to enable LLMs to accurately select from a large, overlapping, and changing set tools expressed using their APIs and API documentation.\n","We construct, APIBench, a large corpus of APIs with complex and often overlapping functionality by scraping ML APIs (models) from public model hubs.\n","We choose three major model hubs for dataset construction: TorchHub, TensorHub and HuggingFace. We exhaustively include every API call in TorchHub (94 API calls) and TensorHub (696 API calls); For HuggingFace, since the models come in a large number and lots of the models don’t have a specification, we choose the most downloaded 20 models per task category (in a total of 925). We also generate 10 synthetic user question prompts per API using Self-Instruct[42]. Thus, each entry in the dataset becomes an instruction reference API pair. We adopt a common AST sub-tree matching technique to evaluate the functional correctness of the generated API. We first parse the generated code into an AST tree, then find a sub-tree whose root node is the API call that we care about (e.g.,torch.hub.load) and use it to index our dataset. We check the functional correctness and hallucination problem for the LLMs, reporting the corresponding accuracy.We then finetune Gorilla, a LLaMA-7B-based model with document retrieval using our dataset. We find that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy as well as reducing hallucination errors. We show an example output in Fig.1. Further, our retrieval-aware training of Gorilla enables the model to adapt to changes in the API documentation. Finally, we demonstrate Gorilla’s ability to understand and reason about constraints.Figure 2:Accuracy (vs) hallucinationin four settings, that is,zero-shot(i.e., without any retriever), andwith retrievers.BM25andGPTare commonly used retrievers and theoracleretriever returns relevant documents at 100%, indicating an upper bound. Higher in the graph (higher accuracy) and to the left is better (lower hallucination). Across the entire dataset, our model, Gorilla, improves accuracy while reducing hallucination.\n","  2Related WorkLarge Language ModelsRecent strides in the field of LLMs have renovated many downstream domains[10,40,48,47], not only in traditional natural language processing tasks but also in program synthesis. Many of these advances are achieved by augmenting pre-trained LLMs by prompting[44,14]and instruction fine-tuning[11,31,43,15]. Recent open-sourced models like\n","LLaMa[40], Alpaca[38], and Vicuna[9]have furthered the understanding of LLMs and facilitated their experimentation. While our approach, Gorilla, incorporates techniques akin to those mentioned, its primary emphasis is on enhancing the LLMs’ ability to utilize millions of tools, as opposed to refining their conversational skills. Additionally, we pioneer the study of fine-tuning a base model by supplementing it with information retrieval - a first, to the best of our knowledge.Tool UsageThe discussion of tool usage within LLMs has seen an upsurge, with models like Toolformer taking the lead[33,19,21,26]. Tools often incorporated include web-browsing[34], calculators[12,39], translation systems[39], and Python interpreters[14].\n","While these efforts can be seen as preliminary explorations of marrying LLMs with tool usage, they generally focus on specific tools. Our paper, in contrast, aims to explore a vast array of tools (i.e., API calls) in an open-ended fashion, potentially covering a wide range of applications.With the recent launch of Toolformer[33]and GPT-4[29], the importance of API calls has been highlighted, encouraging many works in employing API calls as tooling[35,24]. Moreover, the application of API calls in robotics has been explored to some extent[41,1]. However, these works primarily aim at showcasing the potential of “prompting” LLMs rather than establishing a systematic method for evaluation and training (including fine-tuning). Our work, on the other hand, concentrates on systematic evaluation and building a pipeline for future use.LLMs for Program SynthesisHarnessing LLMs for program synthesis has historically been a challenging task[23,7,45,16,13,20]. Researchers have proposed an array of strategies to prompt LLMs to perform better in coding tasks, including in-context learning[44,18,7], task decomposition[17,46], and self-debugging[8,36]. Besides prompting, there have also been efforts to pretrain language models specifically for code generation[28,22,27].However, these strategies focus on prompting large language models or pre-training them for general program synthesis. In our research, in contrast, we focus on a much restricted domain: the synthesis of linear programs using API calls. General program synthesis, not only is complex, but is also hard to verify and evaluate. API calls, on the other hand, function more like tool usage. This allows the LLM to significantly expand its capabilities without grappling with low-level implementation details.\n","  3MethodologyFigure 3:Gorilla: A system for enabling LLMs to interact with APIs.The upper half represents the training procedure as described in Sec3. This is the most exhaustive API data-set for ML to the best of our knowledge. During inference (lower half), Gorilla supports two modes - with retrieval, and zero-shot. In this example, it is able to suggest the right API call for generating the image from the user’s natural language query.In this section, we describe APIBench, a comprehensive benchmark constructed from TorchHub, TensorHub, and HuggingFace API Model Cards. We begin by outlining the process of collecting the API dataset and how we generated instruction-answer pairs. We then introduce Gorilla, a novel training paradigm with a information–retriever incorporated into the training and inference pipelines. Finally, we present our AST tree matching evaluation metric.3.1Dataset CollectionTo collect the dataset, we meticulously recorded all online model cards for HuggingFace’s “The Model Hub”, PyTorch Hub, and TensorFlow Hub Models. Throughout the rest of the paper, we call these HuggingFace, Torch Hub, and TensorFlow Hub respectively for brevity.API DocumentationThe HuggingFace platform hosts and servers about 203,681 models. However, many of them have poor documentation, lack dependencies, have no information in their model card, etc. To filter these out, we pick the top 20 models from each domain.\n","We consider 7 domains in multimodal data, 8 in CV, 12 in NLP, 5 in Audio, 2 in tabular data, and 2 in reinforcement learning. Post filtering, we got a total of 925 models from HuggingFace. TensorFlow Hub is versioned into v1 and v2. The latest version (v2) has 801 models in total, and we process all of them. Post filtering out models, whose mode cards had little to no information, we are left with 626 models. Similar to TensorFlow Hub, we get 95 models from Torch Hub.\n","We then converted the model cards for each of these 1,645 API calls into a json object with the following fields: {domain, framework, functionality, api_name, api_call, api_arguments, environment_requirements, example_code, performance, and description.}. We provide more information in the Appendix. These fields were chose to generalize beyond the API calls within ML domain, to other domains, includin RESTful API calls.Instruction GenerationGuided by the self-instruct paradigm[42], we employed GPT-4 to generate synthetic instruction data.\n","We provided three in-context examples, along with a reference API documentation, and tasked the model with generating real-world use cases that call upon the API. We specifically instructed the model to refrain from using any API names or hints when creating instructions. We constructed six examples (Instruction-API pairs) for each of the three model hubs. These 18 points, were the only hand-generated or modified data. For each of our 1,645 API datapoints, we sample 3 of 6 corresponding instruction examples to generate a total of 10 instruction-api pairs as demonstrated in Figure3. We would like to highlight that we only need to employ GPT-4 to generate the instructions and this can be swapped with open-source alternatives such as LLaMA, Alpaca, etc.3.2GorillaOur model Gorilla, is retrieve-aware finetuned LLaMA-7B model, specifically for API calls. As shown in Fig3, we employ self-instruct to generate {instruction, API} pairs. To fine-tune LLaMA, we convert this to a user-agent chat-style conversation, where each data-point is a conversation with one round each for the user and the agent. We then perform standard instruction finetuning on the base LLaMA-7B model. For our experiments, we train Gorilla with and without the retriever.API Call with ConstraintsAPI calls often come with inherent constraints. These constraints necessitate that the LLM not only comprehend the functionality of the API call but also categorize the calls according to different constraint parameters.\n","This requirement introduces an additional layer of complexity to the process, demanding a more nuanced understanding from the LLM.\n","Specifically, for machine learning API calls, two common sets of constraints are: parameter size and a lower bound on accuracy. Consider, for instance, the following prompt:‘‘Invoke an image classification model that uses less than 10M parameters, but maintains an ImageNet accuracy of at least 70%’’. Such a prompt presents a substantial challenge for the LLM to accurately interpret and respond to. Not only must the LLM understand the user’s functional description, but it also needs to reason about the various constraints embedded within the request. This challenge underlines the intricate demands placed on LLMs in real-world API calls. It is not sufficient for the model to merely comprehend the basic functionality of an API call; it must also be capable of navigating the complex landscape of constraints that accompany such calls. These observations necessitate the need to fine-tune an LLM for APIs.Retriever-Aware trainingFor training with retriever, the instruction-tuned dataset, also has an additional\"Use this API documentation for reference: <retrieved_API_doc_JSON>\"appended to the user prompt. Through this, we aim to teach the LLM to parse the second half of the question to answer the first half. We demonstrate that this a) makes the LLM adapt to test-time changes in API documentation, and b) improves performance from in-context learning, and finally c) show that it reduces hallucination error.Surprisingly, we find that augmenting a LLM with retrieval, does not always lead to improved performance, and can at-times hurt performance. We share more insights along with details in Sec4.Gorilla InferenceDuring Inference, the user provides the prompt in natural language (Fig:3). This can be for a simple task (e.g,“I would like to identify the objects in an image”), or they can specify a vague goal, (.e.g,“I am going to the zoo, and would like to track animals”). Gorilla, similar to training, can be used for inference in two modes: zero-shot and with retrieval. In zero-shot, this prompt (with NO further prompt tuning) is fed to the Gorilla LLM model when then returns the API call that will help in accomplishing the task and/or goal. In retrieval mode, the retriever (either of BM25 or GPT-Index) first retrieves the most up-to-date API documentation stored in the API Database. This is then concatenated to the user prompt along with the messageUse this API documentation for reference:before feeding it to Gorilla. The output of Gorilla is an API to be invoked. Besides the concatenation as described, we doNOfurther prompt tuning in our system. While we do have a system to execute these APIs, that is not a focus of this paper.3.3Verifying APIsInductive program synthesis, where a program is synthesized to satisfy test cases, has found success in several avenues[4,25].\n","However, test cases fall short when evaluating API calls, as it is often hard to verify the semantic correctness of the code. For example, consider the task of classifying an image.\n","There are over 40 different models that can be used for the task. Even if we were to narrow down to a single family of Densenet, there are four different configurations possible. Hence, there exist multiple correct answers and it is hard to tell if the API being used is functionally equivalent to the reference API by unit tests. Thus, to evaluate the performance of our model, we compare their functional equivalence using the dataset we collected. To trace which API in the dataset is the LLM calling, we adopt the AST tree-matching strategy. Since we only consider one API call in this paper, checking if the AST of the candidate API call is a sub-tree of the reference API call reveals which API is being used in the dataset.Identifying and even defining hallucinations can be challenging.\n","We use the AST matching process to directly identify the hallucinations.\n","We define a hallucination as an API call that is not a sub-tree of any API in the database – invoking an entirely imagined tool.\n","This form of hallucination is distinct from invoking an API incorrectly which we instead define as an error.AST Sub-Tree MatchingWe perform AST sub-tree matching to identify which API in our dataset is the LLM calling. Since each API call can have many arguments, we need to match on each of these arguments. Further, since, Python allows for default arguments, for each API, we define which arguments to match in our database. For example, we checkrepo_or_dirandmodelarguments in our function call. In this way, we can easily check if the argument matches the reference API or not. Please refer to Fig.4for more details. In this example, Gorilla returns a torch API call. We first build the tree, and verify that it matches a subtree in our dataset along nodestorch.hub.load,pytorch/vision, anddensenet121. But, we don’t check for match along leaf nodepretrained = Truesince that is an optional python argument.Figure 4:AST Sub-Tree Matching to evaluate API calls.On the left is an API call returned by Gorilla. We first build the associated API tree. We then compare this to our dataset, to see if the API dataset has a subtree match. In the above example, the matching subtree is highlighted in brown, signifying that the API call is indeed correct.Pretrained=Trueis an optional argument.\n","  4EvaluationWe carried out an array of experiments on our collected dataset, benchmarking our model Gorilla with other models, and exploring how different retrieval methods may impact the performance of the model in making API calls. We then demonstrate that Gorilla can easily adapt to test-time changes in API documentation. In addition, we assess Gorilla’s ability to reason about API calls under constraints. Lastly, we examined how integrating different retrieval methods during training influences the model’s final performance.BaselinesPrimarily, we compare Gorilla with state-of-the-art language models in a zero-shot setting. The models under consideration include: GPT-4 by OpenAI, we use thegpt-4-0314checkpoint; GPT-3.5-turbo with thegpt-3.5-turbo-0301checkpoint, both of which are RLHF-tuned model specifically designed for conversation; Claude withclaude-v1checkpoint, a language model by Anthropic, renowned for its lengthy context capabilities; LLaMA-7B, a large language model by Meta and the finest open-source model to date.RetrieversThe termZero-shot(abbreviated as 0-shot in tables) refers to scenarios where no retriever is used. The sole input to the model is the user’s natural language prompt. ForBM25, we consider each API as a separate document. During retrieval, we use the user’s query to search the index and fetch the most relevant (top-1) API.\n","This API is concatenated with the user’s prompt to query the LLMs. Similarly, GPT-Index refers to the retrieval modeltext-davinci-003from OpenAI. Like BM25, each API call is indexed as an individual document, and the most relevant document, given a user query, is retrieved and appended to the user prompt. Lastly, we include an Oracle retriever, which serves two purposes: first, to identify the potential for performance improvement through more efficient retrievers, and second, to assist users who know which API to use but may need to help invoking it.\n","In all cases, when a retriever is used, it is appended to the user’s prompt as follows:<user_prompt>Use this API documentation for reference:<retrieved_API_doc_JSON>.\n","The dataset for these evaluations is detailed in Sec3. We emphasize that we have maintained a holdout test set on which we report our findings. The holdout test set was created by dividing the self-instruct dataset’s instruction, API pairs into training and testing sets.4.1AST Accuracy on API callFigure 5:Accuracy with GPT-retriever.Gorilla outperforms on Torch Hub and Hugging-Face while matching performance on Tensorflow Hub for all existing SoTA LLMs - closed source, and open source.We first demonstrate the results for the AST accuracy for different models. We present the results in Tab.1. We test each model for different retriever settings defined above. We report the overall accuracy, the error by hallucination and the error by selecting wrong API call. Note that for TorchHub and TensorHub, we evaluate all the models using AST tree accuracy score. However, for HuggingFace, since the dataset is not exhaustive, for all the models except Gorilla, we only check if they can provide the correct domain names. So this problem reduces to picking one of the multiple choices.Finetuning without RetrievalIn Tab.1we show that lightly fine-tuned Gorilla gets the state-of-the-art performance zero-shot over all the models, 20.43% better than GPT-4 and 10.75% better than ChatGPT. When compared to other open-source models LLAMA, the improvement is as big as 83%. his suggests quantitatively, that finetuning is better than retrieval, at-least in our scope.In addition, we found that finetuning without retriever and putting ground truth retriever in evaluation time rarely helps the performance: 0.88% worse in TensorHub and 0.97% better in HuggingFace. If we put BM25 or GPT-Index as retriever, results will be significantly dropped: 21.50% in Torch Hub and 47.57% in HuggingFace. The result illustrates that adding a non-optimal retriever at test time will sometime misguide the model and result in more errors. We will discuss an interesting ablation on how finetuning with the retriever will help the performance in the next paragraph.Table 1:Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIsLLM (retriever)TorchHubHuggingFaceTensorFlow Huboverall↑↑\\uparrowhallu↓↓\\downarrowerr↓↓\\downarrowoverall↑↑\\uparrowhallu↓↓\\downarrowerr↓↓\\downarrowoverall↑↑\\uparrowhallu↓↓\\downarrowerr↓↓\\downarrowLLAMA (0-shot)010000.0097.572.4301000GPT-3.5 (0-shot)48.3818.8132.7916.8135.7347.4641.7547.8810.36GPT-4 (0-shot)38.7036.5524.719.8037.1643.0318.2078.653.13Claude (0-shot)18.8165.5915.596.1977.6516.159.1988.462.33Gorilla (0-shot)59.136.9833.8771.6810.9517.3683.795.4010.80LLAMA (BM-25)8.6076.8814.513.0077.9919.028.9077.3713.72GPT-3.5 (BM-25)38.176.9854.8317.268.3074.4454.163.6442.18GPT-4 (BM-25)35.4811.2953.2216.4815.9367.5934.0137.0828.90Claude (BM-25)39.785.3754.8314.6015.8269.5835.1821.1643.64Gorilla (BM-25)40.324.3055.3717.036.4276.5541.892.7755.32LLAMA (GPT-Index)14.5175.89.6710.1875.6614.2015.6277.666.71GPT-3.5 (GPT-Index)60.211.6138.1729.087.8544.8065.593.7930.50GPT-4 (GPT-Index)59.131.0739.7844.5811.1844.2543.9431.5324.52Claude (GPT-Index)60.213.7636.0241.3718.8139.8255.6216.2028.17Gorilla (GPT-Index)61.82038.1747.468.1944.3664.962.3332.70LLAMA (Oracle)16.1279.034.8317.7077.105.2012.5587.000.43GPT-3.5 (Oracle)66.311.6032.0889.716.643.6595.030.294.67GPT-4 (Oracle)66.120.5333.3385.0710.624.3155.9137.956.13Claude (Oracle)63.443.7632.7977.2119.583.2174.7421.603.64Gorilla (Oracle)67.20032.7991.267.081.6694.161.893.94Finetuning with RetrievalWe now discuss an interesting experiment on how finetuning language with retriever incorporated is helping the performance. The settings for this experiment are finetuning the base LLAMA with the prompt (instruction generated), reference API document (from golden-truth oracle), and the example output generated by GPT-4. In Tab.2, we can see that incorporating ground truth retriever in the finetuning pipeline achieves significantly better results 12.37% better than training without retriever in Torch Hub and 23.46% better in HuggingFace. However, we found that at evaluation time, current retrievers still have a big gap between the ground truth retriever: using GPT-Index at evaluation results in 29.20% accuracy degradation, and using BM25 results in a 52.27% accuracy degradation. Nevertheless, we can still conclude that with a better retriever, finetuning with retriever is still a better method to adopt while in another scenario, when a good retriever is not available, zero-shot finetuning might be the preferred choice.Table 2:Comparison of retrieval techniquesGorilla without RetrieverGorilla with Oracle retrieverzero-shotBM25GPT-IndexOraclezero-shotBM25GPT-IndexOracleTorch Hub (overall)↑↑\\uparrow59.1337.6360.2154.83040.3261.8267.20HuggingFace (overall)↑↑\\uparrow71.6811.2828.1045.58017.0447.4691.26TensorHub (overall)↑↑\\uparrow83.7934.3052.4082.91041.8964.9694.16Torch Hub (Hallu)↓↓\\downarrow6.9811.294.3015.591004.3000HuggingFace (Hallu)↓↓\\downarrow10.9546.4641.4852.7799.676.428.197.08TensorHub (Hallu)↓↓\\downarrow5.4020.4319.7013.281002.772.331.89Hallucination with LLMOne phenomenon we observe is that zero-shot prompting with LLMs (GPT-4/GPT-3.5) to call APIs results in dire hallucination errors. These errors, while diverse, commonly manifest in erroneous behavior such as the model invoking the \"AutoModel.from_pretrained(dir_name)\" command with arbitrary GitHub repository names. Surprisingly, we also found that in TorchHub, HuggingFace and TensorFlow Hub, GPT-3.5 has less hallucination errors than GPT-4. This finding is also consistent for the settings when various retrieving methods are provided: 0-shot, BM25, GPT-Index and the oracle. This might suggest that RLHF plays a central role in turning the model to be truthful. Additional examples and discussion are in Appendix.4.2Test-Time Documentation ChangeFigure 6:Gorilla’s retriever–aware training enables it to react to changes in the APIs.The second column demonstrates changes in model  upgrading FCN’s ResNet–50 backbone to ResNet–101. The third column demonstrate changes in model registry frompytorch/visiontoNVIDIA/DeepLearningExamples:torchhubThe rapidly evolving nature of API documentation presents a significant challenge for the application of LLMs in this field. These documents are often updated at a frequency that outpaces the re-training or fine-tuning schedule of LLMs, making these models particularly brittle to changes in the information they are designed to process. This mismatch in update frequency can lead to a decline in the utility and reliability of LLMs over time.However, with the introduction of Gorilla’s retriever-aware training, we can readily adapt to changes in API documentation. This novel approach allows the model to remain updated and relevant, even as the API documentation it relies on undergoes modifications. This is a pivotal advancement in the field, as it ensures that the LLM maintains its efficacy and accuracy over time, providing reliable outputs irrespective of changes in the underlying documentation.For instance, consider the scenario illustrated in Figure 6, where the training of Gorilla has allowed it to react effectively to changes in APIs. This includes alterations such as upgrading the FCN’s ResNet-50 backbone to ResNet-101, as demonstrated in the second column of the figure. This capability ensures that the LLM remains relevant and accurate even as the underlying models and systems undergo upgrades and improvements.\n","Furthermore, the third column in Figure 6 shows how Gorilla adapts to changes in the model registry frompytorch/visiontoNVIDIA/DeepLearningExamples:torchhub. This reflects the model’s ability to adjust to shifts in API sources, which is vital as organizations may change their preferred model registries over time.In summary, Gorilla’s ability to adapt to test-time changes in API documentation offers numerous benefits. It maintains its accuracy and relevance over time, adapts to the rapid pace of updates in API documentation, and adjusts to modifications in underlying models and systems. This makes it a robust and reliable tool for API calls, significantly enhancing its practical utility.4.3API Call with ConstraintsWe now focus on the language model’s capability of understanding constraints. For any given task, which API call to invoke is typically a tradeoff between a multitude of factors. In the case of RESTFul APIs, it could be the cost of each invocation ($), and the latency of response (ms), among others. Similarly, within the scope of ML APIs, it is desirable for Gorilla to respect constraints such as accuracy, number of learnable parameters in the model, the size on disk, peak memory consumption, FLOPS, etc. We present the underlying ablation study evaluating the ability of different models in zero-shot and with retrievers settings to respect a given accuracy constraint. This setting is best understood with an example. If the user were to ask for an Image classification model that achieves at least 80% top-1 accuracy on the Imagenet dataset, then while both are classification models hosted by Torch Hub,ResNeXt-101 32x16dwith a top-1 accuracy of 84.2% would be the right model whose API to call and not, say,MobileNetV2which has a top-1 accuracy of 71.88%.Table 3:Evaluating LLMs on constraint-aware API invocationsGPT-3.5GPT-4Gorilla0-shotBM25GPT-IndexOracle0-shotBM25GPT-IndexOracle0-shotBM25GPT-IndexOracleTorch Hub (overall)73.9462.6781.6980.9862.6756.3371.1169.0171.8357.0471.8378.16Torch Hub (Hallu)19.0130.9814.7814.0815.4927.4614.089.1519.7139.4326.0516.90Torch Hub (err)7.046.333.524.9221.8316.1914.7821.838.453.522.114.92Accuracy const43.6633.8033.0969.0143.6629.5729.5759.1547.8830.2826.7667.60LLAMAClaude0-shotBM25GPT-IndexOracle0-shotBM25GPT-IndexOracleTorch Hub (overall)08.4511.9719.7129.9281.6982.3981.69Torch Hub (Hallu)10091.5488.0278.8767.2516.1915.4913.38Torch Hub (err)0001.42.812.112.114.92Accuracy const06.333.5217.6017.2529.5731.6969.71For Table3, we filtered a subset of the Torch Hub dataset that had accuracy defined for at least one-dataset in its model card (65.26% of TorchHub dataset in Table1). We notice that with constraints, understandably, the accuracy drops across all models, with and without a retriever. Gorilla is able to match performance with the best-performing model GPT-3.5 when using retrievals (BM25, GPT-Index) and has the highest accuracy in the Zero-shot case. This highlights Gorilla’s ability to navigate APIs while considering the trade-offs between different constraints.\n","  5ConclusionLLMs are swiftly gaining popularity across diverse domains. In our study, we spotlight techniques designed to enhance the LLM’s ability to accurately identify the appropriate API for a specific task—a significant but often overlooked aspect in the advancement of this technology. Since APIs function as a universal language enabling diverse systems to communicate effectively, their correct usage can boost the ability of LLMs to interact with tools in the wider world.\n","In this paper, we propose Gorilla, a new novel pipeline for finetuning LLMs to call APIs. The finetuned model’s performance surpasses prompting the state-of-the-art LLM (GPT-4) in three massive datasets we collected. Gorilla generates reliable API calls to ML models without hallucination, demonstrates an impressive capability to adapt to test-time API usage changes, and can satisfy constraints while picking APIs.\n","  6Limitations & Social ImpactsWith the goal of wanting to have a challenging dataset, we chose ML APIs, given their functional similarity. The potential downside to APIs that focus on the ML domain, is their propensity to produce biased predictions if trained on skewed data, potentially disadvantaging certain sub-groups. To counter this concern and foster a deeper understanding of these APIs, we are releasing our extensive dataset, consisting of over 11,000 instruction-API pairs. This resource will serve the wider community as a valuable tool for studying and benchmarking existing APIs, contributing to a more fair and optimized usage of machine learning.\n","  7AcknowledgementThis research is supported in part by gifts to UC Berkley Sky Computing Lab from Astronomer, Google, IBM, Intel, Lacework, Microsoft, Nexla, Samsung SDS, Uber, and VMware.\n","  ReferencesAhn et al.,  [2022]Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C.,\n","Gopalakrishnan, K., Hausman, K., Herzog, A., et al. (2022).Do as i can, not as i say: Grounding language in robotic affordances.arXiv preprint arXiv:2204.01691.Andor et al.,  [2019]Andor, D., He, L., Lee, K., and Pitler, E. (2019).Giving bert a calculator: Finding operations and arguments with\n","reading comprehension.arXiv preprint arXiv:1909.00109.Anthropic,  [2022]Anthropic, h.-c. (2022).Claude.Bavishi et al.,  [2019]Bavishi, R., Lemieux, C., Fox, R., Sen, K., and Stoica, I. (2019).Autopandas: neural-backed generators for program synthesis.Proceedings of the ACM on Programming Languages,\n","3(OOPSLA):1–27.Brown et al.,  [2020]Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,\n","Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020).Language models are few-shot learners.Advances in neural information processing systems,\n","33:1877–1901.Bubeck et al.,  [2023]Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E.,\n","Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. (2023).Sparks of artificial general intelligence: Early experiments with\n","gpt-4.arXiv preprint arXiv:2303.12712.Chen et al.,  [2021]Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J.,\n","Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021).Evaluating large language models trained on code.arXiv preprint arXiv:2107.03374.Chen et al.,  [2023]Chen, X., Lin, M., Schärli, N., and Zhou, D. (2023).Teaching large language models to self-debug.arXiv preprint arXiv:2304.05128.Chiang et al.,  [2023]Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L.,\n","Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. (2023).Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt\n","quality.Chowdhery et al.,  [2022]Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\n","Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2022).Palm: Scaling language modeling with pathways.arXiv preprint arXiv:2204.02311.Chung et al.,  [2022]Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang,\n","X., Dehghani, M., Brahma, S., et al. (2022).Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416.Cobbe et al.,  [2021]Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,\n","M., Tworek, J., Hilton, J., Nakano, R., et al. (2021).Training verifiers to solve math word problems.arXiv preprint arXiv:2110.14168.Devlin et al.,  [2017]Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A.-r., and Kohli,\n","P. (2017).Robustfill: Neural program learning under noisy i/o.InInternational conference on machine learning, pages\n","990–998. PMLR.Gao et al.,  [2022]Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and\n","Neubig, G. (2022).Pal: Program-aided language models.arXiv preprint arXiv:2211.10435.Iyer et al.,  [2022]Iyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster,\n","K., Wang, T., Liu, Q., Koura, P. S., et al. (2022).Opt-iml: Scaling language model instruction meta learning through the\n","lens of generalization.arXiv preprint arXiv:2212.12017.Jain et al.,  [2022]Jain, N., Vaidyanath, S., Iyer, A., Natarajan, N., Parthasarathy, S., Rajamani,\n","S., and Sharma, R. (2022).Jigsaw: Large language models meet program synthesis.InProceedings of the 44th International Conference on Software\n","Engineering, pages 1219–1231.Kim et al.,  [2023]Kim, G., Baldi, P., and McAleer, S. (2023).Language models can solve computer tasks.arXiv preprint arXiv:2303.17491.Kojima et al.,  [2022]Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022).Large language models are zero-shot reasoners.arXiv preprint arXiv:2205.11916.Komeili et al.,  [2021]Komeili, M., Shuster, K., and Weston, J. (2021).Internet-augmented dialogue generation.arXiv preprint arXiv:2107.07566.Lachaux et al.,  [2020]Lachaux, M.-A., Roziere, B., Chanussot, L., and Lample, G. (2020).Unsupervised translation of programming languages.arXiv preprint arXiv:2006.03511.Lazaridou et al.,  [2022]Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. (2022).Internet-augmented language models through few-shot prompting for\n","open-domain question answering.arXiv preprint arXiv:2203.05115.Li et al.,  [2023]Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone,\n","M., Akiki, C., Li, J., Chim, J., et al. (2023).Starcoder: may the source be with you!arXiv preprint arXiv:2305.06161.Li et al.,  [2022]Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R.,\n","Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. (2022).Competition-level code generation with alphacode.Science, 378(6624):1092–1097.Liang et al.,  [2023]Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S., Ji, L.,\n","Mao, S., et al. (2023).Taskmatrix. ai: Completing tasks by connecting foundation models with\n","millions of apis.arXiv preprint arXiv:2303.16434.Menon et al.,  [2013]Menon, A., Tamuz, O., Gulwani, S., Lampson, B., and Kalai, A. (2013).A machine learning framework for programming by example.InInternational Conference on Machine Learning, pages\n","187–195. PMLR.Nakano et al.,  [2021]Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C.,\n","Jain, S., Kosaraju, V., Saunders, W., et al. (2021).Webgpt: Browser-assisted question-answering with human feedback.arXiv preprint arXiv:2112.09332.Nijkamp et al.,  [2023]Nijkamp, E., Hayashi, H., Xiong, C., Savarese, S., and Zhou, Y. (2023).Codegen2: Lessons for training llms on programming and natural\n","languages.arXiv preprint arXiv:2305.02309.Nijkamp et al.,  [2022]Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S.,\n","and Xiong, C. (2022).Codegen: An open large language model for code with multi-turn\n","program synthesis.arXiv preprint arXiv:2203.13474.OpenAI,  [2023]OpenAI (2023).Gpt-4 technical report.OpenAI and https://openai.com/blog/chatgpt,  [2022]OpenAI and https://openai.com/blog/chatgpt (2022).Chatgpt.Sanh et al.,  [2021]Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z.,\n","Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. (2021).Multitask prompted training enables zero-shot task generalization.arXiv preprint arXiv:2110.08207.Scao et al.,  [2022]Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D.,\n","Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., et al. (2022).Bloom: A 176b-parameter open-access multilingual language model.arXiv preprint arXiv:2211.05100.Schick et al.,  [2023]Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M.,\n","Zettlemoyer, L., Cancedda, N., and Scialom, T. (2023).Toolformer: Language models can teach themselves to use tools.arXiv preprint arXiv:2302.04761.Schick and Schütze,  [2020]Schick, T. and Schütze, H. (2020).Exploiting cloze questions for few shot text classification and\n","natural language inference.arXiv preprint arXiv:2001.07676.Shen et al.,  [2023]Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. (2023).Hugginggpt: Solving ai tasks with chatgpt and its friends in\n","huggingface.arXiv preprint arXiv:2303.17580.Shinn et al.,  [2023]Shinn, N., Labash, B., and Gopinath, A. (2023).Reflexion: an autonomous agent with dynamic memory and\n","self-reflection.arXiv preprint arXiv:2303.11366.Shuster et al.,  [2022]Shuster, K., Xu, J., Komeili, M., Ju, D., Smith, E. M., Roller, S., Ung, M.,\n","Chen, M., Arora, K., Lane, J., et al. (2022).Blenderbot 3: a deployed conversational agent that continually learns\n","to responsibly engage.arXiv preprint arXiv:2208.03188.Taori et al.,  [2023]Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang,\n","P., and Hashimoto, T. B. (2023).Stanford alpaca: An instruction-following llama model.https://github.com/tatsu-lab/stanford_alpaca.Thoppilan et al.,  [2022]Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,\n","H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. (2022).Lamda: Language models for dialog applications.arXiv preprint arXiv:2201.08239.Touvron et al.,  [2023]Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,\n","T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023).Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971.Vemprala et al.,  [2023]Vemprala, S., Bonatti, R., Bucker, A., and Kapoor, A. (2023).Chatgpt for robotics: Design principles and model abilities.2023.[42]Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and\n","Hajishirzi, H. (2022a).Self-instruct: Aligning language model with self generated\n","instructions.arXiv preprint arXiv:2212.10560.[43]Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A.,\n","Ashok, A., Dhanasekaran, A. S., Arunkumar, A., Stap, D., et al. (2022b).Super-naturalinstructions: Generalization via declarative\n","instructions on 1600+ nlp tasks.InProceedings of the 2022 Conference on Empirical Methods in\n","Natural Language Processing, pages 5085–5109.Wei et al.,  [2022]Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D.\n","(2022).Chain of thought prompting elicits reasoning in large language\n","models.arXiv preprint arXiv:2201.11903.Xu et al.,  [2022]Xu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. (2022).A systematic evaluation of large language models of code.InProceedings of the 6th ACM SIGPLAN International Symposium on\n","Machine Programming, pages 1–10.Yao et al.,  [2022]Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y.\n","(2022).React: Synergizing reasoning and acting in language models.arXiv preprint arXiv:2210.03629.Zeng et al.,  [2022]Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y.,\n","Zheng, W., Xia, X., et al. (2022).Glm-130b: An open bilingual pre-trained model.arXiv preprint arXiv:2210.02414.Zhang et al.,  [2022]Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,\n","Diab, M., Li, X., Lin, X. V., et al. (2022).Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068.\n","  \n","  \n","  8Appendix8.1Dataset DetailsOur dataset is multi-faceted, comprising three distinct domains: Torch Hub, Tensor Hub, and HuggingFace. Each entry within this dataset is rich in detail, carrying critical pieces of information that further illuminate the nature of the data. Delving deeper into the specifics of each domain, Torch Hub provides 95 APIs. The second domain, Tensor Hub, is more expansive with a total of 696 APIs. Finally, the most extensive of them all, HuggingFace, comprises 925 APIs.To enhance the value and utility of our dataset, we’ve undertaken an additional initiative. With each API, we have generated a set of 10 unique instructions. These instructions, carefully crafted and meticulously tailored, serve as a guide for both training and evaluation. This initiative ensures that every API is not just represented in our dataset, but is also comprehensively understood and effectively utilizable.In essence, our dataset is more than just a collection of APIs across three domains. It is a comprehensive resource, carefully structured and enriched with added layers of guidance and evaluation parameters.Domain ClassificationThe unique domain names encompassed within our dataset are illustrated in Figure7. The dataset consists of three sources with a diverse range of domains: Torch Hub houses 6 domains, Tensor Hub accommodates a much broader selection with 57 domains, while HuggingFace incorporates 37 domains. To exemplify the structure and nature of our dataset, we invite you to refer to the domain names represented in Figure8.API Call TaskIn this task, we test the model’s capability to generate a single line of code, either in a zero-shot fashion or by leveraging an API reference. Primarily designed for evaluation purposes, this task effectively gauges the model’s proficiency in identifying and utilizing the appropriate API call.API Provider ComponentThis facet relates to the provision of the programming language. In this context, the API provider plays a vital role as it serves as a foundation upon which APIs are built and executed.Explanation ElementThis component offers valuable insights into the rationale behind the usage of a particular API, detailing how it aligns with the prescribed requirements. Furthermore, when certain constraints are imposed, this segment also incorporates those limitations. Thus, the explanation element serves a dual purpose, offering a deep understanding of API selection, as well as the constraints that might influence such a selection. This balanced approach ensures a comprehensive understanding of the API usage within the given context.CodeExample code for accomplishing the task. We de-prioritize this as we haven’t tested the execution result of the code. We leave this for future works, but make this data available in-case others want to build on it.Torch Hub domain names: Classification, Semantic Segmentation, Object Detection, Audio Separation, Video Classification, Text-to-SpeechTensor Hub domain names: text-sequence-alignment, text-embedding, text-language-model, text-preprocessing, text-classification, text-generation, text-question-answering, text-retrieval-question-answering, text-segmentation, text-to-mel, image-classification, image-feature-vector, image-object-detection, image-segmentation, image-generator, image-pose-detection, image-rnn-agent, image-augmentation, image-classifier, image-style-transfer, image-aesthetic-quality, image-depth-estimation, image-super-resolution, image-deblurring, image-extrapolation, image-text-recognition, image-dehazing, image-deraining, image-enhancemenmt, image-classification-logits, image-frame-interpolation, image-text-detection, image-denoising, image-others, video-classification, video-feature-extraction, video-generation, video-audio-text, video-text, audio-embedding, audio-event-classification, audio-command-detection, audio-paralinguists-classification, audio-speech-to-text, audio-speech-synthesis, audio-synthesis, audio-pitch-extractionHuggingFace domain names: Multimodal Feature Extraction, Multimodal Text-to-Image, Multimodal Image-to-Text, Multimodal Text-to-Video, Multimodal Visual Question Answering, Multimodal Document Question Answer, Multimodal Graph Machine Learning, Computer Vision Depth Estimation, Computer Vision Image Classification, Computer Vision Object Detection, Computer Vision Image Segmentation, Computer Vision Image-to-Image, Computer Vision Unconditional Image Generation, Computer Vision Video Classification, Computer Vision Zero-Shor Image Classification, Natural Language Processing Text Classification, Natural Language Processing Token Classification, Natural Language Processing Table Question Answering, Natural Language Processing Question Answering, Natural Language Processing Zero-Shot Classification, Natural Language Processing Translation, Natural Language Processing Summarization, Natural Language Processing Conversational, Natural Language Processing Text Generation, Natural Language Processing Fill-Mask, Natural Language Processing Text2Text Generation, Natural Language Processing Sentence Similarity, Audio Text-to-Speech, Audio Automatic Speech Recognition, Audio Audio-to-Audio, Audio Audio Classification, Audio Voice Activity Detection, Tabular Tabular Classification, Tabular Tabular Regression, Reinforcement Learning Reinforcement Learning, Reinforcement Learning RoboticsFigure 7:Domain names: Domain names with the three dataset. Tensor Hub is the smallest dataset while the other two hubs contain many more models.### User: Our customer is a zoo and we want to help them detect movement of different animals. Write a python program in 1 to 2 lines to call API in TensorFlowHub.### Assistant:<domain>: image-object-detection,<api_call>: detector = hub.load(’https://tfhub.dev/google/openimages_v4/ssd_mobilenet_v2/1’),<api_provider>: TensorFlow Hub,<explanation>: This API call loads the pre-trained Single Shot Detector (SSD) MobileNet V2 model from TensorFlow Hub, which has been trained on the Open Images v4 dataset. This model is used for object detection in images. Once loaded, you can pass an image (and an optional score threshold for detection confidence) to the detector to get back the detected objects along with their bounding boxes, class labels, and associated confidence scores.<code>: import tensorflow as tf; import tensorflow_hub as hub; # Load the SSD MobileNet V2 model; detector = hub.load(’https://tfhub.dev/google/openimages_v4/ssd_mobilenet_v2/1’); # Use the model to detect objects in an image (assuming ’image’ is a TensorFlow tensor representing your image); result = detector(image, score_threshold=0.5)### User: I am an engineer at Uber and I need to find a API that can classify pedestrians, cars, etc. from an image of the scene. Write a python program in 1 to 2 lines to call API in TorchHub.<Reference API>: {domain: Object Detection, framework: PyTorch, functionality: Traffic Object Detection, Drivable Area Segmentation, Lane Detection, api_name: HybridNets, api_call: torch.hub.load(repo_or_dir=’datvuthanh/hybridnets’, model=’hybridnets’, pretrained=True), api_arguments: repo_or_dir, model, pretrained, python_environment_requirements: Python>=3.7, PyTorch>=1.10, example_code: ’import torch; model = torch.hub.load(’datvuthanh/hybridnets’, ’hybridnets’, pretrained=True); img = torch.randn(1,3,640,384); features, regression, classification, anchors, segmentation = model(img), performance: dataset: [name: BDD100K, accuracy: Traffic Object Detection: Recall (%): 92.8, mAP@0.5 (%): 77.3, Drivable Area Segmentation: Drivable mIoU (%): 90.5, Lane Line Detection: Accuracy (%): 85.4, Lane Line IoU (%): 31.6], description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.}### Assistant:<domain>: Object Detection,<api_call>: model = torch.hub.load(’datvuthanh/hybridnets’, ’hybridnets’, pretrained=True),<api_provider>: PyTorch,<explanation>: Load the pretrained HybridNets model for object detection from PyTorch Hub to classify pedestrians, cars, etc. in an image of the scene,<code>: import torch; model = torch.hub.load(’datvuthanh/hybridnets’, ’hybridnets’, pretrained=True)Figure 8:Example of the Dataset: Two examples of the dataset, the above one is zero-shot (without information retrievers) and the bottom one is with information retriever.8.2Gorilla DetailsWe provide all the training details for Gorilla in this section. This includes how we divide up the training, evaluation dataset, training hyperparameters for Gorilla.DataFor HuggingFace, we devise the entire dataset into 90% training and 10% evaluation. For Torch Hub and Tensor Hub, we devise the data in to 80% training and 20% testing.TrainingWe train Gorillafor 5 epochs with the 2e-5 learning rate with cosine decay. The details are provide in Tab.4. We finetune it on 8xA100 with 40G memory each.Table 4:Hyperparameters for training GorillaHyperparameter NameValuelearning rate2e-5batch size64epochs5warmup ratio0.03weight decay0max seq length20488.3Performance ComparisonWe provide a full comparison of each model’s performance in this section. In Fig10and Fig.11, the full set of comparisons is provided. We see that especially in zero-shot case, Gorilla surpasses the GPT-4 and GPT-3.5 by a large margin. The GPT-4 and GPT-3.5 gets around 40% accuracy in Torch Hub and Tensor Hub, which are two structured API calls. Compared to that, HuggingFace is a more flexible and diverse Hub, as a result, the performance on HuggingFace is not as competitive.8.3.1EvaluationFor ease of evaluation, we manually cleaned up the dataset to make sure each API call domain only contains the valid call in the form of:API_name(API_arg1subscriptarg1\\mathrm{arg_{1}}, API_arg2subscriptarg2\\mathrm{arg_{2}}, …, API_argksubscriptargk\\mathrm{arg_{k}})Our framework allows the user to define any combination of the arguments to check. For Torch Hub, we check for the API nametorch.hub.loadwith argumentsrepo_or_dirandmodel. For Tensor Hub, we check API namehub.KerasLayerandhub.loadwith argumenthandle. For HuggingFace, since there are many API function names, we don’t list all of them here. One specific note is that we require thepretrained_model_name_or_pathargument for all the calls except forpipeline. Forpipeline, we don’t require thepretrained_model_name_or_pathargument since it automatically select a model for you oncetaskis specified.8.3.2HallucinationWe found especially in HuggingFace, the GPT-4 model incurs serious hallucination problems. It would sometimes put a GitHub name that is not associated with the HuggingFace repository in to the domain ofpretrained_model_name_or_path. Fig.9demonstrates some examples and we also observe that GPT-4 sometimes assumes the user have a local path to the model likeyour_model_name. This is greatly reduced by Gorilla as we see the hallucination error comparison in Tab.1.generate_video = pipeline(\"text-to-video\", model=\"your_model_name\")vqa = pipeline(\"visual-question-answering\", model=\"microsoft/clip-vqa-base\", tokenizer=\"microsoft/clip-vqa-base\")depth_estimator = pipeline(\"depth-estimation\", model=\"intel-isl/MiDaS\", tokenizer=\"intel-isl/MiDaS\")Figure 9:Hallucination Examples: GPT-4 incurs serious hallucination errors in HuggingFace. We show a couple of examples in the figure.Figure 10:Performance: We plot each model’s performance on different configurations. We see that Gorilla performs extremely well in the zero-shot setting. While even when the oracle answer is given, Gorilla is still the best.Figure 11:Accuracy vs Hallucination: We plot each model’s performance on different configurations. We found that in the zero-shot setting, Gorilla has the most accuracy gain while maintaining good factual capability. When prompting with different retrievers, Gorilla is still capable to avoid the hallucination errors.\n","1Introduction\n","  Recent advances in large language models (LLMs)[10,5,32,6,29,30]have enabled significant new capabilities including natural dialogue, mathematical reasoning, and program synthesis.\n","However, despite these advances, LLMs are still fundamentally limited by the information they can store in a fixed set of weights and the things they can compute using a static computation graph and limited context.\n","Furthermore, as the world changes, LLMs require retraining to update their knowledge and reasoning capabilities.\n","  By empowering LLMs to use tools[33], we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks.\n","By providing access to search technologies and databases,[26,39,37]demonstrated that we can augment LLMs to address a significantly larger and more dynamic knowledge space.\n","Similarly, by providing access to computational tools,[39,2]demonstrated that LLMs can accomplish complex computational tasks.\n","Consequently, leading LLM providers[29], have started to integrate plugins to allow LLMs to invoke external tools through APIs.\n","  This transition from a small set of hand-coded tools, to the ability to invoke a vast space of changing cloud APIs could transform LLMs into the primary interface to computing infrastructure and the web.\n","Tasks ranging from booking an entire vacation to hosting a conference, could become as simple as talking to an LLM that has access to the flight, car rental, hotel, catering, and entertainment web APIs.\n","However, much of the prior work[35,24]integrating tools into LLMs considered a small well documented set of APIs that can be easily injected into the prompt.\n","  Supporting a web scale collection of potentially millions of changing APIs requires rethinking our approach to how we integrate tools.\n","It is not longer possible to describe the full set of APIs in a single context.\n","Many of the APIs will have overlapping functionality with nuanced limitations and constraints.\n","Simply evaluating LLMs in this new setting requires new benchmarks.\n","  Figure 1:Examples of API calls. Example API calls generated by GPT-4[29], Claude[3], and Gorilla for the given prompt. In this example, GPT-4 presents a model that doesn’t exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call.\n","  In this paper, we explore the use of self-instruct fine-tuning and retrieval to enable LLMs to accurately select from a large, overlapping, and changing set tools expressed using their APIs and API documentation.\n","We construct, APIBench, a large corpus of APIs with complex and often overlapping functionality by scraping ML APIs (models) from public model hubs.\n","We choose three major model hubs for dataset construction: TorchHub, TensorHub and HuggingFace. We exhaustively include every API call in TorchHub (94 API calls) and TensorHub (696 API calls); For HuggingFace, since the models come in a large number and lots of the models don’t have a specification, we choose the most downloaded 20 models per task category (in a total of 925). We also generate 10 synthetic user question prompts per API using Self-Instruct[42]. Thus, each entry in the dataset becomes an instruction reference API pair. We adopt a common AST sub-tree matching technique to evaluate the functional correctness of the generated API. We first parse the generated code into an AST tree, then find a sub-tree whose root node is the API call that we care about (e.g.,torch.hub.load) and use it to index our dataset. We check the functional correctness and hallucination problem for the LLMs, reporting the corresponding accuracy.\n","  We then finetune Gorilla, a LLaMA-7B-based model with document retrieval using our dataset. We find that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy as well as reducing hallucination errors. We show an example output in Fig.1. Further, our retrieval-aware training of Gorilla enables the model to adapt to changes in the API documentation. Finally, we demonstrate Gorilla’s ability to understand and reason about constraints.\n","  Figure 2:Accuracy (vs) hallucinationin four settings, that is,zero-shot(i.e., without any retriever), andwith retrievers.BM25andGPTare commonly used retrievers and theoracleretriever returns relevant documents at 100%, indicating an upper bound. Higher in the graph (higher accuracy) and to the left is better (lower hallucination). Across the entire dataset, our model, Gorilla, improves accuracy while reducing hallucination.\n","3.1Dataset Collection\n","  To collect the dataset, we meticulously recorded all online model cards for HuggingFace’s “The Model Hub”, PyTorch Hub, and TensorFlow Hub Models. Throughout the rest of the paper, we call these HuggingFace, Torch Hub, and TensorFlow Hub respectively for brevity.\n","  API DocumentationThe HuggingFace platform hosts and servers about 203,681 models. However, many of them have poor documentation, lack dependencies, have no information in their model card, etc. To filter these out, we pick the top 20 models from each domain.\n","We consider 7 domains in multimodal data, 8 in CV, 12 in NLP, 5 in Audio, 2 in tabular data, and 2 in reinforcement learning. Post filtering, we got a total of 925 models from HuggingFace. TensorFlow Hub is versioned into v1 and v2. The latest version (v2) has 801 models in total, and we process all of them. Post filtering out models, whose mode cards had little to no information, we are left with 626 models. Similar to TensorFlow Hub, we get 95 models from Torch Hub.\n","We then converted the model cards for each of these 1,645 API calls into a json object with the following fields: {domain, framework, functionality, api_name, api_call, api_arguments, environment_requirements, example_code, performance, and description.}. We provide more information in the Appendix. These fields were chose to generalize beyond the API calls within ML domain, to other domains, includin RESTful API calls.\n","  Instruction GenerationGuided by the self-instruct paradigm[42], we employed GPT-4 to generate synthetic instruction data.\n","We provided three in-context examples, along with a reference API documentation, and tasked the model with generating real-world use cases that call upon the API. We specifically instructed the model to refrain from using any API names or hints when creating instructions. We constructed six examples (Instruction-API pairs) for each of the three model hubs. These 18 points, were the only hand-generated or modified data. For each of our 1,645 API datapoints, we sample 3 of 6 corresponding instruction examples to generate a total of 10 instruction-api pairs as demonstrated in Figure3. We would like to highlight that we only need to employ GPT-4 to generate the instructions and this can be swapped with open-source alternatives such as LLaMA, Alpaca, etc.\n","2Related Work\n","  Large Language ModelsRecent strides in the field of LLMs have renovated many downstream domains[10,40,48,47], not only in traditional natural language processing tasks but also in program synthesis. Many of these advances are achieved by augmenting pre-trained LLMs by prompting[44,14]and instruction fine-tuning[11,31,43,15]. Recent open-sourced models like\n","LLaMa[40], Alpaca[38], and Vicuna[9]have furthered the understanding of LLMs and facilitated their experimentation. While our approach, Gorilla, incorporates techniques akin to those mentioned, its primary emphasis is on enhancing the LLMs’ ability to utilize millions of tools, as opposed to refining their conversational skills. Additionally, we pioneer the study of fine-tuning a base model by supplementing it with information retrieval - a first, to the best of our knowledge.\n","  Tool UsageThe discussion of tool usage within LLMs has seen an upsurge, with models like Toolformer taking the lead[33,19,21,26]. Tools often incorporated include web-browsing[34], calculators[12,39], translation systems[39], and Python interpreters[14].\n","While these efforts can be seen as preliminary explorations of marrying LLMs with tool usage, they generally focus on specific tools. Our paper, in contrast, aims to explore a vast array of tools (i.e., API calls) in an open-ended fashion, potentially covering a wide range of applications.With the recent launch of Toolformer[33]and GPT-4[29], the importance of API calls has been highlighted, encouraging many works in employing API calls as tooling[35,24]. Moreover, the application of API calls in robotics has been explored to some extent[41,1]. However, these works primarily aim at showcasing the potential of “prompting” LLMs rather than establishing a systematic method for evaluation and training (including fine-tuning). Our work, on the other hand, concentrates on systematic evaluation and building a pipeline for future use.\n","  LLMs for Program SynthesisHarnessing LLMs for program synthesis has historically been a challenging task[23,7,45,16,13,20]. Researchers have proposed an array of strategies to prompt LLMs to perform better in coding tasks, including in-context learning[44,18,7], task decomposition[17,46], and self-debugging[8,36]. Besides prompting, there have also been efforts to pretrain language models specifically for code generation[28,22,27].However, these strategies focus on prompting large language models or pre-training them for general program synthesis. In our research, in contrast, we focus on a much restricted domain: the synthesis of linear programs using API calls. General program synthesis, not only is complex, but is also hard to verify and evaluate. API calls, on the other hand, function more like tool usage. This allows the LLM to significantly expand its capabilities without grappling with low-level implementation details.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"FXssQdXKKN-p"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["79YmvumUw19_"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f98af812a74c42c1a10530a337bdc3fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f60128c644f4206a09b39d457fe70fd","IPY_MODEL_43edb99773664e07a3a0763e2f6ed864","IPY_MODEL_8fc11f16025e420baad59b007db15e62"],"layout":"IPY_MODEL_23c9f7d665c348318aab143ab863c8f2"}},"2f60128c644f4206a09b39d457fe70fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbb679ba053044369e0d2ddbc6c0867b","placeholder":"​","style":"IPY_MODEL_24f0c57b5f2944689d029d90237f8a3b","value":"tokenizer_config.json: 100%"}},"43edb99773664e07a3a0763e2f6ed864":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_21d3e57759574bf6b63a867bc6278392","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c55df13de0ca47a5a9a7e9e8ca15805b","value":350}},"8fc11f16025e420baad59b007db15e62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a400940465b8464aaf2bbdd6a226dc59","placeholder":"​","style":"IPY_MODEL_d5a5766fd81945a6a5e852053928feb3","value":" 350/350 [00:00&lt;00:00, 5.66kB/s]"}},"23c9f7d665c348318aab143ab863c8f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbb679ba053044369e0d2ddbc6c0867b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24f0c57b5f2944689d029d90237f8a3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21d3e57759574bf6b63a867bc6278392":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c55df13de0ca47a5a9a7e9e8ca15805b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a400940465b8464aaf2bbdd6a226dc59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5a5766fd81945a6a5e852053928feb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4845d9186a7467493c8b75d67eccf67":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_261e1f781362429ba269efa7241d58cd","IPY_MODEL_68b0a1a1fd774fa7af55e83fbe1fd44b","IPY_MODEL_2dfaf43f144d49f8823e7af85dab476b"],"layout":"IPY_MODEL_6ff190f4acb545c0bc1c14e39e92ca3e"}},"261e1f781362429ba269efa7241d58cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9be46bdfd8c9470da0b04cefcc1e91cb","placeholder":"​","style":"IPY_MODEL_a36696a63760461e8b3af98f513a4022","value":"vocab.txt: 100%"}},"68b0a1a1fd774fa7af55e83fbe1fd44b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f29907ce612441889c4c5793673a6c1","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e301e3d87f64adb8229e3fa0c57916a","value":231508}},"2dfaf43f144d49f8823e7af85dab476b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8c4865862c24a14a84f32959052e8b0","placeholder":"​","style":"IPY_MODEL_64111a4f28a0473b87b2bd6446245fc0","value":" 232k/232k [00:00&lt;00:00, 1.69MB/s]"}},"6ff190f4acb545c0bc1c14e39e92ca3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9be46bdfd8c9470da0b04cefcc1e91cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a36696a63760461e8b3af98f513a4022":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f29907ce612441889c4c5793673a6c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e301e3d87f64adb8229e3fa0c57916a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8c4865862c24a14a84f32959052e8b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64111a4f28a0473b87b2bd6446245fc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e923a5e5fa7f492bb6bd3b6af00e6587":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a97b4c7e89d34ef985e9f7de92ff5da6","IPY_MODEL_2d7abefb4ca04111a95f57cd8e89ab96","IPY_MODEL_3ab119d4ab0b4fc8ba70cfd70c80788b"],"layout":"IPY_MODEL_af5eb9e096ed41d7ae2b0ccc086b9c1b"}},"a97b4c7e89d34ef985e9f7de92ff5da6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34480d9ec3164cf984822fb3ed8fa404","placeholder":"​","style":"IPY_MODEL_faf8d5b569a8493db0a17f998ec5bd00","value":"tokenizer.json: 100%"}},"2d7abefb4ca04111a95f57cd8e89ab96":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9cedc84729934fbdaa7d0137069a15fe","max":466247,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4501602c41724555b9fd019a060ddf89","value":466247}},"3ab119d4ab0b4fc8ba70cfd70c80788b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63427e83e38e4543bad11ae029b553f0","placeholder":"​","style":"IPY_MODEL_ccc7dd51b69047d184490673de65443f","value":" 466k/466k [00:00&lt;00:00, 4.51MB/s]"}},"af5eb9e096ed41d7ae2b0ccc086b9c1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34480d9ec3164cf984822fb3ed8fa404":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faf8d5b569a8493db0a17f998ec5bd00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9cedc84729934fbdaa7d0137069a15fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4501602c41724555b9fd019a060ddf89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63427e83e38e4543bad11ae029b553f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccc7dd51b69047d184490673de65443f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e148d8c0866438b8f42d45c2a0d9598":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d5c723d7f0b4ab7adac8574101ca097","IPY_MODEL_936f456008454583b1908976fd839060","IPY_MODEL_fb2cf48ae46841e1b95e30ec19d7d0ba"],"layout":"IPY_MODEL_56a567473b75462f98bf44c617622c05"}},"6d5c723d7f0b4ab7adac8574101ca097":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f4945e178cd4241aae5b88c24bba147","placeholder":"​","style":"IPY_MODEL_ea6e81b1b80441bd8fd52860efa2f667","value":"special_tokens_map.json: 100%"}},"936f456008454583b1908976fd839060":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_971bc147bea1447485a41fe60848622b","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc28132d35e04c80a5f180d108dcaa8d","value":112}},"fb2cf48ae46841e1b95e30ec19d7d0ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8eecb01f1b244a35982992e6756c5fc0","placeholder":"​","style":"IPY_MODEL_83edf745816e41f89c89bec2ba36f376","value":" 112/112 [00:00&lt;00:00, 2.72kB/s]"}},"56a567473b75462f98bf44c617622c05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f4945e178cd4241aae5b88c24bba147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea6e81b1b80441bd8fd52860efa2f667":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"971bc147bea1447485a41fe60848622b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc28132d35e04c80a5f180d108dcaa8d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8eecb01f1b244a35982992e6756c5fc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83edf745816e41f89c89bec2ba36f376":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0584db3d4f04dc2bfffac916352e8b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84710db6182f4703aab99f1eea20e617","IPY_MODEL_f92e488fd0d04e569ea98a9187917c1b","IPY_MODEL_a0a05d734f9b4eb1a25f7a89b57cf2ad"],"layout":"IPY_MODEL_4d2d43d42a354e468e6550478017b906"}},"84710db6182f4703aab99f1eea20e617":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a418b0a198ef4b99a7caa4209a1d2c35","placeholder":"​","style":"IPY_MODEL_f5cb58a3bcae43d68ddf0a0c1a33adc1","value":"config.json: 100%"}},"f92e488fd0d04e569ea98a9187917c1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f99526247264dca950ac3406b8fd6eb","max":612,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e5c34a491c34babbf2f0f0ed7cf3e06","value":612}},"a0a05d734f9b4eb1a25f7a89b57cf2ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fd85b87177f4bbca63f67ec76138b40","placeholder":"​","style":"IPY_MODEL_8eba2ca7215e4f918cd7e19ac57a8d3d","value":" 612/612 [00:00&lt;00:00, 6.65kB/s]"}},"4d2d43d42a354e468e6550478017b906":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a418b0a198ef4b99a7caa4209a1d2c35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5cb58a3bcae43d68ddf0a0c1a33adc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f99526247264dca950ac3406b8fd6eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e5c34a491c34babbf2f0f0ed7cf3e06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1fd85b87177f4bbca63f67ec76138b40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8eba2ca7215e4f918cd7e19ac57a8d3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17fe43ef24374257980be07638e2b7a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27eadaa37d0444cda7f62ebcb7cc8149","IPY_MODEL_1e35c3cd1b184da4b2a22ad8ed301fa3","IPY_MODEL_d25332b3eaa0490fa48d3f25f969cc2f"],"layout":"IPY_MODEL_8d9e0cb594f04d5abf66289166fdb521"}},"27eadaa37d0444cda7f62ebcb7cc8149":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c24912eee1f4454b67cd3e9aaf91256","placeholder":"​","style":"IPY_MODEL_df0696a1170243969c5dd9d54b067bfe","value":"model.safetensors: 100%"}},"1e35c3cd1b184da4b2a22ad8ed301fa3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1044f358c8c438181067bfd8edbe350","max":90868376,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f25874fd0bb4c07bff3814fbfc31b28","value":90868376}},"d25332b3eaa0490fa48d3f25f969cc2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc1b022db9634b0b98567089f8de1eb6","placeholder":"​","style":"IPY_MODEL_0646571ed687482691a148f4a2769c9d","value":" 90.9M/90.9M [00:01&lt;00:00, 91.2MB/s]"}},"8d9e0cb594f04d5abf66289166fdb521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c24912eee1f4454b67cd3e9aaf91256":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df0696a1170243969c5dd9d54b067bfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1044f358c8c438181067bfd8edbe350":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f25874fd0bb4c07bff3814fbfc31b28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc1b022db9634b0b98567089f8de1eb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0646571ed687482691a148f4a2769c9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}