{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /c1/yumin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import tool_pool as tp\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Î≥∏ ÎÖ∏Ìä∏Î∂ÅÏùÑ Ïã§ÌñâÌïòÍ∏∞ ÏúÑÌï¥ÏÑ† Semantic scholar keyÏôÄ Openai api keyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool List\n",
    "loadpaper : HTML Í∏∞Î∞òÏúºÎ°ú Section listÎ•º Î∞õÍ≥†, ToolÏùÑ Ìïú Î≤à Îçî Ïã§ÌñâÌïòÎäî Î∞©ÏãùÏúºÎ°ú ÎèôÏûë  \n",
    "loadpaper_wo_figure : loadpaper ÏóêÏÑú figure Í∏∞Îä•Ïù¥ Ï†úÏô∏Îêú Tool  \n",
    "loadpaper_wo_figure_wo_section : loadpaperÏóêÏÑú figure, section Í∏∞Îä•Ïù¥ Ï†úÏô∏Îêú Tool  \n",
    "recommendpaper : Í¥ÄÎ†® ÎÖºÎ¨∏ÏùÑ Ï∞æÏïÑÏ£ºÎäî Tool. citation, reference mode Îëê Í∞ÄÏßÄÎ°ú ÎèôÏûë  \n",
    "code_matching : ÎÖºÎ¨∏ ÍπÉÌóàÎ∏åÏóêÏÑú Í¥ÄÎ†® ÏΩîÎìúÎ•º Ï∞æÏïÑÏ£ºÎäî Tool  \n",
    "  \n",
    "tool_pool.pyÏóêÏÑú ÏßÅÏ†ë toolÏùÑ custom Ìï† Ïàò ÏûàÏäµÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp.loadpaper, tp.recommendpaper, tp.code_matching, tp.loadpaper_wo_figure, tp.loadpaper_wo_figure_wo_section\n",
    "\n",
    "# default tools\n",
    "tools = [tp.loadpaper, tp.recommendpaper, tp.code_matching]\n",
    "\n",
    "# light version. Customize it as you wish!\n",
    "# tools = [tp.loadpaper_wo_figure_wo_section, tp.recommendpaper]\n",
    "\n",
    "# Recommend tool combination.\n",
    "# tools = [tp.loadpaper_wo_figure, tp.recommendpaper]\n",
    "\n",
    "# load Agent prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base model ÏÑ†ÌÉù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAssistant:\n",
    "    def __init__(self, llm, verbose):\n",
    "        self.memory = None\n",
    "        self.llm = llm\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.agent_executor = self.build_agent_executor()\n",
    "        self.agent_with_chat_history = self.bulid_agent_with_chat_history()\n",
    "\n",
    "\n",
    "    def build_agent_executor(self):\n",
    "        # Construct the OpenAI Tools agent\n",
    "        agent = create_openai_tools_agent(self.llm, tools, prompt)\n",
    "        # Create an agent executor by passing in the agent and tools\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=self.verbose)\n",
    "        return agent_executor\n",
    "    \n",
    "    def bulid_agent_with_chat_history(self):\n",
    "        memory = ChatMessageHistory(session_id=\"test-session\")\n",
    "\n",
    "        agent_with_chat_history = RunnableWithMessageHistory(\n",
    "            self.agent_executor,\n",
    "            # This is needed because in most real world scenarios, a session id is needed\n",
    "            # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "            lambda session_id: memory,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "        )\n",
    "        return agent_with_chat_history\n",
    "\n",
    "    def reset_memory(self, session_id='temp'):\n",
    "        self.agent_with_chat_history = self.bulid_agent_with_chat_history()\n",
    "\n",
    "    def send_message(self, message, session_id='temp'):\n",
    "        output = self.agent_with_chat_history.invoke(\n",
    "            {\"input\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "        display(Markdown(output['output']))\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.send_message(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Ïã§Ìñâ Í≥ºÏ†ïÏùÑ ÏÉùÎûµÌïòÎ†§Î©¥ verbose = FalseÎ°ú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî\n",
    "assistant = PaperAssistant(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "Attention Is All You Need\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Background\n",
      "  3 Model Architecture\n",
      "    3.1 Encoder and Decoder Stacks\n",
      "        Encoder:\n",
      "        Decoder:\n",
      "    3.2 Attention\n",
      "      3.2.1 Scaled Dot-Product Attention\n",
      "      3.2.2 Multi-Head Attention\n",
      "      3.2.3 Applications of Attention in our Model\n",
      "    3.3 Position-wise Feed-Forward Networks\n",
      "    3.4 Embeddings and Softmax\n",
      "    3.5 Positional Encoding\n",
      "  4 Why Self-Attention\n",
      "  5 Training\n",
      "    5.1 Training Data and Batching\n",
      "    5.2 Hardware and Schedule\n",
      "    5.3 Optimizer\n",
      "    5.4 Regularization\n",
      "        Residual Dropout\n",
      "        Label Smoothing\n",
      "  6 Results\n",
      "    6.1 Machine Translation\n",
      "    6.2 Model Variations\n",
      "    6.3 English Constituency Parsing\n",
      "  7 Conclusion\n",
      "        Acknowledgements\n",
      "  References\n",
      "  Attention Visualizations\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762', 'sections': ['3.5 Positional Encoding']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscriptùëëmodeld_{\\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "P‚ÄãE(p‚Äão‚Äãs,2‚Äãi)=s‚Äãi‚Äãn‚Äã(p‚Äão‚Äãs/100002‚Äãi/dmodel)ùëÉsubscriptùê∏ùëùùëúùë†2ùëñùë†ùëñùëõùëùùëúùë†superscript100002ùëñsubscriptùëëmodel\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "P‚ÄãE(p‚Äão‚Äãs,2‚Äãi+1)=c‚Äão‚Äãs‚Äã(p‚Äão‚Äãs/100002‚Äãi/dmodel)ùëÉsubscriptùê∏ùëùùëúùë†2ùëñ1ùëêùëúùë†ùëùùëúùë†superscript100002ùëñsubscriptùëëmodel\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "where p‚Äão‚Äãsùëùùëúùë†pos is the position and iùëñi is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2‚ÄãœÄ2ùúã2\\pi to 10000‚ãÖ2‚ÄãœÄ‚ãÖ100002ùúã10000\\cdot 2\\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset kùëòk, P‚ÄãEp‚Äão‚Äãs+kùëÉsubscriptùê∏ùëùùëúùë†ùëòPE_{pos+k} can be represented as a linear function of P‚ÄãEp‚Äão‚ÄãsùëÉsubscriptùê∏ùëùùëúùë†PE_{pos}.\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table¬†3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mIn the paper \"Attention is All You Need,\" the authors describe the concept of **positional encoding** as follows:\n",
      "\n",
      "### Positional Encoding\n",
      "\n",
      "Since the model does not use recurrence or convolution, it needs a way to incorporate the order of the sequence. This is achieved by adding \"positional encodings\" to the input embeddings at the bottom of the encoder and decoder stacks. The positional encodings have the same dimension \\( d_{\\text{model}} \\) as the embeddings, allowing them to be summed together.\n",
      "\n",
      "The authors use sine and cosine functions of different frequencies for the positional encodings:\n",
      "\n",
      "\\[ \\text{PE}_{(\\text{pos}, 2i)} = \\sin\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
      "\n",
      "\\[ \\text{PE}_{(\\text{pos}, 2i+1)} = \\cos\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
      "\n",
      "where:\n",
      "- \\(\\text{pos}\\) is the position,\n",
      "- \\(i\\) is the dimension.\n",
      "\n",
      "Each dimension of the positional encoding corresponds to a sinusoid, with wavelengths forming a geometric progression from \\(2\\pi\\) to \\(10000 \\cdot 2\\pi\\). This design allows the model to learn to attend by relative positions easily. For any fixed offset \\(k\\), \\(\\text{PE}_{\\text{pos}+k}\\) can be represented as a linear function of \\(\\text{PE}_{\\text{pos}}\\).\n",
      "\n",
      "The authors also experimented with learned positional embeddings and found that both learned and fixed (sinusoidal) versions produced nearly identical results. They chose the sinusoidal version because it may allow the model to generalize to sequence lengths longer than those encountered during training.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In the paper \"Attention is All You Need,\" the authors describe the concept of **positional encoding** as follows:\n",
       "\n",
       "### Positional Encoding\n",
       "\n",
       "Since the model does not use recurrence or convolution, it needs a way to incorporate the order of the sequence. This is achieved by adding \"positional encodings\" to the input embeddings at the bottom of the encoder and decoder stacks. The positional encodings have the same dimension \\( d_{\\text{model}} \\) as the embeddings, allowing them to be summed together.\n",
       "\n",
       "The authors use sine and cosine functions of different frequencies for the positional encodings:\n",
       "\n",
       "\\[ \\text{PE}_{(\\text{pos}, 2i)} = \\sin\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
       "\n",
       "\\[ \\text{PE}_{(\\text{pos}, 2i+1)} = \\cos\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
       "\n",
       "where:\n",
       "- \\(\\text{pos}\\) is the position,\n",
       "- \\(i\\) is the dimension.\n",
       "\n",
       "Each dimension of the positional encoding corresponds to a sinusoid, with wavelengths forming a geometric progression from \\(2\\pi\\) to \\(10000 \\cdot 2\\pi\\). This design allows the model to learn to attend by relative positions easily. For any fixed offset \\(k\\), \\(\\text{PE}_{\\text{pos}+k}\\) can be represented as a linear function of \\(\\text{PE}_{\\text{pos}}\\).\n",
       "\n",
       "The authors also experimented with learned positional embeddings and found that both learned and fixed (sinusoidal) versions produced nearly identical results. They chose the sinusoidal version because it may allow the model to generalize to sequence lengths longer than those encountered during training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('\"Attention is all you need\" arxiv id 1706.03762 ÏóêÏÑú positional encoding Ïóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï§ò')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "Attention Is All You Need\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Background\n",
      "  3 Model Architecture\n",
      "    3.1 Encoder and Decoder Stacks\n",
      "        Encoder:\n",
      "        Decoder:\n",
      "    3.2 Attention\n",
      "      3.2.1 Scaled Dot-Product Attention\n",
      "      3.2.2 Multi-Head Attention\n",
      "      3.2.3 Applications of Attention in our Model\n",
      "    3.3 Position-wise Feed-Forward Networks\n",
      "    3.4 Embeddings and Softmax\n",
      "    3.5 Positional Encoding\n",
      "  4 Why Self-Attention\n",
      "  5 Training\n",
      "    5.1 Training Data and Batching\n",
      "    5.2 Hardware and Schedule\n",
      "    5.3 Optimizer\n",
      "    5.4 Regularization\n",
      "        Residual Dropout\n",
      "        Label Smoothing\n",
      "  6 Results\n",
      "    6.1 Machine Translation\n",
      "    6.2 Model Variations\n",
      "    6.3 English Constituency Parsing\n",
      "  7 Conclusion\n",
      "        Acknowledgements\n",
      "  References\n",
      "  Attention Visualizations\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762', 'sections': ['3 Model Architecture']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3 Model Architecture\n",
      "Figure 1: The Transformer - model architecture.\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,‚Ä¶,xn)subscriptùë•1‚Ä¶subscriptùë•ùëõ(x_{1},...,x_{n}) to a sequence of continuous representations ùê≥=(z1,‚Ä¶,zn)ùê≥subscriptùëß1‚Ä¶subscriptùëßùëõ\\mathbf{z}=(z_{1},...,z_{n}). Given ùê≥ùê≥\\mathbf{z}, the decoder then generates an output sequence (y1,‚Ä¶,ym)subscriptùë¶1‚Ä¶subscriptùë¶ùëö(y_{1},...,y_{m}) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure¬†1, respectively.\n",
      "\n",
      "3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder:\n",
      "\n",
      "The encoder is composed of a stack of N=6ùëÅ6N=6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm‚Äã(x+Sublayer‚Äã(x))LayerNormùë•Sublayerùë•\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x)), where Sublayer‚Äã(x)Sublayerùë•\\mathrm{Sublayer}(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512subscriptùëëmodel512d_{\\text{model}}=512.\n",
      "\n",
      "Decoder:\n",
      "\n",
      "The decoder is also composed of a stack of N=6ùëÅ6N=6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position iùëñi can depend only on the known outputs at positions less than iùëñi.\n",
      "\n",
      "3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure¬†2). The input consists of queries and keys of dimension dksubscriptùëëùëòd_{k}, and values of dimension dvsubscriptùëëùë£d_{v}. We compute the dot products of the query with all keys, divide each by dksubscriptùëëùëò\\sqrt{d_{k}}, and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix QùëÑQ. The keys and values are also packed together into matrices KùêæK and VùëâV. We compute the matrix of outputs as:\n",
      "\n",
      "Attention‚Äã(Q,K,V)=softmax‚Äã(Q‚ÄãKTdk)‚ÄãVAttentionùëÑùêæùëâsoftmaxùëÑsuperscriptùêæùëásubscriptùëëùëòùëâ\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V\n",
      "\n",
      "(1)\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk1subscriptùëëùëò\\frac{1}{\\sqrt{d_{k}}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of dksubscriptùëëùëòd_{k} the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dksubscriptùëëùëòd_{k} [3]. We suspect that for large values of dksubscriptùëëùëòd_{k}, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of qùëûq and kùëòk are independent random variables with mean 00 and variance 111. Then their dot product, q‚ãÖk=‚àëi=1dkqi‚Äãki‚ãÖùëûùëòsuperscriptsubscriptùëñ1subscriptùëëùëòsubscriptùëûùëñsubscriptùëòùëñq\\cdot k=\\sum_{i=1}^{d_{k}}q_{i}k_{i}, has mean 00 and variance dksubscriptùëëùëòd_{k}.. To counteract this effect, we scale the dot products by 1dk1subscriptùëëùëò\\frac{1}{\\sqrt{d_{k}}}.\n",
      "\n",
      "3.2.2 Multi-Head Attention\n",
      "\n",
      "Scaled Dot-Product Attention\n",
      "\n",
      "Multi-Head Attention\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "Instead of performing a single attention function with dmodelsubscriptùëëmodeld_{\\text{model}}-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h‚Ñéh times with different, learned linear projections to dksubscriptùëëùëòd_{k}, dksubscriptùëëùëòd_{k} and dvsubscriptùëëùë£d_{v} dimensions, respectively.\n",
      "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvsubscriptùëëùë£d_{v}-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure¬†2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "MultiHead‚Äã(Q,K,V)MultiHeadùëÑùêæùëâ\\displaystyle\\mathrm{MultiHead}(Q,K,V)\n",
      "=Concat‚Äã(head1,‚Ä¶,headh)‚ÄãWOabsentConcatsubscripthead1‚Ä¶subscriptheadhsuperscriptùëäùëÇ\\displaystyle=\\mathrm{Concat}(\\mathrm{head_{1}},...,\\mathrm{head_{h}})W^{O}\n",
      "\n",
      "where‚Äãheadiwheresubscriptheadi\\displaystyle\\text{where}~{}\\mathrm{head_{i}}\n",
      "=Attention‚Äã(Q‚ÄãWiQ,K‚ÄãWiK,V‚ÄãWiV)absentAttentionùëÑsubscriptsuperscriptùëäùëÑùëñùêæsubscriptsuperscriptùëäùêæùëñùëâsubscriptsuperscriptùëäùëâùëñ\\displaystyle=\\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})\n",
      "\n",
      "Where the projections are parameter matrices WiQ‚àà‚Ñùdmodel√ódksubscriptsuperscriptùëäùëÑùëñsuperscript‚ÑùsubscriptùëëmodelsubscriptùëëùëòW^{Q}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}, WiK‚àà‚Ñùdmodel√ódksubscriptsuperscriptùëäùêæùëñsuperscript‚ÑùsubscriptùëëmodelsubscriptùëëùëòW^{K}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}, WiV‚àà‚Ñùdmodel√ódvsubscriptsuperscriptùëäùëâùëñsuperscript‚Ñùsubscriptùëëmodelsubscriptùëëùë£W^{V}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{v}} and WO‚àà‚Ñùh‚Äãdv√ódmodelsuperscriptùëäùëÇsuperscript‚Ñù‚Ñésubscriptùëëùë£subscriptùëëmodelW^{O}\\in\\mathbb{R}^{hd_{v}\\times d_{\\text{model}}}.\n",
      "\n",
      "In this work we employ h=8‚Ñé8h=8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64subscriptùëëùëòsubscriptùëëùë£subscriptùëëmodel‚Ñé64d_{k}=d_{v}=d_{\\text{model}}/h=64.\n",
      "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "‚Ä¢\n",
      "\n",
      "In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "\n",
      "‚Ä¢\n",
      "\n",
      "The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "\n",
      "‚Ä¢\n",
      "\n",
      "Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to ‚àí‚àû-\\infty) all values in the input of the softmax which correspond to illegal connections. See Figure¬†2.\n",
      "\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "FFN‚Äã(x)=max‚Å°(0,x‚ÄãW1+b1)‚ÄãW2+b2FFNùë•0ùë•subscriptùëä1subscriptùëè1subscriptùëä2subscriptùëè2\\mathrm{FFN}(x)=\\max(0,xW_{1}+b_{1})W_{2}+b_{2}\n",
      "\n",
      "(2)\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel=512subscriptùëëmodel512d_{\\text{model}}=512, and the inner-layer has dimensionality df‚Äãf=2048subscriptùëëùëìùëì2048d_{ff}=2048.\n",
      "\n",
      "3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodelsubscriptùëëmodeld_{\\text{model}}. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodelsubscriptùëëmodel\\sqrt{d_{\\text{model}}}.\n",
      "\n",
      "3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscriptùëëmodeld_{\\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "P‚ÄãE(p‚Äão‚Äãs,2‚Äãi)=s‚Äãi‚Äãn‚Äã(p‚Äão‚Äãs/100002‚Äãi/dmodel)ùëÉsubscriptùê∏ùëùùëúùë†2ùëñùë†ùëñùëõùëùùëúùë†superscript100002ùëñsubscriptùëëmodel\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "P‚ÄãE(p‚Äão‚Äãs,2‚Äãi+1)=c‚Äão‚Äãs‚Äã(p‚Äão‚Äãs/100002‚Äãi/dmodel)ùëÉsubscriptùê∏ùëùùëúùë†2ùëñ1ùëêùëúùë†ùëùùëúùë†superscript100002ùëñsubscriptùëëmodel\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "where p‚Äão‚Äãsùëùùëúùë†pos is the position and iùëñi is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2‚ÄãœÄ2ùúã2\\pi to 10000‚ãÖ2‚ÄãœÄ‚ãÖ100002ùúã10000\\cdot 2\\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset kùëòk, P‚ÄãEp‚Äão‚Äãs+kùëÉsubscriptùê∏ùëùùëúùë†ùëòPE_{pos+k} can be represented as a linear function of P‚ÄãEp‚Äão‚ÄãsùëÉsubscriptùê∏ùëùùëúùë†PE_{pos}.\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table¬†3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThe \"Method\" section of the paper \"Attention is All You Need\" is detailed under the \"Model Architecture\" section. Here is a summary of each element:\n",
      "\n",
      "### 3 Model Architecture\n",
      "\n",
      "#### 3.1 Encoder and Decoder Stacks\n",
      "\n",
      "- **Encoder**: \n",
      "  - Composed of 6 identical layers.\n",
      "  - Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
      "  - Residual connections and layer normalization are applied around each sub-layer.\n",
      "  - All sub-layers and embedding layers produce outputs of dimension \\( d_{\\text{model}} = 512 \\).\n",
      "\n",
      "- **Decoder**: \n",
      "  - Also composed of 6 identical layers.\n",
      "  - Each layer has an additional third sub-layer for multi-head attention over the encoder's output.\n",
      "  - Similar to the encoder, residual connections and layer normalization are applied.\n",
      "  - Self-attention sub-layer is modified to prevent positions from attending to subsequent positions, ensuring auto-regressive property.\n",
      "\n",
      "#### 3.2 Attention\n",
      "\n",
      "- **Attention Function**: \n",
      "  - Maps a query and a set of key-value pairs to an output.\n",
      "  - Output is a weighted sum of the values, with weights computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "##### 3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "- Computes dot products of the query with all keys, divides by \\(\\sqrt{d_k}\\), and applies a softmax function to obtain weights on the values.\n",
      "- Efficiently implemented using matrix multiplication.\n",
      "\n",
      "##### 3.2.2 Multi-Head Attention\n",
      "\n",
      "- Projects queries, keys, and values multiple times with different learned linear projections.\n",
      "- Performs attention function in parallel on these projections.\n",
      "- Concatenates and projects the results to form the final values.\n",
      "- Allows the model to attend to information from different representation subspaces at different positions.\n",
      "- Uses 8 parallel attention layers (heads), each with \\( d_k = d_v = d_{\\text{model}} / h = 64 \\).\n",
      "\n",
      "##### 3.2.3 Applications of Attention in the Model\n",
      "\n",
      "- **Encoder-Decoder Attention**: Queries from the decoder, keys, and values from the encoder.\n",
      "- **Self-Attention in Encoder**: Keys, values, and queries from the same layer output.\n",
      "- **Self-Attention in Decoder**: Similar to encoder but prevents leftward information flow to maintain auto-regressive property.\n",
      "\n",
      "#### 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "- Each layer in the encoder and decoder contains a fully connected feed-forward network.\n",
      "- Applied to each position separately and identically.\n",
      "- Consists of two linear transformations with a ReLU activation in between.\n",
      "- Input and output dimensionality is \\( d_{\\text{model}} = 512 \\), and inner-layer dimensionality is \\( d_{ff} = 2048 \\).\n",
      "\n",
      "#### 3.4 Embeddings and Softmax\n",
      "\n",
      "- Uses learned embeddings to convert input and output tokens to vectors of dimension \\( d_{\\text{model}} \\).\n",
      "- Uses a learned linear transformation and softmax function to convert decoder output to predicted next-token probabilities.\n",
      "- Shares the same weight matrix between embedding layers and pre-softmax linear transformation, scaled by \\( \\sqrt{d_{\\text{model}}} \\).\n",
      "\n",
      "#### 3.5 Positional Encoding\n",
      "\n",
      "- Adds positional encodings to input embeddings to incorporate sequence order.\n",
      "- Positional encodings have the same dimension \\( d_{\\text{model}} \\) as embeddings.\n",
      "- Uses sine and cosine functions of different frequencies for encoding.\n",
      "- Allows the model to learn to attend by relative positions.\n",
      "- Experimented with learned positional embeddings but found sinusoidal version to be equally effective and potentially better for extrapolation to longer sequences.\n",
      "\n",
      "This summary encapsulates the key components and mechanisms of the Transformer model as described in the \"Attention is All You Need\" paper.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The \"Method\" section of the paper \"Attention is All You Need\" is detailed under the \"Model Architecture\" section. Here is a summary of each element:\n",
       "\n",
       "### 3 Model Architecture\n",
       "\n",
       "#### 3.1 Encoder and Decoder Stacks\n",
       "\n",
       "- **Encoder**: \n",
       "  - Composed of 6 identical layers.\n",
       "  - Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
       "  - Residual connections and layer normalization are applied around each sub-layer.\n",
       "  - All sub-layers and embedding layers produce outputs of dimension \\( d_{\\text{model}} = 512 \\).\n",
       "\n",
       "- **Decoder**: \n",
       "  - Also composed of 6 identical layers.\n",
       "  - Each layer has an additional third sub-layer for multi-head attention over the encoder's output.\n",
       "  - Similar to the encoder, residual connections and layer normalization are applied.\n",
       "  - Self-attention sub-layer is modified to prevent positions from attending to subsequent positions, ensuring auto-regressive property.\n",
       "\n",
       "#### 3.2 Attention\n",
       "\n",
       "- **Attention Function**: \n",
       "  - Maps a query and a set of key-value pairs to an output.\n",
       "  - Output is a weighted sum of the values, with weights computed by a compatibility function of the query with the corresponding key.\n",
       "\n",
       "##### 3.2.1 Scaled Dot-Product Attention\n",
       "\n",
       "- Computes dot products of the query with all keys, divides by \\(\\sqrt{d_k}\\), and applies a softmax function to obtain weights on the values.\n",
       "- Efficiently implemented using matrix multiplication.\n",
       "\n",
       "##### 3.2.2 Multi-Head Attention\n",
       "\n",
       "- Projects queries, keys, and values multiple times with different learned linear projections.\n",
       "- Performs attention function in parallel on these projections.\n",
       "- Concatenates and projects the results to form the final values.\n",
       "- Allows the model to attend to information from different representation subspaces at different positions.\n",
       "- Uses 8 parallel attention layers (heads), each with \\( d_k = d_v = d_{\\text{model}} / h = 64 \\).\n",
       "\n",
       "##### 3.2.3 Applications of Attention in the Model\n",
       "\n",
       "- **Encoder-Decoder Attention**: Queries from the decoder, keys, and values from the encoder.\n",
       "- **Self-Attention in Encoder**: Keys, values, and queries from the same layer output.\n",
       "- **Self-Attention in Decoder**: Similar to encoder but prevents leftward information flow to maintain auto-regressive property.\n",
       "\n",
       "#### 3.3 Position-wise Feed-Forward Networks\n",
       "\n",
       "- Each layer in the encoder and decoder contains a fully connected feed-forward network.\n",
       "- Applied to each position separately and identically.\n",
       "- Consists of two linear transformations with a ReLU activation in between.\n",
       "- Input and output dimensionality is \\( d_{\\text{model}} = 512 \\), and inner-layer dimensionality is \\( d_{ff} = 2048 \\).\n",
       "\n",
       "#### 3.4 Embeddings and Softmax\n",
       "\n",
       "- Uses learned embeddings to convert input and output tokens to vectors of dimension \\( d_{\\text{model}} \\).\n",
       "- Uses a learned linear transformation and softmax function to convert decoder output to predicted next-token probabilities.\n",
       "- Shares the same weight matrix between embedding layers and pre-softmax linear transformation, scaled by \\( \\sqrt{d_{\\text{model}}} \\).\n",
       "\n",
       "#### 3.5 Positional Encoding\n",
       "\n",
       "- Adds positional encodings to input embeddings to incorporate sequence order.\n",
       "- Positional encodings have the same dimension \\( d_{\\text{model}} \\) as embeddings.\n",
       "- Uses sine and cosine functions of different frequencies for encoding.\n",
       "- Allows the model to learn to attend by relative positions.\n",
       "- Experimented with learned positional embeddings but found sinusoidal version to be equally effective and potentially better for extrapolation to longer sequences.\n",
       "\n",
       "This summary encapsulates the key components and mechanisms of the Transformer model as described in the \"Attention is All You Need\" paper."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('ÎÖºÎ¨∏ MethodÏùò Í∞Å ÏöîÏÜåÎ•º ÏöîÏïΩÌï¥Ï§ò')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset memory Î°ú Î©îÎ™®Î¶¨Î•º Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.\n",
    "assistant.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mÎÖºÎ¨∏Ïùò Ï†úÎ™©ÏùÑ ÏïåÎ†§Ï£ºÏãúÎ©¥, Ìï¥Îãπ ÎÖºÎ¨∏Ïùò Method ÏÑπÏÖòÏùÑ Î∂àÎü¨ÏôÄ ÏöîÏïΩÌï¥ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ÎÖºÎ¨∏Ïùò Ï†úÎ™©ÏùÑ ÏïåÎ†§Ï£ºÏãúÎ©¥, Ìï¥Îãπ ÎÖºÎ¨∏Ïùò Method ÏÑπÏÖòÏùÑ Î∂àÎü¨ÏôÄ ÏöîÏïΩÌï¥ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('ÎÖºÎ¨∏ MethodÏùò Í∞Å ÏöîÏÜåÎ•º ÏöîÏïΩÌï¥Ï§ò')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION', 'rec_type': 'citation'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization', 'abstract': \"Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\", 'influentialCitationCount': 1, 'publicationDate': '2024-02-27'}, {'title': 'RoT: Enhancing Large Language Models with Reflection on Search Trees', 'abstract': 'Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.', 'influentialCitationCount': 1, 'publicationDate': '2024-04-08'}, {'title': 'Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments', 'abstract': 'Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.', 'influentialCitationCount': 0, 'publicationDate': '2024-06-17'}, {'title': 'Dual-Phase Accelerated Prompt Optimization', 'abstract': 'Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.', 'influentialCitationCount': 0, 'publicationDate': '2024-06-19'}, {'title': 'Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows', 'abstract': \"We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback. Website: https://microsoft.github.io/Trace\", 'influentialCitationCount': 0, 'publicationDate': '2024-06-23'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\n",
      "\n",
      "1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\n",
      "   - **Abstract**: Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\n",
      "   - **Publication Date**: 2024-02-27\n",
      "   - **Influential Citation Count**: 1\n",
      "\n",
      "2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\n",
      "   - **Abstract**: Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.\n",
      "   - **Publication Date**: 2024-04-08\n",
      "   - **Influential Citation Count**: 1\n",
      "\n",
      "3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\n",
      "   - **Abstract**: Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.\n",
      "   - **Publication Date**: 2024-06-17\n",
      "   - **Influential Citation Count**: 0\n",
      "\n",
      "4. **Dual-Phase Accelerated Prompt Optimization**\n",
      "   - **Abstract**: Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.\n",
      "   - **Publication Date**: 2024-06-19\n",
      "   - **Influential Citation Count**: 0\n",
      "\n",
      "5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\n",
      "   - **Abstract**: We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback.\n",
      "   - **Publication Date**: 2024-06-23\n",
      "   - **Influential Citation Count**: 0\n",
      "\n",
      "These papers build upon the concepts introduced in \"PROMPTAGENT\" and explore various aspects of prompt optimization, reflection, and learning in large language models.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\n",
       "\n",
       "1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\n",
       "   - **Abstract**: Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\n",
       "   - **Publication Date**: 2024-02-27\n",
       "   - **Influential Citation Count**: 1\n",
       "\n",
       "2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\n",
       "   - **Abstract**: Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.\n",
       "   - **Publication Date**: 2024-04-08\n",
       "   - **Influential Citation Count**: 1\n",
       "\n",
       "3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\n",
       "   - **Abstract**: Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.\n",
       "   - **Publication Date**: 2024-06-17\n",
       "   - **Influential Citation Count**: 0\n",
       "\n",
       "4. **Dual-Phase Accelerated Prompt Optimization**\n",
       "   - **Abstract**: Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.\n",
       "   - **Publication Date**: 2024-06-19\n",
       "   - **Influential Citation Count**: 0\n",
       "\n",
       "5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\n",
       "   - **Abstract**: We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback.\n",
       "   - **Publication Date**: 2024-06-23\n",
       "   - **Influential Citation Count**: 0\n",
       "\n",
       "These papers build upon the concepts introduced in \"PROMPTAGENT\" and explore various aspects of prompt optimization, reflection, and learning in large language models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# semantic scholar error Î∞úÏÉù Ïãú Îã§Ïãú Ìïú Î≤à Ïã§ÌñâÌïòÎ©¥ Îê©ÎãàÎã§.\n",
    "assistant('PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION ÌõÑÏÜç ÎÖºÎ¨∏ Ï∂îÏ≤úÌï¥Ï§ò') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
