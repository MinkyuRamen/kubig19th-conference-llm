{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /c1/yumin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import tool_pool as tp\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tool\n",
    "tools = [tp.loadpaper, tp.recommendpaper]\n",
    "# load Agent prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM that will drive the agent\n",
    "# Only certain models support this\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=1)\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory in Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMessageHistory(session_id=\"test-session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAssistant:\n",
    "    def __init__(self, agent_excuter):\n",
    "        self.agent_with_chat_history = self.bulid_agent_with_chat_history()\n",
    "        self.memory = None\n",
    "        self.agent_excuter = agent_excuter\n",
    "\n",
    "    def bulid_agent_with_chat_history(self):\n",
    "        memory = ChatMessageHistory(session_id=\"test-session\")\n",
    "\n",
    "        agent_with_chat_history = RunnableWithMessageHistory(\n",
    "            self.agent_excuter,\n",
    "            # This is needed because in most real world scenarios, a session id is needed\n",
    "            # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "            lambda session_id: memory,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "        )\n",
    "        return agent_with_chat_history\n",
    "\n",
    "    def reset_memory(self, session_id='temp'):\n",
    "        self.agent_with_chat_history = self.bulid_agent_with_chat_history()\n",
    "\n",
    "    def send_message(self, message, session_id='temp'):\n",
    "        output = self.agent_with_chat_history.invoke(\n",
    "            {\"input\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "        display(Markdown(output['output']))\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.send_message(message)\n",
    "\n",
    "# 인스턴스 생성 및 사용 예시\n",
    "assistant = PaperAssistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION', 'rec_type': 'citation', 'rec_num': 5}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization', 'abstract': \"Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\", 'influentialCitationCount': 1, 'publicationDate': '2024-02-27'}, {'title': 'RoT: Enhancing Large Language Models with Reflection on Search Trees', 'abstract': 'Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.', 'influentialCitationCount': 1, 'publicationDate': '2024-04-08'}, {'title': 'Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments', 'abstract': 'Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.', 'influentialCitationCount': 0, 'publicationDate': '2024-06-17'}, {'title': 'Dual-Phase Accelerated Prompt Optimization', 'abstract': 'Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.', 'influentialCitationCount': 0, 'publicationDate': '2024-06-19'}, {'title': 'Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows', 'abstract': \"We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback. Website: https://microsoft.github.io/Trace\", 'influentialCitationCount': 0, 'publicationDate': '2024-06-23'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\n",
      "\n",
      "1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\n",
      "   - **Abstract:** This paper proposes `Agent-Pro`, an LLM-based agent designed with policy-level reflection and optimization for better handling complex dynamic scenarios, illustrating its performance in games like Blackjack and Texas Hold'em.\n",
      "   - **Publication Date:** 2024-02-27\n",
      "   - **Influential Citation Count:** 1\n",
      "\n",
      "2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\n",
      "   - **Abstract:** The paper introduces the `RoT` framework to improve LLM performance by summarizing guidelines from previous tree search experiences to avoid repeated mistakes, enhancing tree-search-based prompting methods.\n",
      "   - **Publication Date:** 2024-04-08\n",
      "   - **Influential Citation Count:** 1\n",
      "\n",
      "3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\n",
      "   - **Abstract:** The study proposes `ZEPO`, a zero-shot evaluation-oriented prompt optimization framework aimed at producing fairer preference decisions in LLM evaluators to improve alignment with human judgments.\n",
      "   - **Publication Date:** 2024-06-17\n",
      "   - **Influential Citation Count:** 0\n",
      "\n",
      "4. **Dual-Phase Accelerated Prompt Optimization**\n",
      "   - **Abstract:** This paper presents a dual-phase approach for accelerating the prompt optimization process, leveraging high-quality initial prompts and iterative sentence-level optimization, demonstrating effectiveness across multiple datasets.\n",
      "   - **Publication Date:** 2024-06-19\n",
      "   - **Influential Citation Count:** 0\n",
      "\n",
      "5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\n",
      "   - **Abstract:** Proposing `Trace`, an optimization framework modeled after back-propagation for computational workflows, this work includes `OptoPrime`, an LLM-based optimizer capable of solving various optimization problems, from prompt optimization to code debugging.\n",
      "   - **Publication Date:** 2024-06-23\n",
      "   - **Influential Citation Count:** 0\n",
      "\n",
      "These papers build on the concepts explored in the original \"PROMPTAGENT\" paper, advancing the field of prompt optimization and LLM capabilities.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\n",
       "\n",
       "1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\n",
       "   - **Abstract:** This paper proposes `Agent-Pro`, an LLM-based agent designed with policy-level reflection and optimization for better handling complex dynamic scenarios, illustrating its performance in games like Blackjack and Texas Hold'em.\n",
       "   - **Publication Date:** 2024-02-27\n",
       "   - **Influential Citation Count:** 1\n",
       "\n",
       "2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\n",
       "   - **Abstract:** The paper introduces the `RoT` framework to improve LLM performance by summarizing guidelines from previous tree search experiences to avoid repeated mistakes, enhancing tree-search-based prompting methods.\n",
       "   - **Publication Date:** 2024-04-08\n",
       "   - **Influential Citation Count:** 1\n",
       "\n",
       "3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\n",
       "   - **Abstract:** The study proposes `ZEPO`, a zero-shot evaluation-oriented prompt optimization framework aimed at producing fairer preference decisions in LLM evaluators to improve alignment with human judgments.\n",
       "   - **Publication Date:** 2024-06-17\n",
       "   - **Influential Citation Count:** 0\n",
       "\n",
       "4. **Dual-Phase Accelerated Prompt Optimization**\n",
       "   - **Abstract:** This paper presents a dual-phase approach for accelerating the prompt optimization process, leveraging high-quality initial prompts and iterative sentence-level optimization, demonstrating effectiveness across multiple datasets.\n",
       "   - **Publication Date:** 2024-06-19\n",
       "   - **Influential Citation Count:** 0\n",
       "\n",
       "5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\n",
       "   - **Abstract:** Proposing `Trace`, an optimization framework modeled after back-propagation for computational workflows, this work includes `OptoPrime`, an LLM-based optimizer capable of solving various optimization problems, from prompt optimization to code debugging.\n",
       "   - **Publication Date:** 2024-06-23\n",
       "   - **Influential Citation Count:** 0\n",
       "\n",
       "These papers build on the concepts explored in the original \"PROMPTAGENT\" paper, advancing the field of prompt optimization and LLM capabilities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION 후속 논문 추천해줘',\n",
       " 'chat_history': [],\n",
       " 'output': 'Here are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\\n\\n1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\\n   - **Abstract:** This paper proposes `Agent-Pro`, an LLM-based agent designed with policy-level reflection and optimization for better handling complex dynamic scenarios, illustrating its performance in games like Blackjack and Texas Hold\\'em.\\n   - **Publication Date:** 2024-02-27\\n   - **Influential Citation Count:** 1\\n\\n2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\\n   - **Abstract:** The paper introduces the `RoT` framework to improve LLM performance by summarizing guidelines from previous tree search experiences to avoid repeated mistakes, enhancing tree-search-based prompting methods.\\n   - **Publication Date:** 2024-04-08\\n   - **Influential Citation Count:** 1\\n\\n3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\\n   - **Abstract:** The study proposes `ZEPO`, a zero-shot evaluation-oriented prompt optimization framework aimed at producing fairer preference decisions in LLM evaluators to improve alignment with human judgments.\\n   - **Publication Date:** 2024-06-17\\n   - **Influential Citation Count:** 0\\n\\n4. **Dual-Phase Accelerated Prompt Optimization**\\n   - **Abstract:** This paper presents a dual-phase approach for accelerating the prompt optimization process, leveraging high-quality initial prompts and iterative sentence-level optimization, demonstrating effectiveness across multiple datasets.\\n   - **Publication Date:** 2024-06-19\\n   - **Influential Citation Count:** 0\\n\\n5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\\n   - **Abstract:** Proposing `Trace`, an optimization framework modeled after back-propagation for computational workflows, this work includes `OptoPrime`, an LLM-based optimizer capable of solving various optimization problems, from prompt optimization to code debugging.\\n   - **Publication Date:** 2024-06-23\\n   - **Influential Citation Count:** 0\\n\\nThese papers build on the concepts explored in the original \"PROMPTAGENT\" paper, advancing the field of prompt optimization and LLM capabilities.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant('PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION 후속 논문 추천해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Problem Definition\n",
      "  3 Methods\n",
      "    3.1 Belief-aware Decision-Making Process\n",
      "    3.2 Policy-Level Reflection\n",
      "    3.3 DFS-based Policy Evolution\n",
      "  4 Game: Blackjack\n",
      "    4.1 Results\n",
      "    4.2 Analyisis\n",
      "  5 Game: Limit Texas Hold’em\n",
      "    5.1 Results\n",
      "    5.2 Analysis on Learning Process\n",
      "    5.3 Analysis on Policy Evolution\n",
      "  6 Conclusion\n",
      "  Limitations\n",
      "  References\n",
      "  Appendix A Experiment Details\n",
      "    A.1 LLMs\n",
      "    A.2 Baselines\n",
      "    A.3 Setups For Blackjack\n",
      "    A.4 Detailed Setup For Texas Hold’em\n",
      "  Appendix B Introduction of Two Games\n",
      "    B.1 Blackjack\n",
      "    B.2 Limit Texas Hold’em\n",
      "    B.3 The Challenging of two Games\n",
      "  Appendix C Complementary Experiments\n",
      "    C.1 Whether Few-shot Learning Can Handle Such Complex Interaction\n",
      "    C.2 Policy-Level Reflection without Belief\n",
      "    C.3 Detailed Analysis Experiments\n",
      "  Appendix D Related Works\n",
      "    D.1 LLM-based Application\n",
      "    D.2 LLMs For Interactive Scenarios\n",
      "    D.3 Improving the Quality of LLM Responses\n",
      "    D.4 Automatic Prompt Optimization\n",
      "  Appendix E Detailed Prompts\n",
      "    E.1 Baseline’s Prompts For Blackjack\n",
      "    E.2 Agent-Pro’s Prompt For Blackjack\n",
      "    E.3 Baseline’s Prompts For Limited Texas Hold’em\n",
      "    E.4 Agent-Pro’s Prompt For Limited Texas Hold’em\n",
      "  Appendix F Case Study\n",
      "    F.1 Cases For Blackjack\n",
      "    F.2 Cases For Texas Hold’em\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization', 'sections': ['Abstract']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAbstractLarge Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, ”fine-tuning” its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold’em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications 111Code is in https://github.com/zwq2018/Agent-Pro.\u001b[0m\u001b[32;1m\u001b[1;3m### Summary of the Abstract\n",
      "\n",
      "Large Language Models (LLMs) demonstrate strong problem-solving capabilities, but most LLM-based agents are designed for specific tasks using sophisticated prompt engineering. These task-specific agents require manually crafted prompts, rendering them less effective in complex, dynamic scenarios such as large interactive games. To address this limitation, the paper introduces **Agent-Pro**, an LLM-based agent enhanced with Policy-level Reflection and Optimization. Agent-Pro learns and improves from interactive experiences by dynamically generating and reflecting on beliefs to evolve its policies. Instead of action-level reflection, it iteratively refines past trajectories and irrational beliefs for better policy outcomes, utilizing depth-first search for continual policy optimization. Evaluations in Blackjack and Texas Hold’em show that Agent-Pro outperforms both vanilla LLMs and specialized models, highlighting its ability to adapt and improve in complex, dynamic environments. The approach is promising for various LLM-based applications. \n",
      "[Code Repository](https://github.com/zwq2018/Agent-Pro)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Summary of the Abstract\n",
       "\n",
       "Large Language Models (LLMs) demonstrate strong problem-solving capabilities, but most LLM-based agents are designed for specific tasks using sophisticated prompt engineering. These task-specific agents require manually crafted prompts, rendering them less effective in complex, dynamic scenarios such as large interactive games. To address this limitation, the paper introduces **Agent-Pro**, an LLM-based agent enhanced with Policy-level Reflection and Optimization. Agent-Pro learns and improves from interactive experiences by dynamically generating and reflecting on beliefs to evolve its policies. Instead of action-level reflection, it iteratively refines past trajectories and irrational beliefs for better policy outcomes, utilizing depth-first search for continual policy optimization. Evaluations in Blackjack and Texas Hold’em show that Agent-Pro outperforms both vanilla LLMs and specialized models, highlighting its ability to adapt and improve in complex, dynamic environments. The approach is promising for various LLM-based applications. \n",
       "[Code Repository](https://github.com/zwq2018/Agent-Pro)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문 초록 요약해',\n",
       " 'chat_history': [HumanMessage(content='PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION 후속 논문 추천해줘'),\n",
       "  AIMessage(content='Here are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\\n\\n1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\\n   - **Abstract:** This paper proposes `Agent-Pro`, an LLM-based agent designed with policy-level reflection and optimization for better handling complex dynamic scenarios, illustrating its performance in games like Blackjack and Texas Hold\\'em.\\n   - **Publication Date:** 2024-02-27\\n   - **Influential Citation Count:** 1\\n\\n2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\\n   - **Abstract:** The paper introduces the `RoT` framework to improve LLM performance by summarizing guidelines from previous tree search experiences to avoid repeated mistakes, enhancing tree-search-based prompting methods.\\n   - **Publication Date:** 2024-04-08\\n   - **Influential Citation Count:** 1\\n\\n3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\\n   - **Abstract:** The study proposes `ZEPO`, a zero-shot evaluation-oriented prompt optimization framework aimed at producing fairer preference decisions in LLM evaluators to improve alignment with human judgments.\\n   - **Publication Date:** 2024-06-17\\n   - **Influential Citation Count:** 0\\n\\n4. **Dual-Phase Accelerated Prompt Optimization**\\n   - **Abstract:** This paper presents a dual-phase approach for accelerating the prompt optimization process, leveraging high-quality initial prompts and iterative sentence-level optimization, demonstrating effectiveness across multiple datasets.\\n   - **Publication Date:** 2024-06-19\\n   - **Influential Citation Count:** 0\\n\\n5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\\n   - **Abstract:** Proposing `Trace`, an optimization framework modeled after back-propagation for computational workflows, this work includes `OptoPrime`, an LLM-based optimizer capable of solving various optimization problems, from prompt optimization to code debugging.\\n   - **Publication Date:** 2024-06-23\\n   - **Influential Citation Count:** 0\\n\\nThese papers build on the concepts explored in the original \"PROMPTAGENT\" paper, advancing the field of prompt optimization and LLM capabilities.')],\n",
       " 'output': '### Summary of the Abstract\\n\\nLarge Language Models (LLMs) demonstrate strong problem-solving capabilities, but most LLM-based agents are designed for specific tasks using sophisticated prompt engineering. These task-specific agents require manually crafted prompts, rendering them less effective in complex, dynamic scenarios such as large interactive games. To address this limitation, the paper introduces **Agent-Pro**, an LLM-based agent enhanced with Policy-level Reflection and Optimization. Agent-Pro learns and improves from interactive experiences by dynamically generating and reflecting on beliefs to evolve its policies. Instead of action-level reflection, it iteratively refines past trajectories and irrational beliefs for better policy outcomes, utilizing depth-first search for continual policy optimization. Evaluations in Blackjack and Texas Hold’em show that Agent-Pro outperforms both vanilla LLMs and specialized models, highlighting its ability to adapt and improve in complex, dynamic environments. The approach is promising for various LLM-based applications. \\n[Code Repository](https://github.com/zwq2018/Agent-Pro)'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant('\"Learning to Evolve via Policy-Level Reflection and Optimization\" 논문 초록 요약해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m논문 제목을 알려주시면 'Method' 섹션의 요소를 확인하고 요약해드리겠습니다. 논문의 arxiv id를 알고 계시면 함께 알려 주세요.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "논문 제목을 알려주시면 'Method' 섹션의 요소를 확인하고 요약해드리겠습니다. 논문의 arxiv id를 알고 계시면 함께 알려 주세요."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': '논문 Method의 각 요소를 요약해줘',\n",
       " 'chat_history': [],\n",
       " 'output': \"논문 제목을 알려주시면 'Method' 섹션의 요소를 확인하고 요약해드리겠습니다. 논문의 arxiv id를 알고 계시면 함께 알려 주세요.\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant('논문 Method의 각 요소를 요약해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection\n",
      "sections\n",
      "          Abstract.\n",
      "  1. Introduction\n",
      "  2. Related Work\n",
      "    2.1. Crowdsourcing Surveys\n",
      "    2.2. Large Language Models\n",
      "    2.3. Impact of Large Language Models on Crowdsourcing Studies\n",
      "    2.4. Prompt Injection as a Remedy\n",
      "  3. Prompt Injection in Large Language Models\n",
      "    3.1. Prompt Structure\n",
      "    3.2. Injecting Attack Prompts\n",
      "    3.3. Key Aspects of Attack Prompt Construction and Injection\n",
      "      3.3.1. Construction – Question Type\n",
      "      3.3.2. Construction – Method\n",
      "      3.3.3. Injection – Position\n",
      "  4. Case Study – Daily-life Scenario Recommendation\n",
      "    4.1. Task Description\n",
      "    4.2. Details in Attack Prompt Construction and Injection\n",
      "    4.3. Evaluation\n",
      "  5. Results\n",
      "    5.1. Effectiveness of Attack Prompts\n",
      "    5.2. Length of Attack Prompts\n",
      "  6. Software\n",
      "  7. Discussion\n",
      "    7.1. Alternative Approaches for Discerning or Mitigating \\acllm-generated Responses\n",
      "    7.2. Diversity of Opinions among Crowdsourcing Workers\n",
      "    7.3. Influence of \\acllms on the Trustworthiness of Crowdsourcing Surveys\n",
      "    7.4. Limitations\n",
      "  8. Conclusion\n",
      "  References\n",
      "  Appendix A Example of the dialogue flow for the proposed algorithm using GPT-4 API.\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection\" has the following sections:\n",
      "\n",
      "1. **Abstract**\n",
      "2. **Introduction**\n",
      "3. **Related Work**\n",
      "   - 2.1. Crowdsourcing Surveys\n",
      "   - 2.2. Large Language Models\n",
      "   - 2.3. Impact of Large Language Models on Crowdsourcing Studies\n",
      "   - 2.4. Prompt Injection as a Remedy\n",
      "4. **Prompt Injection in Large Language Models**\n",
      "   - 3.1. Prompt Structure\n",
      "   - 3.2. Injecting Attack Prompts\n",
      "   - 3.3. Key Aspects of Attack Prompt Construction and Injection\n",
      "     - 3.3.1. Construction – Question Type\n",
      "     - 3.3.2. Construction – Method\n",
      "     - 3.3.3. Injection – Position\n",
      "5. **Case Study – Daily-life Scenario Recommendation**\n",
      "   - 4.1. Task Description\n",
      "   - 4.2. Details in Attack Prompt Construction and Injection\n",
      "   - 4.3. Evaluation\n",
      "6. **Results**\n",
      "   - 5.1. Effectiveness of Attack Prompts\n",
      "   - 5.2. Length of Attack Prompts\n",
      "7. **Software**\n",
      "8. **Discussion**\n",
      "   - 7.1. Alternative Approaches for Discerning or Mitigating \\acllm-generated Responses\n",
      "   - 7.2. Diversity of Opinions among Crowdsourcing Workers\n",
      "   - 7.3. Influence of \\acllms on the Trustworthiness of Crowdsourcing Surveys\n",
      "   - 7.4. Limitations\n",
      "9. **Conclusion**\n",
      "10. **References**\n",
      "11. **Appendix A Example** of the dialogue flow for the proposed algorithm using GPT-4 API.\n",
      "\n",
      "If you would like a detailed summary of any of these sections, please specify which ones.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The paper \"Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection\" has the following sections:\n",
       "\n",
       "1. **Abstract**\n",
       "2. **Introduction**\n",
       "3. **Related Work**\n",
       "   - 2.1. Crowdsourcing Surveys\n",
       "   - 2.2. Large Language Models\n",
       "   - 2.3. Impact of Large Language Models on Crowdsourcing Studies\n",
       "   - 2.4. Prompt Injection as a Remedy\n",
       "4. **Prompt Injection in Large Language Models**\n",
       "   - 3.1. Prompt Structure\n",
       "   - 3.2. Injecting Attack Prompts\n",
       "   - 3.3. Key Aspects of Attack Prompt Construction and Injection\n",
       "     - 3.3.1. Construction – Question Type\n",
       "     - 3.3.2. Construction – Method\n",
       "     - 3.3.3. Injection – Position\n",
       "5. **Case Study – Daily-life Scenario Recommendation**\n",
       "   - 4.1. Task Description\n",
       "   - 4.2. Details in Attack Prompt Construction and Injection\n",
       "   - 4.3. Evaluation\n",
       "6. **Results**\n",
       "   - 5.1. Effectiveness of Attack Prompts\n",
       "   - 5.2. Length of Attack Prompts\n",
       "7. **Software**\n",
       "8. **Discussion**\n",
       "   - 7.1. Alternative Approaches for Discerning or Mitigating \\acllm-generated Responses\n",
       "   - 7.2. Diversity of Opinions among Crowdsourcing Workers\n",
       "   - 7.3. Influence of \\acllms on the Trustworthiness of Crowdsourcing Surveys\n",
       "   - 7.4. Limitations\n",
       "9. **Conclusion**\n",
       "10. **References**\n",
       "11. **Appendix A Example** of the dialogue flow for the proposed algorithm using GPT-4 API.\n",
       "\n",
       "If you would like a detailed summary of any of these sections, please specify which ones."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection 논문 목차를 요약해줘')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
